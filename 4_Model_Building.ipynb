{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('x_train.csv')\n",
    "X_test = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1525, 7), (382, 7))"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>area</th>\n",
       "      <th>year</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.290340</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.629629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.068127</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.913529</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.995634</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.963808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.744059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.168294</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.823828</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.189168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.625595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.482182</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.548770</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>6.906755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.335255</td>\n",
       "      <td>0.591334</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.595796</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.216088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.393263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.494986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.055158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.516193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.314630</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.652420</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.161122</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.546524</td>\n",
       "      <td>-0.309200</td>\n",
       "      <td>6.543912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.998961</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>8.116716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.330864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.142196</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.993934</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.138860</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.845172</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.087384</td>\n",
       "      <td>-1.684855</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.060371</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>8.069342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>8.100161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.983656</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.140991</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.203731</td>\n",
       "      <td>1.142148</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.772821</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.574343</td>\n",
       "      <td>-1.602171</td>\n",
       "      <td>7.464510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.837330</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.313716</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.905308</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.807578</td>\n",
       "      <td>-1.247584</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.115556</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.562681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.563727</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.624219</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-1.602171</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.976280</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.037962</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.350809</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.139846</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.723199</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.401328</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>6.932448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.335379</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.474761</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.171424</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.980977</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.132479</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.878913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.085901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.025159</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.401328</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.555955</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.233455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.662717</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.316394</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.151571</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.341649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.574343</td>\n",
       "      <td>-1.602171</td>\n",
       "      <td>7.464510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.052268</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.021847</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.140414</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.318742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.324957</td>\n",
       "      <td>-2.452447</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.887744</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>6.745236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.914482</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.722562</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.675157</td>\n",
       "      <td>-1.247584</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.255665</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.398595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.756354</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.018951</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.366164</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.691657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.178319</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.174829</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.881937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-2.604858</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.362402</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>8.516193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.836048</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.171424</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.362011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.694802</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.796343</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.021847</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>-0.769866</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.254824</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>7.085901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.016280</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>6.802395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.928406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.958566</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.489971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.390898</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.469377</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.923353</td>\n",
       "      <td>-0.309200</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.362011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.645505</td>\n",
       "      <td>1.029371</td>\n",
       "      <td>7.154615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.476371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.195817</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.070559</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.113956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.776954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.149231</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>8.740337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.259325</td>\n",
       "      <td>0.462254</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.318009</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.327992</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.532657</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.993934</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.122867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.714909</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.462254</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.186682</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.760263</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.098765</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.811023</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.974213</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.537960</td>\n",
       "      <td>1.016280</td>\n",
       "      <td>8.809863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>0.868299</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.362987</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.703093</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>6.902743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.236399</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.821542</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.993934</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>8.507143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.683165</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.509335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.757790</td>\n",
       "      <td>8.100161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.492774</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.748153</td>\n",
       "      <td>-2.604858</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.543814</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.278091</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.891042</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.430401</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.312553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.526203</td>\n",
       "      <td>0.757790</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.265427</td>\n",
       "      <td>-1.070231</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.594129</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.006034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.270404</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.587211</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.111346</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.069342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.647991</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.362011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.562681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.140414</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.337401</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.962697</td>\n",
       "      <td>7.684784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.517011</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>8.100161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.275384</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.306531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.952339</td>\n",
       "      <td>-0.067544</td>\n",
       "      <td>6.715383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.450024</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.248091</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.120954</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.400659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.188704</td>\n",
       "      <td>-0.863225</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.813996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>8.241440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.754646</td>\n",
       "      <td>-1.186019</td>\n",
       "      <td>8.240121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>-0.147397</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.974189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>1.117836</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.881997</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.450024</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>2.357714</td>\n",
       "      <td>1.435922</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.837330</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>6.856462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.014708</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.204694</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.167636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.120401</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.419381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.453261</td>\n",
       "      <td>0.834346</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.366164</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.691657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.587211</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.265430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.012950</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.485496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.729185</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.936561</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.130059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.852116</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.026591</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>6.388561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.867004</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.693412</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>8.214736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.365775</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.222993</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.571526</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.997999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.610954</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.062312</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.935268</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.330864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.420664</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.683165</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.341649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.740538</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.837401</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>6.902743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.370823</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.779557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.597157</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.110696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>7.296413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.041006</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.588324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.037962</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.410976</td>\n",
       "      <td>-0.769866</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.683165</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.754646</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.906755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.956582</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>8.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.377535</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>8.255828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.089301</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.462945</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.267537</td>\n",
       "      <td>0.462254</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.018733</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.822644</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.653894</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.532657</td>\n",
       "      <td>0.693487</td>\n",
       "      <td>8.216088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.360867</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.486734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.426972</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.762171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.974213</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.917091</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>7.987864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.693487</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.667396</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.241909</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.561133</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.419381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.932681</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.469377</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.901387</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.026591</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.597731</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.468579</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.084226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.852116</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.143125</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.601691</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.813996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.011476</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.336942</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.054840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.620073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.437024</td>\n",
       "      <td>0.849182</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.086410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.586541</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.958566</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.324957</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.837330</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.080248</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.764296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.052268</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.393263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.021847</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.036939</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.110696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.410157</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>8.389360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.445158</td>\n",
       "      <td>0.591334</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.862882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.241394</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.895401</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.011476</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.116205</td>\n",
       "      <td>-1.684855</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.370823</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.697652</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>7.997999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.352363</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.927558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.071755</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.302519</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>7.150701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.172514</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.785360</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>8.514590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.060371</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.594154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.180604</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.958566</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.679927</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.265427</td>\n",
       "      <td>-1.070231</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.269299</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.560080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.625595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.288125</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.228387</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.358228</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.362011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.120401</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.586541</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.036939</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.110696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.208542</td>\n",
       "      <td>0.834346</td>\n",
       "      <td>8.433812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.821103</td>\n",
       "      <td>-1.186019</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.183017</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>8.069342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.086410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.761842</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.065695</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.707868</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.110696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.763272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.090418</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.240121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.634652</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.241394</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>7.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.545422</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.204149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.625595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.420664</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.025159</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.974979</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.472273</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.515440</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.501082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.512440</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.849182</td>\n",
       "      <td>7.716461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.277251</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.221192</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.821103</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.073920</td>\n",
       "      <td>-1.602171</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.084179</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.326673</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.917091</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>7.987864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.796421</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>7.414573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.337401</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.729465</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.555955</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.233455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.011273</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>7.738052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.456494</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.403927</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-0.769866</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>1.016280</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.545761</td>\n",
       "      <td>-1.070231</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.558380</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.085901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.750471</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.574132</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.546524</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.900074</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>8.433812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.532657</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.710608</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.512440</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.373559</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.295406</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.905308</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.927558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.532657</td>\n",
       "      <td>-1.379181</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.006479</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.726337</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.789455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.831444</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.102669</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.635376</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>1.029371</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.248091</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.631637</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.435922</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.389360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.911887</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.586541</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.447544</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.299797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.163231</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.156223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.055158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.043679</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.665613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.926509</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.198983</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.364089</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>0.681710</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.462254</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.166717</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.660231</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.335255</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.907053</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>1.269021</td>\n",
       "      <td>8.593228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.900074</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>8.318742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.882437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.260471</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.060476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.365207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.214735</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>6.882437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.213973</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.811023</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.536005</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.594129</td>\n",
       "      <td>0.462254</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.482182</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.868759</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.359025</td>\n",
       "      <td>-1.247584</td>\n",
       "      <td>7.541683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.761842</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.753239</td>\n",
       "      <td>-2.314574</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.815611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.917091</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.972121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.515440</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.948986</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.890854</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.484745</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.587898</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.513989</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.350232</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.474761</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.600402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>2.357714</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>-4.780613</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.026399</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.055301</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.935143</td>\n",
       "      <td>8.432724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.575871</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>7.400010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.655367</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-0.863225</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.587211</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.393263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.472585</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.516193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.234590</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>8.682708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.296256</td>\n",
       "      <td>-1.311887</td>\n",
       "      <td>7.595890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.667396</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.004678</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.214736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.740538</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.888213</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.768661</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.637716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.783973</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.782586</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.174829</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.881937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.824748</td>\n",
       "      <td>-0.863225</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.116716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.221192</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.469377</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>8.317522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>1.080583</td>\n",
       "      <td>8.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.681710</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>8.354674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.399208</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.026805</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.174703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.955856</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.618927</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>6.829794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.606216</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.562681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.754646</td>\n",
       "      <td>-1.186019</td>\n",
       "      <td>8.240121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.488557</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.362402</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.228387</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>7.278629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.441320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.897370</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>6.796824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.055301</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.464510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>8.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.924539</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.276336</td>\n",
       "      <td>-1.311887</td>\n",
       "      <td>7.578145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>0.591334</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.574132</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.792280</td>\n",
       "      <td>-0.067544</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.130059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.339653</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.864294</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.368340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.761842</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.317396</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>7.150701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.426972</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.762171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.760263</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.148060</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.807618</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.993934</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.355556</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.771453</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.530251</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.158514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.625112</td>\n",
       "      <td>1.093107</td>\n",
       "      <td>7.625595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.573809</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.444622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.234687</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.025538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.997999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.073920</td>\n",
       "      <td>-1.602171</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.895401</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.389360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.031060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.005851</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.989335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.321083</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.389360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.277251</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.365531</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.069023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>1.080583</td>\n",
       "      <td>8.611594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.016719</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.366370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.512440</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.365106</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.410721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.569962</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.435922</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>8.003029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.161122</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.432724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.627205</td>\n",
       "      <td>-0.815793</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>8.621553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.149747</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.762171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>2.016071</td>\n",
       "      <td>0.878407</td>\n",
       "      <td>8.377931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>-0.863225</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.495970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.375019</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.050076</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.622263</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.556414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.350232</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.430401</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.312553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.125693</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.847712</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>-2.314574</td>\n",
       "      <td>7.588324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.810168</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.084871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>8.556414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.691869</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.223687</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.594129</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.293385</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.026591</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>6.388561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.269299</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.419381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.293385</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.350558</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.418240</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.485918</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.993934</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.446143</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.729465</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.266431</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.378826</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.481556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.353729</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.505931</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.134794</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.116716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.111346</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.703093</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>6.902743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.401328</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>6.856462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>1.016280</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>2.016071</td>\n",
       "      <td>0.878407</td>\n",
       "      <td>8.377931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.594129</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.978784</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.214608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.749014</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>8.116716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.801931</td>\n",
       "      <td>-0.769866</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.703420</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.188704</td>\n",
       "      <td>-0.863225</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.606216</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.085901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.513394</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.435285</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.567112</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.047517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.615677</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>8.779557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.034388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.330101</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.606216</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.562681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>7.997999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>-1.449760</td>\n",
       "      <td>7.878913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.066626</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>7.047517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.435922</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>8.003029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.754646</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.373559</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.288125</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.092502</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.078434</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.961296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.245851</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.443534</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.683391</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.979339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.300262</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.600402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.353729</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.713476</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.600402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.772821</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.569751</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.264730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>-0.045081</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.329750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.538884</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.136669</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.762171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.802996</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.494912</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.408082</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.996449</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.060371</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.940437</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>8.556414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.146772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.831444</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>6.684612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.927497</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.397548</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.015242</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.086410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.094073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.711415</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.802395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.258763</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>8.444622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.141022</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.661249</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.330864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.683165</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.509335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.277251</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.001131</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.145716</td>\n",
       "      <td>-1.449760</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.311506</td>\n",
       "      <td>-0.067544</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.531097</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.469377</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.459873</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.248728</td>\n",
       "      <td>1.269021</td>\n",
       "      <td>8.594154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.466163</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.830625</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.740538</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>8.005701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.849182</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.569751</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.264730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.018638</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.171424</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.774224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.054199</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.723199</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.357334</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-2.610182</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-4.010725</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.543814</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.407485</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.987864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.557457</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.092300</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.116716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.911887</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.206729</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.159041</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.704859</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.823828</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.021847</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.993934</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.884245</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>8.732305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.332183</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.335255</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.401328</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.486734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.117847</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.694802</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.293385</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>8.428362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.197839</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>8.318742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.504411</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.243613</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.211527</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.691146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.333524</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.907053</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.327992</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.075849</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.378826</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.481556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.974979</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.171424</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.362011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.180604</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.058335</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.796824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.698681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.261927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.613608</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>7.637716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.316394</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.782586</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.012950</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.105524</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.492774</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.469377</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.476371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.895401</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.341649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.729185</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.748305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.403736</td>\n",
       "      <td>-0.769866</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.390898</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.080436</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.507143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.119174</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.972421</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.796824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.138860</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.366370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.073193</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.018951</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.110696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.512440</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.431163</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.126891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.425529</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-2.545422</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.703420</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.229644</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.267449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.188875</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.834346</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.782586</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.972121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.625595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-0.815793</td>\n",
       "      <td>8.366370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.419381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.779557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.867004</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.810168</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.086103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.274579</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.420664</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.983656</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.143125</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.858942</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.460370</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.517011</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.472273</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.086410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.359025</td>\n",
       "      <td>-1.247584</td>\n",
       "      <td>7.541683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.469377</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.723486</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.171051</td>\n",
       "      <td>0.462254</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.125693</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.351298</td>\n",
       "      <td>-1.602171</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.117847</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.796343</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>7.823246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.629629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.956582</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>8.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>2.065566</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.968624</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.782586</td>\n",
       "      <td>0.709833</td>\n",
       "      <td>7.593374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.708637</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.699842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.034909</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>7.150701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.291879</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.255828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>1.085077</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.900266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.703420</td>\n",
       "      <td>-0.067544</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.358305</td>\n",
       "      <td>1.029371</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.316394</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.098643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.939143</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.653894</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.647991</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.094703</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.748153</td>\n",
       "      <td>-1.311887</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.138860</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.098765</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.506658</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.466089</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.840547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.108374</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.543912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.110272</td>\n",
       "      <td>0.591334</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.833338</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.466089</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.840547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.737132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.339614</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.882437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.159041</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.070559</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.618927</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>6.829794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.591334</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.296256</td>\n",
       "      <td>-1.311887</td>\n",
       "      <td>7.595890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.043042</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.085901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.306531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.013970</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>8.779557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.204149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.275384</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>-0.863225</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.989737</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.266094</td>\n",
       "      <td>-0.769866</td>\n",
       "      <td>7.478170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>2.357714</td>\n",
       "      <td>-5.114004</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.476972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.171424</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.774224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.413367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.555452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.353729</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.469377</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.899153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.728157</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.145716</td>\n",
       "      <td>-1.449760</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.407677</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.917091</td>\n",
       "      <td>0.709833</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>7.935587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.212164</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.624219</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.672405</td>\n",
       "      <td>-0.067544</td>\n",
       "      <td>8.242493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.980977</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.351158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.890854</td>\n",
       "      <td>0.709833</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.096675</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>8.611594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.867004</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.972421</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.796824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.171424</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.429521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.456494</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.480586</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-3.710604</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.367459</td>\n",
       "      <td>-0.309200</td>\n",
       "      <td>8.100161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.587898</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.810168</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.016280</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.532089</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.059403</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.098858</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>7.878913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.093107</td>\n",
       "      <td>7.823246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.254824</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.782586</td>\n",
       "      <td>0.709833</td>\n",
       "      <td>7.593374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.621229</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.055301</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.689293</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.821103</td>\n",
       "      <td>-1.186019</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.100161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.536005</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.740337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.969896</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.267449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.868299</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.818374</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.899153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.834346</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.418834</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.631637</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.342779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.982581</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.034909</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>7.150701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.958566</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.704859</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.159041</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-1.379181</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.453261</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.613608</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>7.637716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.831444</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.237348</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.782586</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.972121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.418834</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.852116</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.792280</td>\n",
       "      <td>-0.067544</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.976280</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.683165</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.604709</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.241440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.142714</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.214735</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.509335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.601691</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.813996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.365775</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.810168</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.316394</td>\n",
       "      <td>1.080583</td>\n",
       "      <td>8.098643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.128058</td>\n",
       "      <td>-1.015637</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.857602</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>8.400659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.793587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.186025</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.083546</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.930090</td>\n",
       "      <td>1.105524</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.926509</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>6.802395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.750471</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.446143</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.631637</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.605655</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.242706</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.171424</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.911887</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.760041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.136731</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.346309</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.715383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.754646</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.094703</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.228387</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>8.779557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.068127</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>8.341649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>7.296413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.204149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.212530</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.124151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>2.028552</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.878534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.168890</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>1.105524</td>\n",
       "      <td>7.772753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.870434</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>8.691146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.591334</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.184063</td>\n",
       "      <td>-0.815793</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.255828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.377397</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.174703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-0.708637</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.121624</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.587211</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.265430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.399208</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-0.736578</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.532089</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.337401</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.018951</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.560314</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.689293</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.767737</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.968624</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.433812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.722562</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.546524</td>\n",
       "      <td>-0.309200</td>\n",
       "      <td>6.543912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.811023</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.496498</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.107720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>2.357714</td>\n",
       "      <td>-5.114004</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.476972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>8.055158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.350558</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.335379</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.021183</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.084226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.384549</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>7.600402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.433812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.130331</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.354674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.137274</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.006680</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>8.556414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>2.164918</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.542430</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.151679</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.038835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.296413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.793782</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.899153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.978784</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.214608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.096675</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.932681</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.899153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.750471</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.526132</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.087384</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.708637</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.481556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.245851</td>\n",
       "      <td>-0.309200</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>8.241440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.054199</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.474188</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.867004</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.083546</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.316394</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.865630</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-0.725353</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.945587</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.254885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.015434</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.870053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>2.044052</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>8.809863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.143125</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.917091</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.698681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.878990</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.893572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.155328</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.447544</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.299797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.768661</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.278091</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.558998</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.764886</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.117847</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.004678</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.267537</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.634652</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.408082</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.043679</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.116716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.134794</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>-0.012950</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.114062</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.201269</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.836048</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.128058</td>\n",
       "      <td>-1.015637</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>6.802395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.295406</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.749014</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>8.116716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.467304</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.150701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.501249</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.523175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.810168</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.800555</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.056175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.512181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.055158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.464510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.612848</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>1.042345</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.056484</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.317522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.080436</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.512071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.335255</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.304859</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.513868</td>\n",
       "      <td>-0.815793</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.783973</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.243613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.392648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.324957</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.963808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.701979</td>\n",
       "      <td>-1.449760</td>\n",
       "      <td>8.241440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.948986</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.295406</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.137274</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.823828</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.888213</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.216644</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.226209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.955074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.912298</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.168890</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.134891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.243613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.392648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>7.520235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.593228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.267537</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.362402</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.140991</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.162747</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>1.080583</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.223687</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.561281</td>\n",
       "      <td>-1.684855</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.420664</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.664184</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>-2.604858</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.919356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.647344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.945587</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.845172</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.255665</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.047517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.330864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.398923</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.223687</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.983790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.401328</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>6.932448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.923353</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.368280</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.675581</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.091098</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.016719</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.159740</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.318742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.639107</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.054199</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-0.055301</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.192177</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>7.928406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.055158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.912298</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.475906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.723486</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.505931</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.648417</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.926509</td>\n",
       "      <td>1.093107</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.209625</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>6.678342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.134604</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.509335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.389360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.675581</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.418834</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.046137</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>8.341649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.915352</td>\n",
       "      <td>-1.247584</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.150701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.237840</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-3.058335</td>\n",
       "      <td>-0.124280</td>\n",
       "      <td>6.802395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.037962</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.080248</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.764296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.635376</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.681710</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.837330</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.940434</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.714909</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.337401</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.831444</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>6.684612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.849666</td>\n",
       "      <td>-1.126967</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.437732</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.151609</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.575871</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>7.729735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.982581</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.184063</td>\n",
       "      <td>-0.815793</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>2.492774</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.895401</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.507143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.064478</td>\n",
       "      <td>-1.449760</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.086410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.091216</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.243613</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>6.902743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.420664</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.977141</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.163231</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.923353</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.300262</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>1.093107</td>\n",
       "      <td>8.389133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.958415</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>8.610684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.114062</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>6.856462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.864294</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>7.368340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.410721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.022765</td>\n",
       "      <td>-1.311887</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.054199</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.689293</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.996449</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.575462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.055301</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.237840</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.748153</td>\n",
       "      <td>-1.311887</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.722562</td>\n",
       "      <td>0.591334</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.988896</td>\n",
       "      <td>-0.039990</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.277423</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.727535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.311506</td>\n",
       "      <td>-0.067544</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.878161</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.859413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.087384</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.926509</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.285099</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.180321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.459873</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.057461</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.943012</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.474761</td>\n",
       "      <td>0.757790</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.561281</td>\n",
       "      <td>-1.684855</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.874957</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.432724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.515440</td>\n",
       "      <td>-0.276691</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.266431</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.989737</td>\n",
       "      <td>7.562681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.152739</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.694802</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.841886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>7.590852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.993140</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.163947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.751831</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-1.015434</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.870053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.645505</td>\n",
       "      <td>1.029371</td>\n",
       "      <td>7.154615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.318742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.244228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.174869</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.762171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.276336</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.330864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city   zipcode  bathrooms  bedrooms      area      year     price\n",
       "0     0.458299 -0.155084   1.339383 -0.019637  0.290340 -0.376494  7.803843\n",
       "1    -0.443515 -1.600115   0.791788  0.857772 -0.386938  0.186894  7.696213\n",
       "2     1.360114  1.771624   0.791788  0.857772  0.981316  0.403203  8.629629\n",
       "3    -0.443515 -0.155084   1.339383  1.480304  1.068127  0.403203  7.955074\n",
       "4    -0.443515 -2.081792  -0.909197 -0.019637  0.231976  0.892801  7.824046\n",
       "5    -1.345330 -0.155084   1.786801  1.480304  1.913529  0.892801  8.342840\n",
       "6    -1.345330 -0.155084   0.791788  0.857772  1.995634  0.863869  7.963808\n",
       "7    -1.345330 -0.155084  -0.909197 -1.519579 -1.651490  0.403203  6.744059\n",
       "8     1.360114  2.253301   0.791788  0.857772  0.168294  0.403203  8.294050\n",
       "9    -1.345330 -0.155084  -0.909197 -1.519579 -0.640334  0.403203  7.244228\n",
       "10    1.360114  0.808270  -0.909197  0.217293 -1.878161  0.403203  7.740664\n",
       "11    1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.935587\n",
       "12   -0.443515 -0.155084   1.339383  0.857772  0.823828  0.403203  7.863267\n",
       "13   -0.443515 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.189168\n",
       "14    1.360114  2.253301  -0.909197  0.217293  0.109323  0.403203  7.625595\n",
       "15   -0.443515 -0.155084   1.339383  1.480304  1.104286  0.403203  7.989560\n",
       "16   -1.345330 -2.563469  -0.909197 -0.019637  0.482182  0.608868  7.492760\n",
       "17   -0.443515 -0.155084  -0.909197  0.217293 -3.548770 -3.777927  6.906755\n",
       "18    0.458299 -0.155084   0.791788  0.857772  1.335255  0.591334  7.824046\n",
       "19    0.458299 -0.155084   0.791788 -0.019637 -0.640334  0.403203  7.649693\n",
       "20    0.458299 -0.155084   1.786801  1.480304  1.595796  0.423156  8.216088\n",
       "21   -0.443515 -0.155084  -0.909197 -1.519579 -0.640334  0.403203  7.130899\n",
       "22   -0.443515 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.393263\n",
       "23    1.360114  1.289947  -0.909197  0.217293  0.109323  0.403203  7.494986\n",
       "24    0.458299 -0.155084   0.791788  0.857772  0.231976  0.403203  8.055158\n",
       "25    1.360114  2.253301   1.786801  0.857772  0.231976  0.403203  8.516193\n",
       "26   -1.345330 -2.563469   0.791788 -0.019637  0.314630 -1.865912  7.495542\n",
       "27    0.458299 -0.155084   0.791788  0.857772  0.652420  0.255249  8.131531\n",
       "28   -0.443515 -0.155084  -0.909197 -1.519579 -1.161122  0.403203  7.377759\n",
       "29   -0.443515  0.326593  -0.909197 -0.019637  0.231976  0.403203  7.600902\n",
       "30   -0.443515 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.492760\n",
       "31   -1.345330 -0.155084  -0.909197  0.217293 -1.546524 -0.309200  6.543912\n",
       "32    0.458299 -0.155084   1.339383  0.857772  0.998961 -1.523959  8.116716\n",
       "33    1.360114  1.771624  -0.909197 -0.019637 -0.160267  0.403203  8.330864\n",
       "34    1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.438384\n",
       "35   -0.443515 -0.155084  -0.909197 -0.019637  1.330956  0.921166  7.824046\n",
       "36   -1.345330 -0.155084   0.791788  0.857772  1.142196  0.186894  7.740664\n",
       "37    0.458299 -0.155084   0.791788  0.857772  0.993934 -0.411353  7.955074\n",
       "38   -0.443515 -2.081792   1.339383  0.857772 -0.138860 -1.965715  7.781139\n",
       "39   -0.443515  0.326593   1.786801  1.480304  1.845172  0.537418  8.101678\n",
       "40   -0.443515 -1.600115   0.791788  0.857772  0.850889  0.481415  7.783224\n",
       "41    0.458299 -0.155084   1.786801 -0.019637  0.087384 -1.684855  7.989560\n",
       "42   -1.345330 -0.155084  -0.909197  0.857772  0.060371  0.320519  7.467371\n",
       "43   -0.443515 -1.600115  -0.909197 -0.019637  0.712043  0.403203  7.919356\n",
       "44    0.458299 -0.155084   2.165087  1.480304  0.844156 -0.276691  8.069342\n",
       "45    0.458299 -0.155084   0.791788  0.857772  0.231976 -5.783300  8.100161\n",
       "46   -0.443515 -0.155084  -0.909197 -0.019637 -0.983656  0.403203  7.309881\n",
       "47    0.458299 -0.155084  -0.909197  0.217293 -2.131557  0.362451  7.408531\n",
       "48    0.458299 -0.155084   1.339383  0.857772  0.844156 -0.276691  8.039157\n",
       "49   -1.345330 -2.563469   0.085816 -0.019637 -0.140991 -0.483697  7.467371\n",
       "50   -0.443515  0.326593  -0.909197 -1.519579  0.109323  0.403203  7.166266\n",
       "51   -0.443515 -2.081792   0.791788 -0.019637 -0.203731  1.142148  7.803843\n",
       "52   -1.345330 -0.155084   1.339383  1.480304  1.772821 -0.124280  7.822044\n",
       "53   -1.345330 -0.155084   0.791788  1.480304  0.574343 -1.602171  7.464510\n",
       "54   -0.443515  0.326593  -0.909197 -0.019637 -0.837330  0.186894  7.492760\n",
       "55   -0.443515 -0.155084  -0.909197 -0.019637  0.404178  1.067949  7.696213\n",
       "56    0.458299 -0.155084  -0.909197 -1.519579 -1.313716 -0.559850  7.377759\n",
       "57   -1.345330 -0.155084   1.786801  0.857772  1.905308  0.163372  8.188689\n",
       "58   -1.345330 -0.155084   1.786801  1.480304  1.807578 -1.247584  7.824046\n",
       "59    0.458299 -0.155084   0.791788 -0.019637 -0.115556 -0.483697  7.972466\n",
       "60   -1.345330 -0.155084   0.085816 -0.019637  0.563613 -0.244897  7.562681\n",
       "61   -0.443515 -0.155084  -0.909197 -0.019637  0.404178  1.067949  7.696213\n",
       "62    1.360114  2.253301  -0.909197 -1.519579 -0.563727  0.403203  8.159089\n",
       "63    0.458299 -0.155084  -0.909197 -1.519579 -0.779181  0.403203  7.467371\n",
       "64   -0.443515 -0.155084   0.791788 -0.019637 -0.160267  0.537418  7.696213\n",
       "65   -0.443515 -0.155084   0.791788 -0.019637  0.624219  0.115149  8.242756\n",
       "66    1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  7.861342\n",
       "67    0.458299 -0.155084  -0.909197 -0.019637 -0.386938 -1.602171  7.438384\n",
       "68   -0.443515 -1.600115   0.791788  1.480304  1.104286  0.976280  8.268732\n",
       "69    1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.972466\n",
       "70   -0.443515  0.326593  -0.909197 -1.519579 -1.037962  0.065262  7.309881\n",
       "71    1.360114  2.253301  -0.909197 -1.519579 -1.350809  0.403203  8.006368\n",
       "72    1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.861342\n",
       "73   -1.345330 -2.563469   1.339383 -0.019637  1.139846 -2.072918  7.937375\n",
       "74   -0.443515 -2.081792   1.339383  0.857772  0.005305 -2.072918  8.004700\n",
       "75   -0.443515  0.326593  -0.909197 -0.019637  0.231976  0.403203  7.600902\n",
       "76   -1.345330 -0.155084  -0.909197  0.857772  0.404178  0.403203  8.242756\n",
       "77    1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  8.159089\n",
       "78   -0.443515  0.326593   0.791788  1.480304  1.723199  0.442837  8.188689\n",
       "79   -1.345330 -0.155084   0.791788 -1.519579 -0.401328 -0.447073  6.932448\n",
       "80   -1.345330 -0.155084   2.165087  1.480304  2.335379  0.341637  8.699515\n",
       "81   -1.345330 -0.155084  -0.909197 -0.019637 -0.474761  0.403203  7.374629\n",
       "82   -0.443515  0.326593  -0.909197 -1.519579 -1.171424  0.403203  7.090077\n",
       "83    1.360114  1.289947  -0.909197  0.217293 -1.980977  0.403203  7.693937\n",
       "84    1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  7.863267\n",
       "85   -0.443515 -0.155084   0.791788 -0.019637 -0.132479 -1.772553  7.822044\n",
       "86   -0.443515 -2.081792   0.085816 -0.019637 -0.160267 -1.965715  7.824046\n",
       "87    0.458299 -0.155084   0.791788  0.857772  0.231976  0.299088  7.878913\n",
       "88    0.458299 -0.155084  -0.909197  0.217293  0.109323  0.403203  7.085901\n",
       "89   -1.345330 -0.155084   0.085816  0.857772  1.025159  0.277334  7.600902\n",
       "90    1.360114  2.253301  -0.909197  0.217293 -1.446442  1.154153  7.546974\n",
       "91    1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.970740\n",
       "92   -1.345330 -0.155084  -0.909197 -1.519579 -0.401328  1.042345  7.130899\n",
       "93    1.360114 -0.155084  -0.909197 -0.019637 -0.779181  0.573584  7.696213\n",
       "94   -1.345330 -0.155084  -0.909197 -0.019637 -0.555955  0.442837  7.233455\n",
       "95    0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.408531\n",
       "96    0.458299 -0.155084   0.791788 -0.019637  0.044781  0.403203  7.824046\n",
       "97   -1.345330 -2.563469   1.339383 -0.019637  0.662717 -2.072918  7.824046\n",
       "98   -0.443515 -0.636761  -0.909197 -0.019637 -0.316394  0.403203  7.696213\n",
       "99    0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.065262  7.377759\n",
       "100  -0.443515 -0.155084   1.339383  0.857772  1.151571 -0.640236  7.738488\n",
       "101   1.360114 -0.155084   0.791788  0.857772  0.404178  0.403203  8.341649\n",
       "102  -1.345330 -0.155084   0.791788  1.480304  0.574343 -1.602171  7.464510\n",
       "103  -1.345330 -0.155084   0.791788  1.480304  1.052268 -3.191392  7.467371\n",
       "104   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.346010\n",
       "105   1.360114  2.253301  -0.909197 -1.519579 -1.021847  0.403203  7.882315\n",
       "106   1.360114  1.289947  -0.909197 -0.019637  0.140414  0.403203  8.070906\n",
       "107   0.458299 -1.118438  -0.909197  0.217293 -1.878161 -0.599484  7.240650\n",
       "108   0.458299 -0.155084  -0.909197 -1.519579 -1.446442  0.403203  7.435438\n",
       "109   1.360114 -0.155084   0.085816  0.857772  0.485372  0.362451  8.318742\n",
       "110   0.458299 -0.155084   1.786801 -0.019637  0.324957 -2.452447  7.970740\n",
       "111   1.360114  2.253301  -0.909197  0.217293 -1.259247  0.403203  7.863267\n",
       "112  -0.443515 -0.636761  -0.909197 -0.019637 -0.386938  0.403203  7.240650\n",
       "113  -1.345330 -0.155084  -0.909197 -1.519579 -1.887744 -0.559850  6.745236\n",
       "114  -1.345330 -0.155084   0.791788 -0.019637  0.914482  0.626192  7.843849\n",
       "115  -0.443515 -0.155084   0.791788 -0.019637 -0.722562  0.403203  7.546974\n",
       "116  -1.345330 -0.155084   1.339383  0.857772  1.675157 -1.247584  7.783224\n",
       "117  -0.443515 -2.081792  -0.909197 -1.519579 -1.255665  0.403203  6.398595\n",
       "118   1.360114  0.808270  -0.909197 -0.019637 -0.756354  0.403203  8.131531\n",
       "119  -0.443515 -2.081792   1.339383  0.857772  1.018951 -2.968402  8.160518\n",
       "120  -1.345330 -0.155084   0.791788 -0.019637  1.366164  0.139461  7.691657\n",
       "121   0.458299 -0.155084  -0.909197 -1.519579 -1.178319  0.403203  7.495542\n",
       "122  -1.345330 -0.155084   0.791788  0.857772  1.174829  0.277334  7.881937\n",
       "123   0.458299 -0.155084  -0.909197  0.217293  0.109323 -2.604858  7.374629\n",
       "124   1.360114  2.253301   0.085816  1.480304  0.362402  1.042345  8.516193\n",
       "125  -0.443515 -1.600115  -0.909197 -1.519579 -0.927611  0.403203  7.374629\n",
       "126   1.360114  1.289947   0.085816 -0.019637 -0.160267 -0.682168  8.131531\n",
       "127  -1.345330 -0.155084   1.339383  1.480304  0.836048 -3.777927  7.438384\n",
       "128  -0.443515 -0.155084  -0.909197 -1.519579 -1.171424  0.403203  7.362011\n",
       "129   1.360114  1.771624  -0.909197 -1.519579 -0.694802  0.403203  7.882315\n",
       "130  -1.345330 -0.155084   1.339383 -0.019637  1.796343 -0.342456  7.989560\n",
       "131  -0.443515 -0.636761  -0.909197 -1.519579 -1.021847  0.090420  7.522941\n",
       "132  -0.443515 -0.155084  -0.909197 -1.519579 -1.259247 -0.769866  7.279319\n",
       "133  -1.345330 -0.155084  -0.909197 -1.519579 -0.254824  0.741980  7.085901\n",
       "134  -1.345330 -0.155084  -0.909197 -1.519579 -1.651490 -0.599484  7.003065\n",
       "135   1.360114  2.253301  -0.909197  0.857772  0.109323  1.016280  8.411833\n",
       "136  -1.345330 -0.155084  -0.909197 -0.019637  0.231976  0.819355  6.802395\n",
       "137   1.360114  0.808270  -0.909197  0.857772  0.109323  0.403203  8.455318\n",
       "138   0.458299 -0.155084   1.339383 -0.019637  0.044781  0.403203  7.928406\n",
       "139  -1.345330 -0.155084   0.791788  0.857772  0.981316  0.299088  7.740664\n",
       "140   0.458299 -0.155084  -0.909197 -1.519579 -0.958566  0.403203  7.489971\n",
       "141  -1.345330 -2.563469   1.339383 -0.019637  0.390898 -0.483697  7.598399\n",
       "142  -1.345330 -0.155084   0.791788  1.480304  1.330956  0.403203  7.598399\n",
       "143  -0.443515 -0.155084   0.791788  0.857772  0.469377  0.921166  7.935587\n",
       "144   0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.362451  7.313220\n",
       "145   0.458299 -0.155084   2.165087  1.480304  1.923353 -0.309200  8.455318\n",
       "146  -0.443515 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.783224\n",
       "147   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.362011\n",
       "148  -0.443515 -0.155084  -0.909197  0.217293 -2.645505  1.029371  7.154615\n",
       "149   1.360114  2.253301   0.085816  1.480304  0.109323  0.403203  8.476371\n",
       "150   0.458299 -0.155084  -0.909197 -0.019637  0.195817  0.518990  8.070906\n",
       "151  -1.345330 -2.563469  -0.909197 -1.519579 -1.070559  0.403203  7.113956\n",
       "152   1.360114 -0.155084  -0.909197 -0.019637 -1.446442  0.403203  7.776954\n",
       "153   0.458299 -0.155084   1.786801  1.480304  1.149231  0.555614  8.740337\n",
       "154   1.360114  1.771624  -0.909197 -1.519579 -0.640334  0.039658  7.955074\n",
       "155   0.458299 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.492760\n",
       "156  -0.443515  0.326593  -0.909197 -1.519579 -0.509907  0.403203  7.600902\n",
       "157   1.360114  2.253301  -0.909197 -1.519579 -0.259325  0.462254  8.160518\n",
       "158  -1.345330 -0.155084   0.791788  1.480304  1.318009 -1.772553  7.937375\n",
       "159   1.360114  1.771624   0.791788  0.857772 -0.327992  0.403203  8.292799\n",
       "160  -0.443515 -0.155084   1.339383 -0.019637  0.532657 -2.072918  7.863267\n",
       "161  -1.345330 -0.155084   0.791788  0.857772  0.993934  0.210040  7.649693\n",
       "162   0.458299 -0.155084  -0.909197  0.217293  0.109323  0.403203  7.122867\n",
       "163   0.458299 -0.155084   0.791788 -0.019637  0.714909  0.403203  8.455318\n",
       "164   1.360114  2.253301  -0.909197 -1.519579 -0.927611  0.403203  8.101678\n",
       "165   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.462254  7.549609\n",
       "166   1.360114 -0.155084   0.791788 -0.019637  0.186682  0.403203  8.101678\n",
       "167  -1.345330 -0.155084   0.791788  0.857772  0.760263 -1.772553  7.309881\n",
       "168  -1.345330 -0.155084   0.085816 -0.019637 -0.098765  0.362451  7.244228\n",
       "169   1.360114  1.289947   0.085816  0.857772  0.404178  0.626192  8.268732\n",
       "170   1.360114 -0.155084   0.791788  1.480304 -0.160267  1.067949  8.294050\n",
       "171   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.309881\n",
       "172  -0.443515 -0.155084   0.791788  1.963178  1.811023  0.403203  8.188689\n",
       "173   0.458299 -0.155084  -0.909197 -0.019637 -0.974213  0.232821  7.647309\n",
       "174   1.360114  2.253301   0.791788  0.857772  1.537960  1.016280  8.809863\n",
       "175   1.360114  1.289947   1.339383  1.963178  0.868299  1.055204  8.612503\n",
       "176  -0.443515 -0.155084   0.791788  1.480304  1.362987  0.555614  8.006368\n",
       "177   0.458299 -0.155084   0.791788  1.480304  0.563613  0.442837  8.281471\n",
       "178  -1.345330 -2.563469  -0.909197 -0.019637 -0.703093  0.382970  6.902743\n",
       "179   1.360114  2.253301  -0.909197 -0.019637 -0.927611  0.403203  8.146130\n",
       "180  -1.345330 -0.155084   0.791788  0.857772  1.236399  0.341637  7.822044\n",
       "181   0.458299 -0.155084  -0.909197 -1.519579 -1.821542  0.403203  7.495542\n",
       "182  -0.443515  0.326593   0.085816  0.857772  0.993934  0.163372  8.037543\n",
       "183   1.360114 -0.155084   0.791788  0.857772  1.330956  0.804208  8.507143\n",
       "184   1.360114  0.808270  -0.909197 -0.019637 -0.160267  0.403203  8.188689\n",
       "185  -1.345330 -0.155084   0.791788  0.857772  0.683165 -0.153504  7.509335\n",
       "186   0.458299 -0.155084   0.791788  0.857772  0.844156  0.757790  8.100161\n",
       "187  -0.443515 -0.155084  -0.909197 -0.019637 -0.779181  0.403203  7.309881\n",
       "188   1.360114  0.808270  -0.909197 -0.019637  0.231976  0.403203  8.188689\n",
       "189  -0.443515 -0.636761  -0.909197 -1.519579 -0.386938  0.403203  7.696213\n",
       "190   0.458299 -0.155084   2.492774  1.963178  1.748153 -2.604858  8.268732\n",
       "191  -1.345330 -0.155084   1.339383  0.857772  1.543814  0.186894  8.006368\n",
       "192  -1.345330 -0.155084   0.791788  0.857772  0.278091  0.299088  7.438384\n",
       "193   0.458299 -0.155084  -0.909197 -1.519579 -0.891042 -0.124280  7.549609\n",
       "194   0.458299 -0.155084   0.791788  0.857772  0.485372  0.403203  7.955074\n",
       "195  -1.345330 -0.155084   0.791788 -0.019637 -0.430401 -0.559850  7.312553\n",
       "196  -0.443515 -0.636761   0.085816  0.857772  1.526203  0.757790  8.039157\n",
       "197  -0.443515 -0.636761   0.791788  1.480304  0.485372  1.154153  7.822044\n",
       "198  -1.345330 -0.155084   1.339383  0.857772  1.265427 -1.070231  7.598399\n",
       "199   0.458299 -0.155084   0.791788  0.857772  0.594129  0.320519  8.006034\n",
       "200   1.360114 -0.155084  -0.909197  0.217293 -2.270404  0.403203  7.546974\n",
       "201   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.346010\n",
       "202  -0.443515 -0.155084  -0.909197 -0.019637 -0.587211  0.804208  7.696213\n",
       "203  -1.345330 -0.155084  -0.909197 -0.019637 -0.111346  0.341637  7.170120\n",
       "204   1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  8.069342\n",
       "205   0.458299 -0.155084   0.791788  1.480304  0.647991 -3.191392  7.882315\n",
       "206  -0.443515  0.326593  -0.909197 -0.019637 -0.927611  0.186894  7.362011\n",
       "207  -1.345330 -0.155084   0.085816 -0.019637  0.563613 -0.244897  7.562681\n",
       "208  -0.443515  0.326593  -0.909197 -0.019637  0.140414 -0.244897  7.781139\n",
       "209  -1.345330 -0.155084   0.791788 -0.019637  1.337401 -0.447073  8.070906\n",
       "210  -0.443515 -0.636761  -0.909197 -0.019637  0.319800  0.962697  7.684784\n",
       "211  -0.443515 -0.155084   0.791788  0.857772  0.517011  0.788900  8.100161\n",
       "212  -1.345330 -0.155084   1.786801  1.480304  1.275384  0.255249  7.824046\n",
       "213  -0.443515 -0.636761  -0.909197 -1.519579 -1.259247  0.403203  7.306531\n",
       "214   0.458299 -0.155084   2.165087  0.857772  0.712043  0.403203  8.187299\n",
       "215  -1.345330 -0.155084  -0.909197 -1.519579 -0.952339 -0.067544  6.715383\n",
       "216  -1.345330 -0.155084   0.791788  0.857772  0.450024  0.277334  7.313220\n",
       "217  -0.443515 -1.600115  -0.909197 -0.019637 -0.248091  0.518990  7.937375\n",
       "218   0.458299 -0.155084   1.786801  1.480304  1.120954  0.299088  8.400659\n",
       "219  -0.443515  0.326593  -0.909197 -1.519579 -1.188704 -0.863225  7.495542\n",
       "220   1.360114  0.808270  -0.909197 -0.019637 -0.779181  0.403203  7.813996\n",
       "221   1.360114  2.253301  -0.909197 -0.019637  0.109323  1.067949  8.241440\n",
       "222   0.458299 -1.118438   0.791788 -0.019637  0.712043 -0.682168  7.955074\n",
       "223  -0.443515 -0.636761   1.339383  0.857772  0.754646 -1.186019  8.240121\n",
       "224   0.458299 -0.155084   0.791788  1.480304 -0.147397  0.863869  7.974189\n",
       "225   1.360114  1.771624  -0.909197  0.217293 -1.651490  1.117836  7.575585\n",
       "226  -0.443515  0.326593   0.791788  1.480304  1.104286  0.320519  8.342840\n",
       "227   0.458299 -1.118438  -0.909197 -1.519579 -0.779181  0.403203  7.207860\n",
       "228   1.360114  0.808270  -0.909197 -1.519579 -0.881997  0.277334  7.972466\n",
       "229  -1.345330 -0.155084   0.791788  0.857772  0.450024  0.277334  7.313220\n",
       "230   1.360114  0.808270   1.786801  2.357714  1.435922  0.403203  8.699515\n",
       "231  -1.345330 -0.155084  -0.909197 -1.519579 -0.837330 -0.521272  6.856462\n",
       "232  -0.443515 -0.155084   0.791788  0.857772 -0.014708  0.403203  7.647309\n",
       "233   0.458299 -0.155084   0.791788  1.963178  1.204694  0.210040  8.167636\n",
       "234   1.360114  2.253301  -0.909197  0.217293  0.109323  0.403203  7.549609\n",
       "235  -0.443515 -0.155084  -0.909197 -0.019637 -1.120401  0.320519  7.575585\n",
       "236   1.360114  0.808270   0.791788  1.480304  0.231976  0.403203  8.517193\n",
       "237   0.458299 -1.118438  -0.909197 -1.519579 -0.927611 -0.095634  7.419381\n",
       "238   1.360114 -0.155084  -0.909197 -1.519579 -1.259247  0.403203  7.718685\n",
       "239  -0.443515 -0.155084  -0.909197 -0.019637  0.453261  0.834346  7.598399\n",
       "240   1.360114  0.808270   0.791788  0.857772  0.109323  0.403203  8.294050\n",
       "241   1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  7.972466\n",
       "242  -1.345330 -0.155084   0.791788  0.857772  0.404178  0.423156  7.919356\n",
       "243   1.360114  1.289947  -0.909197 -1.519579 -2.131557  0.573584  7.740664\n",
       "244  -1.345330 -0.155084   0.791788 -0.019637  1.366164  0.139461  7.691657\n",
       "245  -0.443515  0.326593   0.791788  1.480304  1.104286  0.320519  8.342840\n",
       "246  -0.443515 -0.155084  -0.909197 -0.019637 -0.587211  0.403203  7.265430\n",
       "247   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.919356\n",
       "248  -0.443515 -0.155084  -0.909197  0.857772  0.044781 -0.012950  7.863267\n",
       "249  -0.443515 -0.636761  -0.909197 -0.019637 -0.927611  0.788900  7.166266\n",
       "250   1.360114 -0.155084   0.791788  0.857772  1.330956 -0.447073  8.485496\n",
       "251  -1.345330 -0.155084   0.791788  0.857772  0.729185  0.403203  7.244228\n",
       "252  -0.443515 -0.636761   1.786801  0.857772  0.936561  0.403203  8.160518\n",
       "253   0.458299 -0.155084   0.791788  0.857772  0.712043  0.163372  8.130059\n",
       "254   1.360114 -0.155084  -0.909197 -1.519579 -0.852116  0.403203  8.039157\n",
       "255  -1.345330 -0.155084  -0.909197 -1.519579 -2.026591  0.819355  6.388561\n",
       "256   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.039658  7.781139\n",
       "257   0.458299 -0.155084   0.791788  0.857772  0.231976 -5.783300  8.188689\n",
       "258  -1.345330 -0.155084  -0.909197 -1.519579 -0.867004  0.403203  7.090077\n",
       "259  -0.443515 -1.600115   1.786801  1.963178  1.693412 -1.965715  8.214736\n",
       "260  -0.443515  0.326593   0.791788 -0.019637  0.365775 -0.276691  7.824046\n",
       "261   1.360114  1.771624  -0.909197  0.217293  0.109323  1.042345  7.781139\n",
       "262  -0.443515 -0.155084  -0.909197 -1.519579 -0.640334  0.341637  7.166266\n",
       "263   0.458299 -0.155084   0.791788 -0.019637 -0.386938  0.403203  7.693937\n",
       "264  -0.443515 -0.155084   0.791788 -0.019637  0.222993 -1.523959  7.673223\n",
       "265   0.458299 -0.155084  -0.909197 -0.019637 -0.571526  0.403203  7.997999\n",
       "266  -0.443515 -0.155084  -0.909197 -0.019637 -0.610954 -0.559850  7.377759\n",
       "267  -1.345330 -2.563469  -0.909197 -0.019637  0.062312  0.423156  8.294050\n",
       "268  -1.345330 -0.155084   0.791788  0.857772  0.850889  0.299088  7.467371\n",
       "269   1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.972466\n",
       "270   1.360114  1.289947   0.791788  0.857772  0.935268  0.423156  8.330864\n",
       "271  -1.345330 -0.155084   0.791788  0.857772  0.420664  0.139461  7.649693\n",
       "272  -1.345330 -0.155084   0.791788  0.857772  0.712043 -0.095634  7.467371\n",
       "273   1.360114  0.808270   0.791788  1.480304  0.683165  0.403203  8.341649\n",
       "274  -1.345330 -0.155084   0.791788 -0.019637  0.740538  0.320519  7.598399\n",
       "275  -0.443515 -0.155084   1.339383  0.857772  0.837401 -2.072918  7.824046\n",
       "276  -1.345330 -0.155084  -0.909197 -1.519579 -0.779181  0.555614  6.902743\n",
       "277   0.458299 -0.155084   0.791788  0.857772  0.370823  0.382970  8.779557\n",
       "278  -1.345330 -0.155084  -0.909197  0.857772  0.597157  0.382970  7.170120\n",
       "279  -0.443515 -0.636761   0.791788 -0.019637 -0.386938  0.403203  7.495542\n",
       "280  -1.345330 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  7.110696\n",
       "281  -1.345330 -0.155084   0.791788  0.857772  0.151609 -0.213787  7.296413\n",
       "282   0.458299 -0.155084  -0.909197 -1.519579  0.231976  0.403203  8.411833\n",
       "283   1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.935587\n",
       "284  -1.345330 -0.155084   0.085816 -0.019637 -0.041006 -0.521272  7.588324\n",
       "285  -0.443515  0.326593  -0.909197 -1.519579 -1.037962  0.065262  7.309881\n",
       "286  -0.443515 -0.155084  -0.909197 -1.519579 -0.410976 -0.769866  7.598399\n",
       "287  -1.345330 -0.155084   0.791788 -0.019637  0.683165  0.403203  7.693937\n",
       "288  -1.345330 -0.155084   0.791788  0.857772  0.754646 -0.095634  7.408531\n",
       "289  -0.443515 -0.155084  -0.909197  0.217293 -2.131557  0.403203  6.906755\n",
       "290  -1.345330 -0.155084   1.786801  0.857772  1.956582 -1.523959  8.202482\n",
       "291   0.458299 -0.155084   0.791788  0.857772  0.377535  0.537418  8.255828\n",
       "292   0.458299 -0.155084   1.786801  1.480304  0.089301  0.277334  8.039157\n",
       "293   0.458299 -0.155084   0.791788  0.857772  0.462945  0.320519  8.465900\n",
       "294   0.458299 -0.155084   0.791788  0.857772  0.267537  0.462254  7.937375\n",
       "295   0.458299 -0.155084   0.791788 -0.019637 -0.018733  0.403203  8.004700\n",
       "296  -0.443515 -0.636761  -0.909197 -1.519579 -0.822644  0.403203  7.374629\n",
       "297   0.458299 -0.155084   0.791788 -0.019637  0.109323  0.403203  7.467371\n",
       "298  -0.443515  0.326593   0.791788  0.857772  0.653894 -4.194079  8.229511\n",
       "299   1.360114  0.808270   0.085816 -0.019637  0.532657  0.693487  8.216088\n",
       "300   0.458299 -0.155084   1.786801  0.857772  1.360867  0.403203  8.486734\n",
       "301  -0.443515 -0.155084  -0.909197  0.217293 -1.426972 -0.599484  7.762171\n",
       "302   0.458299 -0.155084  -0.909197 -1.519579 -0.974213  0.403203  7.435438\n",
       "303   1.360114 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.824046\n",
       "304   0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.299088  7.244228\n",
       "305  -0.443515  0.326593   0.791788  0.857772  0.917091  0.804208  7.987864\n",
       "306   0.458299 -0.155084   1.339383 -0.019637  0.231976  0.403203  8.004700\n",
       "307   1.360114  1.771624  -0.909197 -1.519579 -0.640334  0.693487  8.006368\n",
       "308  -0.443515 -0.636761  -0.909197 -1.519579 -1.446442  0.403203  7.374629\n",
       "309  -0.443515 -2.081792  -0.909197 -1.519579 -0.667396 -2.775240  7.377759\n",
       "310   1.360114  0.808270  -0.909197  0.217293 -2.241909  0.403203  7.536364\n",
       "311   1.360114 -0.155084  -0.909197 -1.519579 -0.561133 -0.153504  7.824046\n",
       "312   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.824046\n",
       "313   0.458299 -0.155084  -0.909197 -1.519579 -1.446442  0.403203  7.408531\n",
       "314  -0.443515 -0.155084   0.791788  0.857772  1.104286  0.277334  8.187299\n",
       "315   1.360114 -0.155084  -0.909197 -1.519579 -0.927611  0.725996  8.039157\n",
       "316   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.781139\n",
       "317   1.360114 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.495542\n",
       "318   0.458299 -0.155084  -0.909197 -1.519579 -0.779181  0.039658  7.419381\n",
       "319  -1.345330 -0.155084   0.791788  0.857772  0.932681  0.115149  7.824046\n",
       "320  -0.443515 -0.636761   1.786801  0.857772  0.469377 -1.772553  8.294050\n",
       "321   1.360114  0.808270   1.339383  0.857772  0.901387  0.013593  8.517193\n",
       "322  -0.443515  0.326593  -0.909197  0.217293 -2.026591  0.403203  7.090077\n",
       "323   0.458299 -0.155084  -0.909197  0.217293 -1.651490  0.403203  7.346010\n",
       "324   0.458299 -0.155084   0.791788 -0.019637 -0.597731  0.403203  7.522941\n",
       "325  -0.443515 -0.155084  -0.909197  0.217293 -2.468579  0.403203  7.084226\n",
       "326  -1.345330 -0.155084  -0.909197  0.857772  0.563613  0.500324  7.696213\n",
       "327  -0.443515 -0.636761  -0.909197 -0.019637 -0.852116  0.403203  7.438384\n",
       "328  -1.345330 -0.155084   0.085816  0.857772 -0.143125 -0.095634  7.170120\n",
       "329  -0.443515  0.326593  -0.909197 -0.019637  0.044781  0.186894  7.901007\n",
       "330  -0.443515 -0.155084   1.339383  0.857772  0.601691  0.403203  7.813996\n",
       "331  -1.345330 -0.155084   0.791788  0.857772  1.011476 -0.725353  7.649693\n",
       "332  -0.443515 -0.155084   0.791788  0.857772  0.336942  0.819355  7.130899\n",
       "333   0.458299 -0.155084  -0.909197 -0.019637 -0.640334  0.403203  7.824046\n",
       "334   0.458299 -0.155084  -0.909197 -1.519579 -0.779181  0.403203  7.438384\n",
       "335   1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  8.054840\n",
       "336  -1.345330 -0.155084   0.791788  1.480304  1.330956  0.403203  7.598399\n",
       "337   0.458299 -0.155084   0.791788 -0.019637  0.044781  0.255249  7.824046\n",
       "338   1.360114  0.808270  -0.909197 -0.019637 -0.779181  0.039658  7.970740\n",
       "339  -1.345330 -0.155084  -0.909197  0.217293 -1.259247  0.403203  6.620073\n",
       "340  -1.345330 -2.563469  -0.909197 -0.019637  0.437024  0.849182  7.522941\n",
       "341   0.458299 -0.155084   0.791788 -0.019637  0.044781  0.163372  8.086410\n",
       "342   0.458299 -0.155084   0.791788 -0.019637  0.586541  0.039658  7.843849\n",
       "343   1.360114  2.253301  -0.909197 -1.519579 -0.958566  0.403203  7.901007\n",
       "344  -0.443515 -0.155084  -0.909197 -1.519579 -0.927611  0.362451  7.492760\n",
       "345  -0.443515 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.170120\n",
       "346   1.360114 -0.155084   0.085816 -0.019637  0.231976 -0.447073  8.188689\n",
       "347  -0.443515 -2.081792   0.085816 -0.019637  0.563613  0.403203  7.696213\n",
       "348  -0.443515 -2.081792  -0.909197 -1.519579  0.109323  0.403203  7.492760\n",
       "349   0.458299 -0.155084   1.339383 -0.019637  0.324957 -1.965715  7.970740\n",
       "350  -0.443515 -0.636761  -0.909197 -0.019637 -0.386938  0.403203  7.240650\n",
       "351  -0.443515  0.326593  -0.909197 -0.019637 -0.837330  0.186894  7.492760\n",
       "352  -0.443515 -0.155084   1.339383  1.480304  1.104286  0.403203  7.989560\n",
       "353  -0.443515  0.326593  -0.909197 -0.019637  1.080248  0.403203  7.764296\n",
       "354  -1.345330 -0.155084   0.791788  1.480304  1.052268 -3.191392  7.467371\n",
       "355  -0.443515  0.326593  -0.909197 -1.519579  0.109323  0.403203  6.998510\n",
       "356  -0.443515 -1.600115  -0.909197 -1.519579 -0.927611  0.403203  7.393263\n",
       "357  -1.345330 -0.155084   1.339383 -0.019637  0.981316 -2.775240  7.824046\n",
       "358   0.458299 -0.155084  -0.909197 -1.519579 -1.021847 -0.640236  7.467371\n",
       "359  -1.345330 -0.155084   0.791788 -0.019637 -0.036939 -0.483697  7.110696\n",
       "360   0.458299 -0.155084   2.165087  1.480304  1.410157 -0.124280  8.389360\n",
       "361   1.360114 -0.155084  -0.909197 -0.019637  0.445158  0.591334  8.039157\n",
       "362   1.360114  2.253301  -0.909197 -1.519579 -1.087045  0.403203  7.862882\n",
       "363   0.458299 -0.155084  -0.909197 -1.519579 -1.241394 -0.183332  7.492760\n",
       "364  -0.443515 -0.155084  -0.909197 -0.019637 -0.160267 -0.244897  7.408531\n",
       "365   0.458299 -0.155084   0.791788  0.857772  1.895401 -2.072918  7.901007\n",
       "366  -0.443515 -0.155084  -0.909197 -1.519579 -1.087045  0.277334  7.090077\n",
       "367  -1.345330 -0.155084   0.791788  0.857772  1.011476 -0.725353  7.649693\n",
       "368   0.458299 -0.155084   1.339383  1.480304  1.116205 -1.684855  7.989560\n",
       "369  -1.345330 -2.563469  -0.909197 -0.019637  0.370823  0.741980  7.374629\n",
       "370   0.458299 -0.155084  -0.909197  0.217293 -1.446442  0.403203  7.467371\n",
       "371   1.360114  1.771624  -0.909197 -1.519579 -0.927611  0.403203  8.006368\n",
       "372  -0.443515  0.326593   0.085816  0.857772  0.697652  0.518990  7.997999\n",
       "373  -0.443515 -0.155084   0.791788  0.857772  1.352363  0.163372  7.861342\n",
       "374   0.458299 -0.155084   0.791788  1.480304  1.104286  0.232821  8.160518\n",
       "375  -0.443515 -0.155084  -0.909197  0.217293  0.109323  0.403203  6.927558\n",
       "376  -1.345330 -0.155084   0.791788 -0.019637 -0.071755  0.403203  7.408531\n",
       "377   0.458299 -0.155084  -0.909197  0.857772  0.850889  0.921166  7.781139\n",
       "378   0.458299 -0.155084   0.791788  1.480304  0.302519 -1.865912  7.901007\n",
       "379  -0.443515 -0.155084  -0.909197  0.217293 -2.131557  1.042345  7.150701\n",
       "380   0.458299 -0.155084   0.791788  1.480304  1.172514  0.403203  8.188689\n",
       "381   1.360114 -0.155084   0.791788 -0.019637  0.231976  0.403203  8.188689\n",
       "382  -1.345330 -0.155084   0.791788  0.857772  0.785360 -2.188705  8.514590\n",
       "383  -1.345330 -0.155084  -0.909197  0.857772  0.060371  0.320519  7.467371\n",
       "384  -0.443515 -2.081792  -0.909197 -0.019637  0.044781  0.403203  7.696213\n",
       "385  -0.443515 -0.155084  -0.909197 -1.519579 -0.640334  0.341637  7.166266\n",
       "386   1.360114  0.808270  -0.909197 -0.019637 -0.927611  0.403203  7.740664\n",
       "387   1.360114  2.253301  -0.909197  0.857772  0.231976  0.403203  8.594154\n",
       "388  -0.443515 -0.155084   1.339383  0.857772  1.180604 -1.965715  7.989560\n",
       "389  -0.443515  0.326593   0.791788 -0.019637  0.044781  0.403203  7.740664\n",
       "390   0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.403203  7.549609\n",
       "391   0.458299 -0.155084   0.791788 -0.019637  0.044781  0.115149  7.718685\n",
       "392  -0.443515 -0.155084  -0.909197 -1.519579 -1.651490  0.403203  7.003065\n",
       "393   0.458299 -0.155084  -0.909197 -0.019637 -0.958566 -5.783300  7.740664\n",
       "394   1.360114 -0.155084  -0.909197  0.217293 -2.679927  0.403203  7.435438\n",
       "395   0.458299 -0.155084   1.786801 -0.019637  0.923600 -0.244897  7.955074\n",
       "396  -1.345330 -0.155084   1.339383  0.857772  1.265427 -1.070231  7.598399\n",
       "397   0.458299 -0.155084   0.791788 -0.019637  0.269299  0.403203  7.560080\n",
       "398   0.458299 -0.155084  -0.909197 -1.519579 -1.651490  0.403203  7.625595\n",
       "399   1.360114 -0.155084  -0.909197  0.217293 -1.288125  0.403203  7.937375\n",
       "400   0.458299 -0.155084   0.791788 -0.019637  0.231976  0.403203  7.935587\n",
       "401   0.458299 -0.155084   0.791788 -0.019637  0.228387  0.403203  7.919356\n",
       "402  -1.345330 -0.155084   0.085816 -1.519579  0.044781 -0.244897  7.346010\n",
       "403  -0.443515 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.090077\n",
       "404  -0.443515 -2.081792  -0.909197 -0.019637  0.231976  0.892801  7.824046\n",
       "405   0.458299 -0.155084  -0.909197 -1.519579 -2.358228  0.403203  7.362011\n",
       "406   0.458299 -0.155084   0.791788 -0.019637 -0.927611  0.403203  7.600902\n",
       "407   1.360114  2.253301  -0.909197 -1.519579 -0.927611  0.403203  7.972466\n",
       "408  -0.443515 -0.155084  -0.909197 -0.019637 -1.120401  0.320519  7.575585\n",
       "409   1.360114  0.808270   0.791788 -0.019637  0.109323  0.403203  8.159089\n",
       "410  -0.443515 -0.636761  -0.909197 -1.519579 -1.087045  0.403203  7.492760\n",
       "411   0.458299 -0.155084   0.791788 -0.019637  0.586541  0.341637  8.342840\n",
       "412  -1.345330 -0.155084   0.791788 -0.019637 -0.036939 -0.483697  7.110696\n",
       "413   1.360114  0.808270  -0.909197 -0.019637  0.208542  0.834346  8.433812\n",
       "414  -1.345330 -0.155084  -0.909197 -0.019637  0.821103 -1.186019  7.522941\n",
       "415   0.458299 -0.155084   0.791788 -0.019637  0.183017  0.403203  7.673223\n",
       "416   1.360114  2.253301  -0.909197 -0.019637  0.109323  1.042345  8.069342\n",
       "417   1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  8.086410\n",
       "418  -0.443515 -0.155084  -0.909197 -1.519579 -1.761842  0.403203  6.998510\n",
       "419  -1.345330 -0.155084   0.791788  0.857772  1.065695  0.065262  7.673223\n",
       "420   0.458299 -0.155084  -0.909197  0.217293 -2.707868  0.403203  7.110696\n",
       "421   1.360114  1.289947  -0.909197 -0.019637  0.109323  0.403203  8.039157\n",
       "422   1.360114  2.253301   1.786801  1.480304  0.109323  0.403203  8.763272\n",
       "423   1.360114  1.771624  -0.909197 -0.019637 -0.090418  0.403203  8.240121\n",
       "424  -1.345330 -2.563469   0.791788  0.857772  0.634652  0.537418  7.803843\n",
       "425   0.458299 -0.155084  -0.909197 -1.519579 -1.241394 -0.183332  7.536364\n",
       "426  -0.443515  0.326593  -0.909197  0.217293 -2.545422  0.403203  7.204149\n",
       "427   0.458299 -0.155084   0.791788 -0.019637 -0.386938  0.403203  7.625595\n",
       "428  -0.443515 -0.636761   0.791788 -0.019637  0.420664 -3.455133  8.131531\n",
       "429  -0.443515 -0.155084  -0.909197 -1.519579 -0.779181  0.090420  7.130899\n",
       "430   1.360114  2.253301  -0.909197 -1.519579 -1.651490  0.403203  7.781139\n",
       "431  -1.345330 -0.155084   0.791788  0.857772  1.025159 -0.213787  7.492760\n",
       "432   0.458299 -0.155084  -0.909197 -0.019637 -0.509907  0.403203  7.492760\n",
       "433  -1.345330 -2.563469   0.791788 -0.019637  0.974979  0.423156  8.070906\n",
       "434  -0.443515 -0.155084   0.085816 -1.519579 -0.472273  0.403203  7.783224\n",
       "435  -1.345330 -0.155084   0.791788  0.857772  0.515440  0.362451  7.501082\n",
       "436   1.360114 -0.155084  -0.909197 -0.019637 -0.512440  0.210040  8.160518\n",
       "437   1.360114  0.808270  -0.909197  0.217293 -1.651490  0.403203  7.824046\n",
       "438   1.360114  2.253301  -0.909197 -1.519579 -1.087045  0.849182  7.716461\n",
       "439  -0.443515 -2.081792  -0.909197 -1.519579 -1.277251 -3.191392  7.240650\n",
       "440  -1.345330 -0.155084   0.791788  0.857772  0.221192  0.403203  7.495542\n",
       "441   1.360114  1.771624   0.791788 -0.019637  0.821103 -0.447073  8.517193\n",
       "442  -1.345330 -0.155084   0.791788 -0.019637  0.073920 -1.602171  7.740664\n",
       "443   0.458299 -0.155084  -0.909197  0.857772 -0.084179  0.382970  8.006368\n",
       "444  -1.345330 -0.155084   0.791788  0.857772  0.326673 -0.640236  7.279319\n",
       "445  -0.443515  0.326593   0.791788  0.857772  0.917091  0.804208  7.987864\n",
       "446   1.360114  2.253301  -0.909197 -1.519579 -0.927611  0.403203  7.972466\n",
       "447   0.458299 -0.155084  -0.909197 -1.519579  0.796421  0.819355  7.414573\n",
       "448  -1.345330 -0.155084   0.791788  0.857772  1.337401 -0.682168  7.696213\n",
       "449   0.458299 -0.155084  -0.909197 -1.519579 -1.651490 -5.783300  7.522941\n",
       "450  -1.345330 -0.155084   1.339383  1.480304  1.729465 -0.682168  7.882315\n",
       "451  -1.345330 -0.155084   0.791788  0.857772  0.926199 -3.777927  7.598399\n",
       "452  -1.345330 -0.155084  -0.909197 -0.019637 -0.555955  0.442837  7.233455\n",
       "453   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.901007\n",
       "454   0.458299 -0.155084   0.791788 -0.019637  0.011273 -2.188705  7.738052\n",
       "455  -1.345330 -0.155084   0.791788  0.857772  0.456494  0.341637  7.696213\n",
       "456   0.458299 -0.155084   0.791788  1.480304  1.403927 -0.276691  8.188689\n",
       "457   0.458299 -1.118438   1.339383 -0.019637  0.231976 -0.769866  7.649693\n",
       "458  -0.443515  0.326593   0.791788 -0.019637  0.981316  1.016280  7.937375\n",
       "459  -0.443515 -0.155084   1.786801  1.963178  1.545761 -1.070231  8.070906\n",
       "460  -1.345330 -0.155084   1.339383 -0.019637  1.558380 -1.865912  7.937375\n",
       "461  -0.443515 -0.155084  -0.909197 -1.519579 -1.651490  0.403203  7.085901\n",
       "462   1.360114 -0.155084  -0.909197  0.217293 -2.750471  0.403203  7.495542\n",
       "463  -1.345330 -0.155084  -0.909197 -0.019637  0.712043  0.382970  7.937375\n",
       "464   1.360114  1.771624  -0.909197 -1.519579 -0.574132  0.403203  7.937375\n",
       "465   0.458299 -0.155084  -0.909197 -1.519579 -1.546524  0.403203  7.313220\n",
       "466   1.360114  0.808270   0.791788  0.857772  0.900074  0.573584  8.433812\n",
       "467   0.458299 -0.155084   0.791788  1.480304  0.532657  0.186894  8.101678\n",
       "468   0.458299 -0.155084   0.791788 -0.019637  0.710608  0.320519  8.146130\n",
       "469  -1.345330 -0.155084  -0.909197 -0.019637 -0.512440  0.341637  7.130899\n",
       "470  -0.443515 -0.155084   0.791788  1.480304  1.373559  0.773428  8.159089\n",
       "471   0.458299 -0.155084  -0.909197 -1.519579 -1.295406  0.403203  7.309881\n",
       "472  -0.443515  0.326593   1.339383  0.857772  1.330956  0.403203  8.411833\n",
       "473  -1.345330 -0.155084   1.786801  0.857772  1.905308  0.163372  8.188689\n",
       "474  -0.443515 -0.155084  -0.909197  0.217293  0.109323  0.403203  6.927558\n",
       "475  -0.443515 -0.155084   1.339383  0.857772  0.532657 -1.379181  7.863267\n",
       "476   0.458299 -0.155084   1.339383  0.857772  1.006479 -1.772553  7.935587\n",
       "477  -1.345330 -0.155084   1.339383  0.857772  0.726337  0.403203  7.789455\n",
       "478  -0.443515 -0.636761  -0.909197 -0.019637 -0.831444  0.403203  7.598399\n",
       "479   0.458299 -0.155084   0.791788 -0.019637  0.102669 -0.411353  7.937375\n",
       "480  -0.443515  0.326593  -0.909197 -0.019637 -0.640334  0.403203  7.783224\n",
       "481  -1.345330 -0.155084   1.786801  1.480304  1.635376 -0.153504  7.781139\n",
       "482  -1.345330 -0.155084   1.339383 -0.019637 -0.160267  0.403203  7.342779\n",
       "483   1.360114  1.771624  -0.909197  0.217293 -1.259247  1.029371  7.575585\n",
       "484   1.360114  0.808270  -0.909197 -0.019637 -0.640334  0.277334  8.070906\n",
       "485  -0.443515 -2.081792  -0.909197  0.857772 -0.248091  0.403203  7.673223\n",
       "486   0.458299 -0.155084  -0.909197  0.217293 -1.446442  0.320519  7.244228\n",
       "487  -0.443515 -0.155084   0.791788  1.480304  1.631637  0.403203  8.294050\n",
       "488   1.360114  1.289947  -0.909197 -0.019637  0.231976  0.403203  7.989560\n",
       "489   0.458299 -0.155084   2.165087  0.857772  1.435922  0.423156  8.389360\n",
       "490   1.360114  0.808270  -0.909197  0.217293  0.109323  0.403203  7.696213\n",
       "491  -1.345330 -0.155084   2.165087  0.857772  1.911887  0.403203  8.188689\n",
       "492   0.458299 -1.118438   0.791788  0.857772  0.231976  0.573584  8.039157\n",
       "493   0.458299 -0.155084   1.786801  1.963178  0.109323  0.403203  8.131531\n",
       "494   1.360114 -0.155084   0.791788  0.857772  0.586541  0.139461  8.411833\n",
       "495  -0.443515 -1.600115  -0.909197 -1.519579 -0.447544  0.403203  7.299797\n",
       "496   0.458299 -0.155084   1.339383  1.480304  1.163231  0.299088  8.156223\n",
       "497   0.458299 -0.155084   0.791788 -0.019637 -0.270619  0.403203  7.598399\n",
       "498   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.693937\n",
       "499  -0.443515 -0.155084   1.339383  0.857772  0.563613  0.403203  8.055158\n",
       "500   1.360114  2.253301   0.085816  1.480304  1.043679  1.055204  8.612503\n",
       "501   1.360114  1.771624   0.791788  0.857772  0.712043  0.403203  8.665613\n",
       "502   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.740230\n",
       "503   1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.070906\n",
       "504   0.458299 -0.155084  -0.909197  0.217293 -1.926509  0.403203  7.408531\n",
       "505  -0.443515 -0.155084  -0.909197 -0.019637 -0.509907  0.403203  7.408531\n",
       "506  -1.345330 -0.155084   0.791788  0.857772  1.198983 -0.599484  7.575585\n",
       "507   0.458299 -0.155084   0.791788 -0.019637  0.364089  0.403203  7.919356\n",
       "508  -1.345330 -0.155084   0.791788  1.963178  0.681710  0.362451  7.408531\n",
       "509  -1.345330 -0.155084   0.791788  0.857772  0.850889  0.462254  7.882315\n",
       "510  -1.345330 -0.155084   1.339383  1.480304  1.166717 -0.559850  7.647309\n",
       "511   0.458299 -0.155084   0.085816 -0.019637  0.109323  0.403203  7.781139\n",
       "512   0.458299 -0.155084  -0.909197 -1.519579 -0.779181  0.403203  7.824046\n",
       "513  -1.345330 -0.155084   0.791788 -0.019637  0.404178 -0.599484  7.718685\n",
       "514   1.360114 -0.155084  -0.909197 -1.519579 -1.446442  0.403203  7.863267\n",
       "515   0.458299 -0.155084  -0.909197 -0.019637 -0.927611  0.660231  7.673223\n",
       "516   1.360114 -0.155084  -0.909197 -1.519579 -0.640334  0.255249  7.740664\n",
       "517   1.360114  0.808270  -0.909197 -1.519579 -1.087045  0.403203  7.919356\n",
       "518   0.458299 -0.155084   0.791788  0.857772  1.335255  1.055204  7.955074\n",
       "519  -0.443515 -0.155084  -0.909197 -0.019637 -0.509907  0.907053  7.824046\n",
       "520   1.360114  1.771624  -0.909197 -0.019637  0.044781  1.269021  8.593228\n",
       "521   0.458299 -0.155084   0.791788  1.480304  0.900074  0.255249  8.101678\n",
       "522  -0.443515 -0.155084  -0.909197 -0.019637 -0.270619  0.403203  7.435438\n",
       "523   1.360114 -0.155084   0.791788 -0.019637  0.485372 -1.965715  8.318742\n",
       "524  -1.345330 -2.563469  -0.909197 -1.519579 -0.927611  0.403203  6.882437\n",
       "525  -0.443515 -1.600115   0.791788  0.857772  1.330956  0.481415  8.160518\n",
       "526  -1.345330 -0.155084   0.791788  0.857772  0.260471  0.403203  7.060476\n",
       "527   1.360114  2.253301  -0.909197  0.857772  0.712043  0.403203  8.365207\n",
       "528   0.458299 -1.118438  -0.909197 -0.019637 -0.509907  0.403203  7.673223\n",
       "529  -0.443515 -0.155084  -0.909197 -1.519579 -0.640334  0.892801  7.207860\n",
       "530  -1.345330 -2.563469  -0.909197 -0.019637 -0.214735  0.232821  6.882437\n",
       "531   0.458299 -0.155084   0.791788  0.857772  0.213973  0.403203  7.882315\n",
       "532  -1.345330 -0.155084  -0.909197 -1.519579 -0.927611  0.186894  6.956545\n",
       "533  -1.345330 -0.155084   0.791788  0.857772  1.811023  0.039658  8.070906\n",
       "534  -0.443515 -1.600115  -0.909197 -0.019637  0.712043  0.403203  7.919356\n",
       "535  -0.443515  0.326593  -0.909197 -1.519579 -1.878161  1.042345  7.467371\n",
       "536  -0.443515 -0.155084   0.791788  1.480304  1.536005  0.725996  8.160518\n",
       "537  -0.443515 -0.155084  -0.909197 -0.019637  0.594129  0.462254  8.037543\n",
       "538   1.360114 -0.155084  -0.909197  0.857772  0.093129  0.788900  8.101678\n",
       "539   0.458299 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.408531\n",
       "540  -1.345330 -2.563469  -0.909197 -0.019637  0.482182  0.608868  7.492760\n",
       "541  -1.345330 -0.155084   0.791788  0.857772  1.868759  0.299088  8.004700\n",
       "542   0.458299 -0.155084   1.339383 -0.019637  0.231976  0.403203  7.989560\n",
       "543  -1.345330 -2.563469   0.085816 -0.019637  0.359025 -1.247584  7.541683\n",
       "544  -1.345330 -0.155084   0.791788  0.857772  0.712043 -0.095634  7.467371\n",
       "545  -0.443515 -0.155084  -0.909197 -1.519579 -0.386938  0.403203  7.313220\n",
       "546   0.458299 -0.155084  -0.909197 -1.519579 -1.761842  0.403203  7.408531\n",
       "547  -1.345330 -0.155084   0.791788  0.857772  0.753239 -2.314574  7.495542\n",
       "548   1.360114  0.808270  -0.909197 -1.519579 -1.651490  0.403203  7.815611\n",
       "549   0.458299 -0.155084   1.339383  0.857772  0.917091 -0.376494  7.972121\n",
       "550  -1.345330 -0.155084   0.791788  0.857772  0.515440 -0.276691  7.279319\n",
       "551  -0.443515 -0.155084   0.791788 -0.019637  0.850889  0.948986  7.822044\n",
       "552   1.360114  0.808270  -0.909197  0.857772  0.890854  0.608868  8.411833\n",
       "553  -0.443515  0.326593  -0.909197 -0.019637 -0.484745  0.819355  7.374629\n",
       "554  -1.345330 -0.155084  -0.909197 -1.519579 -0.927611  0.186894  6.956545\n",
       "555   1.360114  2.253301  -0.909197 -1.519579 -1.587898  0.403203  7.598399\n",
       "556   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.901007\n",
       "557   0.458299 -1.118438  -0.909197 -1.519579 -1.513989  0.403203  7.279319\n",
       "558   1.360114 -0.155084  -0.909197 -1.519579 -0.270619  0.403203  7.970740\n",
       "559  -1.345330 -0.155084   0.791788 -0.019637  1.350232 -0.213787  7.972466\n",
       "560  -0.443515  0.326593   1.339383  0.857772  1.330956  0.403203  8.411833\n",
       "561  -1.345330 -0.155084  -0.909197 -0.019637 -0.474761  0.403203  7.374629\n",
       "562  -0.443515 -0.155084  -0.909197 -0.019637  0.404178  0.442837  7.600402\n",
       "563  -1.345330 -0.155084  -0.909197 -0.019637  0.485372  0.362451  7.313220\n",
       "564   0.458299 -0.155084   2.165087  2.357714  1.330956 -4.194079  8.160518\n",
       "565   0.458299 -0.155084  -0.909197 -1.519579 -0.779181 -4.780613  7.495542\n",
       "566   1.360114 -0.155084  -0.909197  0.857772 -0.160267  0.403203  8.131531\n",
       "567   1.360114 -0.155084  -0.909197 -0.019637  0.109323  0.403203  7.989560\n",
       "568   0.458299 -0.155084   1.786801  1.480304  1.026399  0.403203  8.039157\n",
       "569   0.458299 -0.155084   0.791788  0.857772 -0.055301 -0.682168  7.882315\n",
       "570   0.458299 -0.155084   0.791788 -0.019637 -0.160267  0.403203  7.546974\n",
       "571  -0.443515  0.326593  -0.909197 -0.019637 -0.779181  0.403203  7.374629\n",
       "572   1.360114  2.253301  -0.909197 -0.019637 -0.386938  0.403203  8.131531\n",
       "573   0.458299 -1.118438  -0.909197 -1.519579 -0.160267  0.788900  7.495542\n",
       "574   1.360114  2.253301  -0.909197  0.857772  1.104286  0.935143  8.432724\n",
       "575   1.360114  0.808270   0.791788 -0.019637 -0.640334  0.403203  7.937375\n",
       "576  -0.443515 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.492760\n",
       "577   0.458299 -0.155084  -0.909197  0.217293  0.109323  0.773428  7.166266\n",
       "578  -1.345330 -0.155084   0.791788  0.857772  0.575871 -0.521272  7.495542\n",
       "579  -1.345330 -0.155084   0.791788 -0.019637  0.036944 -2.188705  7.400010\n",
       "580   0.458299 -0.155084   0.791788  0.857772  0.655367  0.163372  7.972466\n",
       "581   1.360114 -0.155084   0.791788  0.857772  0.109323 -0.863225  8.517193\n",
       "582  -0.443515 -0.155084  -0.909197 -1.519579 -0.587211  0.403203  7.393263\n",
       "583   1.360114  2.253301   0.791788  0.857772  0.472585  0.210040  8.516193\n",
       "584   0.458299 -0.155084   1.786801  1.480304  2.234590 -0.376494  8.682708\n",
       "585   0.458299 -1.118438   0.791788 -0.019637 -0.640334  0.403203  7.495542\n",
       "586  -1.345330 -0.155084   0.791788  1.480304  1.296256 -1.311887  7.595890\n",
       "587  -0.443515 -2.081792  -0.909197 -1.519579 -0.667396 -2.775240  7.377759\n",
       "588   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.408531\n",
       "589   0.458299 -0.155084  -0.909197 -1.519579 -0.004678  0.442837  8.292799\n",
       "590   1.360114  2.253301  -0.909197 -0.019637 -0.509907  0.403203  8.214736\n",
       "591  -0.443515 -0.636761   0.791788  1.480304  0.485372  1.154153  7.822044\n",
       "592  -1.345330 -0.155084   0.791788 -0.019637  0.740538  0.320519  7.598399\n",
       "593  -1.345330 -0.155084   0.791788 -0.019637  0.888213  0.481415  7.598399\n",
       "594   0.458299 -0.155084   0.791788 -0.019637 -0.160267  0.403203  7.546974\n",
       "595  -0.443515 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  7.693937\n",
       "596   0.458299 -0.155084  -0.909197 -0.019637  0.109323  0.403203  7.600902\n",
       "597  -1.345330 -0.155084   0.791788  0.857772  0.768661  0.403203  7.130899\n",
       "598  -0.443515  0.326593   0.791788 -0.019637  0.005305  0.403203  7.637716\n",
       "599  -1.345330 -0.155084   0.791788 -0.019637  0.783973 -0.640236  7.522941\n",
       "600   0.458299 -0.155084   1.786801  0.857772  0.782586 -0.244897  8.160518\n",
       "601   0.458299 -0.155084  -0.909197 -0.019637  0.231976  0.403203  7.740664\n",
       "602   1.360114  1.771624  -0.909197  0.217293  0.109323  0.403203  7.647309\n",
       "603  -1.345330 -0.155084   0.791788  0.857772  1.174829  0.277334  7.881937\n",
       "604   0.458299 -1.118438   1.786801  0.857772  1.824748 -0.863225  8.268732\n",
       "605   0.458299 -0.155084   0.791788  0.857772  0.563613  0.299088  8.116716\n",
       "606   1.360114  1.771624  -0.909197 -0.019637  0.044781  0.403203  8.160518\n",
       "607   1.360114 -0.155084  -0.909197 -0.019637  0.221192  0.403203  7.673223\n",
       "608   1.360114 -0.155084  -0.909197 -0.019637  0.469377  0.423156  8.131531\n",
       "609   1.360114 -0.155084  -0.909197 -0.019637  0.231976  1.067949  8.317522\n",
       "610   1.360114  2.253301  -0.909197 -1.519579 -1.087045  0.403203  8.160518\n",
       "611  -0.443515 -1.600115   0.791788  1.480304  1.220605  1.080583  8.202482\n",
       "612  -1.345330 -0.155084  -0.909197  0.857772  0.712043  0.299088  7.495542\n",
       "613  -1.345330 -0.155084   0.791788 -0.019637  0.681710  0.403203  7.696213\n",
       "614   1.360114  1.289947  -0.909197 -0.019637  0.850889  0.804208  8.354674\n",
       "615  -1.345330 -0.155084   0.791788 -0.019637  0.399208 -0.376494  7.313220\n",
       "616  -1.345330 -0.155084  -0.909197 -0.019637 -0.026805  0.382970  7.309881\n",
       "617   1.360114  1.771624  -0.909197 -0.019637 -0.160267  0.403203  8.174703\n",
       "618   1.360114  1.289947   0.791788  1.480304  0.955856  0.299088  8.294050\n",
       "619  -0.443515  0.326593  -0.909197 -1.519579 -0.779181  0.403203  7.377759\n",
       "620  -1.345330 -0.155084  -0.909197  0.857772  0.404178  0.403203  8.242756\n",
       "621  -0.443515 -0.636761  -0.909197 -0.019637  0.231976  0.403203  8.146130\n",
       "622   0.458299 -0.155084   0.791788 -0.019637  0.151609  0.403203  7.546974\n",
       "623  -0.443515 -0.636761  -0.909197 -1.519579 -1.446442  0.403203  7.374629\n",
       "624  -1.345330 -0.155084  -0.909197 -0.019637 -0.618927 -0.483697  6.829794\n",
       "625  -1.345330 -0.155084   0.791788  0.857772  0.606216 -0.599484  7.562681\n",
       "626  -0.443515 -0.636761   1.339383  0.857772  0.754646 -1.186019  8.240121\n",
       "627   0.458299 -0.155084   0.791788  0.857772  0.488557  0.442837  8.006368\n",
       "628  -0.443515 -1.600115   0.791788 -0.019637  0.362402  0.403203  7.863267\n",
       "629   0.458299 -0.155084   1.786801  0.857772  0.228387  0.403203  8.159089\n",
       "630  -1.345330 -0.155084  -0.909197 -0.019637  0.231976  0.741980  7.278629\n",
       "631  -0.443515 -0.636761  -0.909197 -1.519579  0.109323 -3.191392  7.441320\n",
       "632  -1.345330 -0.155084  -0.909197 -1.519579 -1.897370  0.232821  6.796824\n",
       "633   0.458299 -0.155084  -0.909197 -0.019637 -0.055301 -0.411353  7.464510\n",
       "634   0.458299 -1.118438  -0.909197 -1.519579 -1.259247  0.299088  7.467371\n",
       "635   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.803843\n",
       "636   1.360114  1.771624  -0.909197 -0.019637 -0.509907  0.039658  8.187299\n",
       "637   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.882315\n",
       "638   0.458299 -0.155084   0.791788  0.857772  0.404178  0.186894  8.039157\n",
       "639   0.458299 -0.155084   0.791788 -0.019637 -0.924539 -0.599484  7.824046\n",
       "640   1.360114 -0.155084   0.791788 -0.019637 -0.160267  0.403203  8.131531\n",
       "641  -1.345330 -0.155084   0.791788 -0.019637  0.276336 -1.311887  7.578145\n",
       "642  -1.345330 -0.155084  -0.909197  0.857772  0.272820  0.591334  7.313220\n",
       "643   0.458299 -0.155084  -0.909197 -1.519579 -0.574132  0.403203  7.309881\n",
       "644   0.458299 -0.155084  -0.909197  0.217293  0.109323  0.403203  7.170120\n",
       "645  -1.345330 -0.155084   1.339383  0.857772  0.792280 -0.067544  7.346010\n",
       "646   1.360114 -0.155084   0.791788 -0.019637  0.044781  0.403203  8.130059\n",
       "647   0.458299 -0.155084   0.791788 -0.019637 -0.339653  0.403203  7.781139\n",
       "648  -1.345330 -0.155084   0.791788  0.857772  0.864294 -0.039990  7.368340\n",
       "649  -0.443515 -0.155084  -0.909197 -1.519579 -1.761842  0.403203  6.998510\n",
       "650  -0.443515 -0.155084   0.791788 -0.019637  0.093129 -0.376494  7.647309\n",
       "651  -1.345330 -0.155084  -0.909197 -1.519579 -1.317396 -0.599484  6.956545\n",
       "652  -0.443515 -2.081792  -0.909197 -1.519579 -1.087045  0.403203  7.313220\n",
       "653  -0.443515 -0.155084  -0.909197  0.217293 -2.131557  1.042345  7.150701\n",
       "654   1.360114 -0.155084  -0.909197  0.217293 -1.259247  0.139461  7.600902\n",
       "655  -0.443515 -0.155084  -0.909197  0.217293 -1.426972 -0.599484  7.762171\n",
       "656  -1.345330 -0.155084   0.791788  0.857772  0.760263 -1.772553  7.309881\n",
       "657   0.458299 -0.155084   0.791788  1.480304  1.148060  0.403203  8.229511\n",
       "658  -0.443515 -0.155084   0.791788 -0.019637 -0.160267  0.537418  7.696213\n",
       "659   1.360114  2.253301  -0.909197  0.217293 -1.807618  0.403203  7.935587\n",
       "660  -1.345330 -0.155084   0.791788 -0.019637  0.993934 -0.447073  8.281471\n",
       "661   0.458299 -0.155084   1.786801  0.857772  1.355556 -0.483697  8.160518\n",
       "662   0.458299 -0.155084   0.791788  0.857772  0.771453  0.255249  8.242756\n",
       "663   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.718685\n",
       "664   0.458299 -0.155084  -0.909197 -0.019637 -0.530251  0.442837  7.803843\n",
       "665  -1.345330 -0.155084  -0.909197  0.857772 -0.160267  0.299088  7.158514\n",
       "666   0.458299 -0.155084  -0.909197 -1.519579 -0.927611  0.442837  7.600902\n",
       "667   1.360114  2.253301  -0.909197 -1.519579 -2.625112  1.093107  7.625595\n",
       "668   0.458299 -0.155084   0.791788  1.480304  1.573809  0.320519  8.444622\n",
       "669   0.458299 -0.155084   0.791788 -0.019637  0.109323  0.403203  7.492760\n",
       "670  -0.443515 -0.636761   0.791788  0.857772  0.981316 -3.777927  8.411833\n",
       "671  -1.345330 -0.155084   0.791788  0.857772 -0.234687  0.403203  7.025538\n",
       "672   1.360114  1.771624  -0.909197 -1.519579 -1.259247  0.139461  7.997999\n",
       "673  -0.443515 -2.081792  -0.909197 -0.019637 -0.386938  0.403203  7.598399\n",
       "674  -1.345330 -0.155084   0.791788 -0.019637  0.073920 -1.602171  7.740664\n",
       "675   0.458299 -1.118438   0.791788  0.857772  0.109323  0.403203  7.882315\n",
       "676  -0.443515 -0.155084   1.786801  1.963178  1.895401 -2.968402  8.101678\n",
       "677   1.360114  2.253301  -0.909197  0.857772  0.485372  0.403203  8.389360\n",
       "678   1.360114  1.771624  -0.909197 -0.019637 -0.640334  0.403203  8.101678\n",
       "679   1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  8.031060\n",
       "680  -1.345330 -0.155084  -0.909197 -1.519579 -1.005851  0.403203  6.989335\n",
       "681  -0.443515 -2.081792  -0.909197 -1.519579 -1.321083  0.403203  7.492760\n",
       "682   1.360114  1.771624   0.791788 -0.019637  0.981316  0.403203  8.389360\n",
       "683   0.458299 -0.155084  -0.909197 -1.519579 -1.277251  0.442837  7.244228\n",
       "684  -1.345330 -0.155084  -0.909197 -0.019637 -0.365531  0.403203  7.069023\n",
       "685   1.360114  1.289947  -0.909197 -1.519579  0.109323  0.382970  7.495542\n",
       "686   0.458299 -0.155084   1.339383  0.857772  0.231976 -1.865912  7.824046\n",
       "687  -0.443515 -0.636761   0.791788  1.480304  1.220605  1.080583  8.611594\n",
       "688  -1.345330 -0.155084  -0.909197  0.857772 -0.016719  0.382970  7.244228\n",
       "689   1.360114  2.253301  -0.909197 -0.019637 -0.509907  0.403203  8.159089\n",
       "690   1.360114  1.771624  -0.909197  0.857772  0.109323  0.403203  8.366370\n",
       "691  -1.345330 -0.155084  -0.909197 -0.019637 -0.512440  0.403203  7.090077\n",
       "692   0.458299 -0.155084   1.786801  1.480304  1.365106  0.403203  8.410721\n",
       "693   0.458299 -0.155084   1.786801  0.857772  1.569962 -0.682168  8.292799\n",
       "694  -0.443515 -0.155084   1.786801  0.857772  1.435922 -2.188705  8.003029\n",
       "695  -0.443515 -0.155084  -0.909197 -1.519579 -1.161122  0.403203  7.377759\n",
       "696   1.360114  2.253301   0.791788  1.480304  0.109323  0.403203  8.432724\n",
       "697   0.458299 -0.155084   0.791788 -0.019637  0.627205 -0.815793  7.989560\n",
       "698   1.360114  2.253301   1.786801  0.857772  0.712043  1.055204  8.621553\n",
       "699   0.458299 -0.155084   0.791788 -0.019637  0.149747  0.403203  7.762171\n",
       "700  -0.443515 -0.155084   1.786801  1.963178  2.016071  0.878407  8.377931\n",
       "701   1.360114  0.808270  -0.909197 -1.519579 -1.651490 -0.863225  7.649693\n",
       "702   1.360114  0.808270  -0.909197 -1.519579 -0.509907  0.403203  7.824046\n",
       "703   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  8.495970\n",
       "704   1.360114  1.289947  -0.909197 -0.019637 -0.375019  0.362451  8.229511\n",
       "705  -1.345330 -0.155084   0.791788  0.857772  0.404178  0.423156  7.919356\n",
       "706   1.360114  1.771624  -0.909197  0.217293 -3.050076  0.403203  7.647309\n",
       "707   0.458299 -0.155084   0.085816 -1.519579  0.109323  0.403203  7.495542\n",
       "708   0.458299 -0.155084   0.791788  1.480304  1.622263  0.403203  8.556414\n",
       "709  -1.345330 -0.155084   0.791788 -0.019637  1.350232 -0.213787  7.972466\n",
       "710  -1.345330 -0.155084   0.791788 -0.019637 -0.430401 -0.559850  7.312553\n",
       "711   1.360114 -0.155084  -0.909197 -1.519579  0.109323  0.403203  8.242756\n",
       "712  -1.345330 -2.563469   0.791788  0.857772  1.125693  0.423156  8.131531\n",
       "713   1.360114 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.781139\n",
       "714  -1.345330 -0.155084   1.339383  0.857772  1.847712 -0.411353  7.972466\n",
       "715   1.360114  1.289947  -0.909197 -0.019637  0.712043  0.403203  8.187299\n",
       "716   0.458299 -1.118438  -0.909197 -0.019637 -0.640334 -2.314574  7.588324\n",
       "717   0.458299 -0.155084   0.791788  1.480304  0.810168  0.186894  8.084871\n",
       "718   1.360114  1.771624  -0.909197  0.857772  0.404178  1.154153  8.556414\n",
       "719   0.458299 -0.155084   1.786801  0.857772  0.691869 -0.376494  7.901007\n",
       "720   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.408531\n",
       "721   0.458299 -0.155084  -0.909197 -1.519579 -1.223687 -0.682168  7.549609\n",
       "722   0.458299 -0.155084   0.791788  1.480304  0.594129  0.232821  8.070906\n",
       "723  -0.443515 -2.081792   0.791788 -0.019637 -0.293385  0.403203  8.006368\n",
       "724  -0.443515 -0.155084   0.085816 -0.019637  0.231976 -3.191392  7.824046\n",
       "725  -1.345330 -0.155084  -0.909197 -1.519579 -2.026591  0.819355  6.388561\n",
       "726   1.360114  0.808270  -0.909197  0.857772  0.269299  0.573584  8.294050\n",
       "727   0.458299 -0.155084  -0.909197 -1.519579 -2.131557  0.090420  7.419381\n",
       "728  -0.443515 -2.081792   0.791788 -0.019637 -0.293385  0.403203  8.006368\n",
       "729  -1.345330 -0.155084   0.791788 -0.019637  0.350558  0.139461  7.309881\n",
       "730  -0.443515 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  7.536364\n",
       "731   0.458299 -0.155084   0.791788 -0.019637 -0.418240 -0.599484  7.740664\n",
       "732  -0.443515 -0.636761  -0.909197 -1.519579 -1.485918  0.773428  7.244228\n",
       "733  -1.345330 -0.155084   0.791788 -0.019637  0.993934 -0.447073  8.281471\n",
       "734  -1.345330 -0.155084   0.791788 -0.019637  1.446143  0.725996  8.465900\n",
       "735  -1.345330 -0.155084   1.339383  1.480304  1.729465 -0.682168  7.882315\n",
       "736  -0.443515 -0.155084  -0.909197 -1.519579 -1.266431 -0.411353  7.279319\n",
       "737  -1.345330 -0.155084   0.791788  1.480304  1.378826  0.090420  7.481556\n",
       "738  -1.345330 -0.155084  -0.909197 -0.019637 -0.353729 -0.447073  7.090077\n",
       "739  -0.443515 -0.636761  -0.909197 -1.519579 -1.505931  1.055204  7.346010\n",
       "740  -1.345330 -2.563469   0.791788 -0.019637  0.134794  0.403203  7.207860\n",
       "741   1.360114 -0.155084  -0.909197 -0.019637  0.404178  0.403203  8.116716\n",
       "742  -1.345330 -0.155084  -0.909197 -0.019637 -0.111346  0.341637  7.170120\n",
       "743  -1.345330 -2.563469  -0.909197 -0.019637 -0.703093  0.382970  6.902743\n",
       "744  -0.443515 -0.636761  -0.909197 -1.519579 -1.259247  0.403203  7.492760\n",
       "745  -0.443515 -2.081792  -0.909197 -0.019637 -0.270619  0.403203  7.309881\n",
       "746  -1.345330 -0.155084   0.791788 -0.019637 -0.401328 -0.521272  6.856462\n",
       "747  -0.443515  0.326593   0.791788 -0.019637  0.981316  1.016280  7.937375\n",
       "748  -0.443515 -0.155084   1.786801  1.963178  2.016071  0.878407  8.377931\n",
       "749  -0.443515 -0.636761  -0.909197 -1.519579 -1.259247  0.403203  7.435438\n",
       "750  -1.345330 -0.155084   0.085816  0.857772  0.093129  0.403203  7.536364\n",
       "751  -1.345330 -0.155084   0.791788 -0.019637  0.594129  0.403203  7.207860\n",
       "752   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.313220\n",
       "753  -1.345330 -0.155084   0.791788 -1.519579  0.978784  0.403203  6.214608\n",
       "754  -1.345330 -2.563469   0.791788  0.857772  0.749014  0.773428  8.116716\n",
       "755   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.989560\n",
       "756   0.458299 -0.155084   1.786801 -0.019637  0.801931 -0.769866  8.342840\n",
       "757  -1.345330 -0.155084   0.791788  0.857772  0.703420  0.481415  7.279319\n",
       "758  -0.443515  0.326593  -0.909197 -1.519579 -1.188704 -0.863225  7.495542\n",
       "759   0.458299 -0.155084  -0.909197  0.217293  0.109323  0.403203  6.956545\n",
       "760  -1.345330 -2.563469  -0.909197 -0.019637  0.606216  0.643312  7.279319\n",
       "761  -1.345330 -2.563469  -0.909197 -1.519579 -1.259247  0.403203  7.085901\n",
       "762  -1.345330 -0.155084   0.791788  0.857772  1.513394 -2.188705  7.843849\n",
       "763   0.458299 -0.155084   0.791788 -0.019637 -0.435285 -0.725353  7.693937\n",
       "764  -1.345330 -0.155084   0.791788 -0.019637  0.563613  0.090420  7.279319\n",
       "765  -0.443515 -1.600115  -0.909197 -1.519579 -1.567112  0.403203  7.047517\n",
       "766  -0.443515 -1.600115  -0.909197 -0.019637 -0.779181  0.403203  7.313220\n",
       "767   1.360114  2.253301   0.791788  0.857772  1.615677  1.154153  8.779557\n",
       "768   0.458299 -0.155084  -0.909197 -1.519579 -1.446442  0.403203  7.034388\n",
       "769   0.458299 -1.118438  -0.909197 -0.019637  0.044781  0.013593  7.575585\n",
       "770  -1.345330 -0.155084  -0.909197 -0.019637  0.330101  0.403203  7.377759\n",
       "771   1.360114  2.253301  -0.909197  0.217293 -1.878161  0.403203  7.647309\n",
       "772  -0.443515 -1.600115   0.791788 -0.019637  0.231976  0.403203  8.004700\n",
       "773  -1.345330 -0.155084   0.791788  0.857772  0.606216 -0.599484  7.562681\n",
       "774  -0.443515 -0.155084   0.085816  0.857772  0.231976 -0.725353  7.997999\n",
       "775   0.458299 -0.155084   0.791788  0.857772  0.485372 -1.449760  7.878913\n",
       "776   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.600902\n",
       "777  -0.443515 -2.081792  -0.909197  0.217293 -3.066626 -4.194079  7.047517\n",
       "778  -0.443515 -0.155084   1.786801  0.857772  1.435922 -2.188705  8.003029\n",
       "779  -1.345330 -0.155084   0.791788  0.857772  0.754646  0.362451  7.696213\n",
       "780  -0.443515 -0.155084   0.791788 -0.019637  1.373559  0.892801  7.972466\n",
       "781   0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.403203  7.598399\n",
       "782   1.360114 -0.155084   0.210938  0.217293 -1.288125 -0.153504  7.822044\n",
       "783   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.279319\n",
       "784  -1.345330 -0.155084   0.791788 -0.019637 -0.092502  0.403203  7.090077\n",
       "785   1.360114  0.808270  -0.909197 -1.519579 -2.131557  0.403203  7.718685\n",
       "786  -1.345330 -2.563469  -0.909197  0.217293 -2.078434  0.403203  6.961296\n",
       "787  -1.345330 -0.155084  -0.909197 -0.019637 -0.245851  0.403203  7.673223\n",
       "788   1.360114 -0.155084   0.791788  0.857772  0.443534  0.423156  8.411833\n",
       "789  -1.345330 -0.155084   1.786801  1.480304  1.683391  0.065262  8.070906\n",
       "790   1.360114  1.289947  -0.909197 -0.019637  0.109323 -0.244897  7.979339\n",
       "791   0.458299 -0.155084   0.791788 -0.019637 -0.300262  0.090420  7.600402\n",
       "792  -0.443515  0.326593  -0.909197 -1.519579  0.109323  0.403203  6.998510\n",
       "793   1.360114  1.289947  -0.909197 -0.019637 -0.353729  0.725996  8.229511\n",
       "794  -1.345330 -0.155084   0.791788  0.857772  0.713476 -0.599484  7.600402\n",
       "795  -1.345330 -0.155084   1.339383  1.480304  1.772821 -0.124280  7.822044\n",
       "796  -1.345330 -0.155084   0.791788  0.857772  0.569751 -1.865912  7.264730\n",
       "797   1.360114 -0.155084   1.339383  0.857772  0.712043  0.741980  8.242756\n",
       "798   0.458299 -1.118438   0.791788  0.857772  0.109323  0.403203  7.861342\n",
       "799  -1.345330 -0.155084  -0.909197  1.480304 -0.045081  0.186894  7.329750\n",
       "800  -1.345330 -0.155084   0.791788 -0.019637  0.538884 -0.640236  7.549609\n",
       "801   0.458299 -1.118438   0.791788  0.857772  0.136669  0.403203  7.762171\n",
       "802   1.360114 -0.155084  -0.909197 -1.519579 -1.802996  0.403203  7.598399\n",
       "803   0.458299 -0.155084  -0.909197  0.857772  0.494912  0.362451  7.901007\n",
       "804  -1.345330 -0.155084   1.339383  1.480304  1.408082 -0.039990  7.970740\n",
       "805  -1.345330 -0.155084   0.791788  1.480304  0.996449 -3.455133  7.467371\n",
       "806  -1.345330 -0.155084   1.339383 -0.019637  0.060371 -0.483697  7.522941\n",
       "807  -1.345330 -0.155084   0.085816 -0.019637  0.319800 -0.483697  7.207860\n",
       "808   0.458299 -0.155084   1.786801  1.480304  1.940437 -1.772553  8.556414\n",
       "809  -0.443515 -0.636761  -0.909197  0.217293  0.109323  0.403203  7.146772\n",
       "810  -1.345330 -0.155084  -0.909197 -1.519579 -0.831444 -0.483697  6.684612\n",
       "811   0.458299 -0.155084   0.791788  0.857772  0.927497 -0.342456  8.268732\n",
       "812   0.458299 -0.155084   0.791788  0.857772  0.397548  0.277334  8.160518\n",
       "813   1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.281471\n",
       "814   0.458299 -0.155084  -0.909197  0.857772  0.015242  0.382970  8.086410\n",
       "815   0.458299 -0.155084   0.791788  0.857772  0.319800  0.382970  8.094073\n",
       "816  -1.345330 -0.155084  -0.909197 -0.019637 -0.711415  0.403203  6.802395\n",
       "817   0.458299 -0.155084   1.339383  0.857772  1.258763 -0.725353  8.444622\n",
       "818   0.458299 -0.155084   0.791788  0.857772  1.141022 -0.153504  8.160518\n",
       "819   0.458299 -0.155084   0.791788  0.857772  0.661249  0.403203  8.330864\n",
       "820   1.360114  1.289947  -0.909197 -0.019637  0.231976  0.403203  8.101678\n",
       "821  -1.345330 -0.155084   0.791788  0.857772  0.683165 -0.153504  7.509335\n",
       "822   1.360114  0.808270  -0.909197  0.217293 -1.878161  0.403203  7.549609\n",
       "823  -0.443515 -2.081792  -0.909197 -1.519579 -1.277251 -3.191392  7.240650\n",
       "824   1.360114  2.253301  -0.909197 -1.519579 -0.927611  0.403203  7.863267\n",
       "825  -1.345330 -0.155084  -0.909197 -1.519579 -2.001131  0.362451  6.956545\n",
       "826  -1.345330 -0.155084   1.339383 -0.019637  1.145716 -1.449760  7.937375\n",
       "827  -1.345330 -0.155084   1.339383  0.857772  1.311506 -0.067544  7.575585\n",
       "828  -1.345330 -2.563469   0.791788 -0.019637  0.036944  0.573584  7.696213\n",
       "829  -0.443515 -0.155084   0.085816  0.857772  0.531097  0.403203  7.575585\n",
       "830  -0.443515 -0.636761   1.786801  0.857772  0.469377 -1.772553  8.294050\n",
       "831   1.360114  0.808270   0.791788  0.857772 -0.459873 -1.523959  8.294050\n",
       "832   1.360114  1.771624   0.085816  0.857772  1.248728  1.269021  8.594154\n",
       "833   0.458299 -0.155084   1.786801 -0.019637  0.466163 -0.411353  8.070906\n",
       "834  -0.443515 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.495542\n",
       "835   1.360114  1.289947   1.786801  0.857772  0.109323  0.403203  8.160518\n",
       "836   0.458299 -0.155084   1.339383 -0.019637  0.830625 -0.183332  7.972466\n",
       "837   0.458299 -0.155084   1.339383 -0.019637  0.740538 -0.411353  7.783224\n",
       "838  -1.345330 -0.155084  -0.909197 -0.019637  0.151609  0.299088  7.377759\n",
       "839   1.360114  1.289947  -0.909197 -1.519579 -1.087045  0.804208  8.005701\n",
       "840   0.458299 -0.155084  -0.909197 -0.019637 -0.160267  0.849182  7.600902\n",
       "841  -1.345330 -0.155084   0.791788  0.857772  0.569751 -1.865912  7.264730\n",
       "842   0.458299 -0.155084  -0.909197  0.217293  0.109323  0.403203  7.003065\n",
       "843  -0.443515 -0.155084  -0.909197 -0.019637 -1.018638 -0.725353  7.522941\n",
       "844  -1.345330 -0.155084  -0.909197 -1.519579 -1.171424  0.403203  6.774224\n",
       "845  -0.443515 -1.600115  -0.909197 -1.519579 -1.651490  0.403203  7.240650\n",
       "846   0.458299 -1.118438  -0.909197 -1.519579 -1.054199  0.403203  7.522941\n",
       "847  -0.443515  0.326593   1.786801  1.480304  1.723199  0.725996  8.612503\n",
       "848  -1.345330 -0.155084   0.085816 -0.019637  0.357334  0.277334  7.309881\n",
       "849   1.360114  2.253301  -0.909197 -1.519579 -0.160267  0.403203  8.131531\n",
       "850   0.458299 -1.118438  -2.610182  0.217293 -4.010725  0.403203  6.956545\n",
       "851   1.360114  2.253301  -0.909197 -0.019637 -0.509907  0.403203  8.131531\n",
       "852  -1.345330 -0.155084   1.339383  0.857772  1.543814  0.186894  8.006368\n",
       "853   0.458299 -0.155084   0.791788  0.857772  0.407485 -0.521272  7.987864\n",
       "854   0.458299 -0.155084   0.791788  0.857772  0.557457  0.255249  8.006368\n",
       "855   0.458299 -0.155084  -0.909197  0.217293 -2.131557  0.403203  7.374629\n",
       "856   0.458299 -0.155084   0.791788  1.480304  1.092300  0.186894  8.116716\n",
       "857  -1.345330 -0.155084   2.165087  0.857772  1.911887  0.403203  8.188689\n",
       "858  -1.345330 -0.155084   0.791788 -0.019637  0.206729  0.725996  7.492760\n",
       "859   1.360114  2.253301   0.085816 -0.019637  0.109323  0.403203  8.612503\n",
       "860  -1.345330 -0.155084  -0.909197 -0.019637  0.159041  0.382970  7.279319\n",
       "861  -1.345330 -0.155084   0.791788 -0.019637  0.704859 -0.039990  7.313220\n",
       "862  -0.443515  0.326593   0.791788 -0.019637  0.823828 -2.072918  8.037543\n",
       "863  -0.443515 -2.081792  -0.909197 -1.519579  0.109323  0.403203  7.492760\n",
       "864   0.458299 -0.155084   2.165087  0.857772  0.712043  0.403203  8.187299\n",
       "865  -0.443515 -0.636761  -0.909197 -1.519579 -1.021847  0.090420  7.522941\n",
       "866  -0.443515  0.326593   0.085816  0.857772  0.993934  0.163372  8.037543\n",
       "867   0.458299 -0.155084   0.791788  0.857772  0.884245  0.626192  8.732305\n",
       "868   1.360114  0.808270  -0.909197 -1.519579 -1.332183  0.403203  7.901007\n",
       "869  -1.345330 -0.155084   1.339383  0.857772  1.335255 -0.124280  7.598399\n",
       "870  -1.345330 -0.155084  -0.909197 -1.519579 -0.401328  1.042345  7.130899\n",
       "871  -0.443515 -0.636761   0.791788 -0.019637 -0.386938  0.403203  7.495542\n",
       "872   1.360114  2.253301   0.085816  0.857772  0.712043  0.403203  8.486734\n",
       "873  -0.443515 -2.081792   0.085816 -0.019637  0.117847 -3.455133  8.242756\n",
       "874   0.458299 -0.155084  -0.909197 -0.019637 -0.694802  0.403203  7.673223\n",
       "875   1.360114  1.771624   0.085816 -0.019637 -0.293385  1.154153  8.428362\n",
       "876  -0.443515 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.600902\n",
       "877  -0.443515 -0.636761   2.165087  1.480304  1.197839 -1.965715  8.318742\n",
       "878  -0.443515 -0.636761  -0.909197 -1.519579 -1.878161  0.403203  7.166266\n",
       "879   0.458299 -0.155084   0.791788  1.480304  0.504411  0.299088  7.989560\n",
       "880  -1.345330 -0.155084  -0.909197 -1.519579 -0.243613  0.210040  7.346010\n",
       "881   0.458299 -0.155084   1.339383  0.857772  1.211527  0.382970  8.691146\n",
       "882   0.458299 -0.155084   0.791788  0.857772  0.333524  0.277334  8.070906\n",
       "883  -0.443515 -0.155084  -0.909197 -0.019637 -0.509907  0.907053  7.824046\n",
       "884   0.458299 -0.155084  -0.909197 -0.019637 -0.327992  0.403203  7.467371\n",
       "885  -0.443515 -2.081792  -0.909197 -0.019637 -0.386938  0.403203  7.575585\n",
       "886   0.458299 -0.155084  -0.909197  0.857772  0.109323 -0.411353  7.598399\n",
       "887  -1.345330 -0.155084   1.339383 -0.019637  0.075849 -0.213787  7.495542\n",
       "888   1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  7.863267\n",
       "889  -1.345330 -0.155084   0.791788  1.480304  1.378826  0.090420  7.481556\n",
       "890  -1.345330 -2.563469   0.791788 -0.019637  0.974979  0.423156  8.070906\n",
       "891  -0.443515 -0.155084  -0.909197 -1.519579 -1.171424  0.403203  7.362011\n",
       "892  -0.443515 -0.155084   1.339383  0.857772  1.180604 -1.965715  7.989560\n",
       "893  -0.443515 -0.155084  -0.909197  0.217293 -3.058335  0.403203  6.796824\n",
       "894   1.360114  2.253301  -0.909197 -1.519579  0.109323  1.055204  7.937375\n",
       "895   0.458299 -0.155084   0.791788 -0.019637 -0.160267  0.403203  8.698681\n",
       "896   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.261927\n",
       "897  -0.443515 -0.636761  -0.909197 -0.019637 -0.613608 -1.523959  7.637716\n",
       "898  -0.443515 -0.636761  -0.909197 -0.019637 -0.316394  0.403203  7.696213\n",
       "899   1.360114  1.771624  -0.909197  0.217293 -1.259247  0.403203  7.649693\n",
       "900   1.360114  1.289947   0.085816 -0.019637  0.782586  0.804208  8.242756\n",
       "901  -0.443515 -0.155084  -0.909197  0.857772  0.044781 -0.012950  7.863267\n",
       "902  -0.443515 -1.600115   0.791788  0.857772  0.850889  0.481415  7.783224\n",
       "903   1.360114  2.253301  -0.909197 -1.519579  0.109323  1.105524  7.901007\n",
       "904   1.360114 -0.155084   2.492774  1.480304  0.109323  0.403203  8.699515\n",
       "905   1.360114 -0.155084  -0.909197  0.857772  0.469377  0.403203  8.476371\n",
       "906  -0.443515 -0.155084  -0.909197 -1.519579 -0.509907  0.403203  7.309881\n",
       "907  -0.443515 -0.155084   1.786801  1.963178  1.895401 -2.968402  8.101678\n",
       "908   1.360114  2.253301  -0.909197  0.857772  0.109323  0.403203  8.341649\n",
       "909   0.458299 -0.155084   1.339383 -0.019637  0.729185  0.403203  7.937375\n",
       "910   1.360114  2.253301   0.791788  0.857772  1.220605  0.403203  8.748305\n",
       "911   0.458299 -0.155084  -0.909197 -0.019637 -0.403736 -0.769866  7.600902\n",
       "912  -1.345330 -2.563469   1.339383 -0.019637  0.390898 -0.483697  7.598399\n",
       "913   0.458299 -0.155084   0.791788 -0.019637  0.044781  0.403203  7.781139\n",
       "914   0.458299 -0.155084  -0.909197 -1.519579 -1.080436  0.403203  7.824046\n",
       "915  -0.443515 -0.155084  -0.909197 -0.019637 -0.640334  0.403203  7.342779\n",
       "916   0.458299 -0.155084   0.791788  0.857772  0.231976  0.299088  8.070906\n",
       "917   1.360114  2.253301   0.791788 -0.019637  0.319800  0.403203  8.507143\n",
       "918   0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.362451  7.377759\n",
       "919  -1.345330 -0.155084   1.339383  1.480304  2.119174 -0.483697  7.696213\n",
       "920  -0.443515 -2.081792  -0.909197  0.217293 -3.972421  0.403203  6.796824\n",
       "921  -0.443515 -0.155084  -0.909197 -0.019637 -0.138860  0.573584  7.783224\n",
       "922   1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.004700\n",
       "923   1.360114  2.253301   0.791788  0.857772  0.231976  0.403203  8.366370\n",
       "924   1.360114  1.771624  -0.909197  0.217293 -2.073193  0.403203  7.647309\n",
       "925   0.458299 -0.155084   1.786801 -0.019637  1.018951 -0.447073  8.101678\n",
       "926  -0.443515 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.090077\n",
       "927   0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.403203  7.244228\n",
       "928  -1.345330 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  7.110696\n",
       "929  -1.345330 -0.155084  -0.909197 -0.019637 -0.512440  0.403203  7.090077\n",
       "930  -0.443515 -0.636761  -0.909197  0.217293 -2.431163  0.403203  7.126891\n",
       "931   0.458299 -0.155084   0.791788 -0.019637 -0.425529 -0.124280  7.649693\n",
       "932   1.360114  0.808270  -0.909197 -0.019637 -2.545422  0.403203  7.863267\n",
       "933  -1.345330 -0.155084   0.791788  0.857772  0.703420  0.481415  7.279319\n",
       "934   0.458299 -0.155084   1.786801  1.480304  1.229644  0.382970  8.267449\n",
       "935   0.458299 -0.155084   1.786801  1.480304  2.188875  0.090420  8.160518\n",
       "936   1.360114 -0.155084  -0.909197 -0.019637  0.109323  0.403203  7.937375\n",
       "937   1.360114  1.289947  -0.909197 -0.019637 -0.927611  0.834346  7.822044\n",
       "938  -0.443515 -0.155084   1.339383 -0.019637  0.782586 -1.965715  7.972121\n",
       "939   0.458299 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  7.625595\n",
       "940   1.360114 -0.155084   0.791788  0.857772  0.109323 -0.815793  8.366370\n",
       "941   0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.419381\n",
       "942   0.458299 -0.155084   0.791788  0.857772  0.404178  0.442837  8.779557\n",
       "943  -0.443515 -0.155084  -0.909197 -1.519579 -2.131557  0.403203  7.309881\n",
       "944  -0.443515 -0.155084  -0.909197 -1.519579 -0.867004  0.518990  7.090077\n",
       "945   0.458299 -0.155084   0.791788  1.480304  0.810168  0.186894  8.086103\n",
       "946   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.824046\n",
       "947  -1.345330 -0.155084   0.791788 -0.019637  0.274579 -4.194079  7.901007\n",
       "948   1.360114  0.808270   0.791788  1.480304  1.104286  1.154153  8.465900\n",
       "949  -0.443515 -0.636761   0.791788 -0.019637  0.420664 -3.455133  8.131531\n",
       "950  -0.443515 -0.155084  -0.909197 -0.019637 -0.983656  0.403203  7.309881\n",
       "951   0.458299 -0.155084  -0.909197 -1.519579 -1.651490  0.403203  7.783224\n",
       "952  -1.345330 -0.155084   0.791788  0.857772  0.926199 -3.777927  7.598399\n",
       "953  -0.443515 -0.155084   0.791788 -0.019637 -0.143125  0.403203  7.718685\n",
       "954  -0.443515 -0.155084   0.791788  0.857772  0.858942  0.403203  7.863267\n",
       "955   1.360114  0.808270   1.786801  1.480304  1.460370  0.362451  8.699515\n",
       "956  -0.443515 -0.155084  -0.909197 -1.519579 -0.640334  0.892801  7.207860\n",
       "957  -0.443515 -0.155084   1.786801  0.857772  0.517011 -1.965715  7.822044\n",
       "958  -0.443515 -2.081792  -0.909197 -1.519579 -1.087045  0.403203  7.313220\n",
       "959   1.360114 -0.155084  -0.909197 -0.019637 -0.472273  0.403203  7.989560\n",
       "960   1.360114  0.808270  -0.909197 -0.019637 -0.386938  0.403203  8.086410\n",
       "961  -1.345330 -2.563469   0.085816 -0.019637  0.359025 -1.247584  7.541683\n",
       "962  -0.443515 -2.081792  -0.909197 -1.519579 -0.160267  0.403203  7.522941\n",
       "963  -0.443515 -0.155084   0.791788  0.857772  0.469377  0.921166  7.935587\n",
       "964   0.458299 -1.118438   0.791788  0.857772  0.723486  0.341637  8.292799\n",
       "965  -1.345330 -0.155084  -0.909197  0.857772 -0.171051  0.462254  7.170120\n",
       "966  -1.345330 -2.563469   0.791788  0.857772  1.125693  0.423156  8.131531\n",
       "967   0.458299 -0.155084   0.791788  0.857772  1.351298 -1.602171  8.465900\n",
       "968  -0.443515 -2.081792   0.085816 -0.019637  0.117847 -3.455133  8.242756\n",
       "969  -1.345330 -0.155084   1.339383 -0.019637  1.796343 -0.342456  7.989560\n",
       "970   1.360114  2.253301  -0.909197 -1.519579  0.109323  1.055204  7.823246\n",
       "971   1.360114  2.253301   0.791788  1.480304  0.109323  0.403203  8.629629\n",
       "972  -1.345330 -0.155084   1.786801  0.857772  1.956582 -1.523959  8.202482\n",
       "973   0.458299 -0.155084  -0.909197 -0.019637  2.065566  0.863869  7.522941\n",
       "974  -0.443515 -0.155084   2.165087  0.857772  0.968624 -3.777927  8.006368\n",
       "975   0.458299 -0.155084   1.339383  0.857772  0.231976 -3.777927  8.006368\n",
       "976   1.360114  1.771624  -0.909197 -1.519579 -0.640334  0.555614  8.160518\n",
       "977  -1.345330 -2.563469   0.791788 -0.019637  0.782586  0.709833  7.593374\n",
       "978   0.458299 -0.155084  -0.909197 -1.519579 -0.708637  0.403203  7.699842\n",
       "979  -0.443515 -1.600115  -0.909197 -1.519579 -0.034909 -2.968402  7.150701\n",
       "980   0.458299 -0.155084   0.791788  1.480304  1.291879  0.277334  8.255828\n",
       "981   1.360114  2.253301  -0.909197 -0.019637 -0.160267  0.403203  7.970740\n",
       "982   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.740664\n",
       "983   1.360114 -0.155084  -0.909197 -1.519579  1.085077  0.863869  7.549609\n",
       "984  -0.443515 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.600902\n",
       "985  -0.443515 -0.155084  -0.909197 -0.019637  1.330956  0.921166  7.824046\n",
       "986   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.900266\n",
       "987  -1.345330 -0.155084   0.791788  0.857772  0.703420 -0.067544  7.598399\n",
       "988  -0.443515 -0.155084   0.791788  0.857772  0.485372 -0.039990  7.937375\n",
       "989   1.360114 -0.155084  -0.909197 -1.519579 -1.358305  1.029371  7.863267\n",
       "990  -0.443515 -2.081792   1.339383 -0.019637 -0.316394 -2.072918  8.006368\n",
       "991   0.458299 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  8.098643\n",
       "992   0.458299 -0.155084   1.786801  0.857772  0.939143 -0.376494  7.882315\n",
       "993  -0.443515  0.326593   0.791788  0.857772  0.653894 -4.194079  8.229511\n",
       "994   0.458299 -0.155084   0.791788  1.480304  0.647991  0.163372  7.989560\n",
       "995  -1.345330 -0.155084   1.339383  1.480304  1.094703  0.090420  7.374629\n",
       "996  -1.345330 -0.155084   2.165087  0.857772  1.748153 -1.311887  8.160518\n",
       "997  -0.443515 -2.081792   1.339383  0.857772 -0.138860 -1.965715  7.781139\n",
       "998  -1.345330 -0.155084   0.085816 -0.019637 -0.098765  0.362451  7.244228\n",
       "999  -0.443515 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.522941\n",
       "1000  1.360114  1.289947  -0.909197  0.217293 -2.506658  0.403203  7.408531\n",
       "1001 -1.345330 -0.155084  -0.909197 -1.519579 -1.466089  0.403203  6.840547\n",
       "1002  0.458299 -0.155084   0.791788 -0.019637  0.108374 -1.772553  7.824046\n",
       "1003  0.458299 -0.155084  -0.909197  0.217293  0.109323  0.403203  6.543912\n",
       "1004  0.458299 -0.155084   0.791788  0.857772  0.109323  0.362451  7.374629\n",
       "1005  0.458299 -0.155084   0.791788 -0.019637  0.110272  0.591334  7.901007\n",
       "1006  0.458299 -0.155084   0.791788 -0.019637  0.109323  0.403203  7.575585\n",
       "1007 -0.443515 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.240650\n",
       "1008 -1.345330 -0.155084   0.791788  0.857772  0.833338  0.403203  7.575585\n",
       "1009 -1.345330 -0.155084  -0.909197 -1.519579 -1.466089  0.403203  6.840547\n",
       "1010  1.360114  2.253301  -0.909197  0.857772  0.109323  0.403203  8.737132\n",
       "1011 -1.345330 -2.563469  -0.909197 -1.519579 -1.339614  0.403203  6.882437\n",
       "1012  1.360114  1.771624  -0.909197  0.217293 -1.651490  0.403203  7.598399\n",
       "1013  0.458299 -0.155084  -0.909197 -0.019637 -0.779181  0.403203  7.600902\n",
       "1014  0.458299 -0.155084   0.791788  0.857772  0.563613  0.403203  8.070906\n",
       "1015  1.360114  1.289947  -0.909197  0.857772  0.231976  0.403203  8.188689\n",
       "1016  0.458299 -0.155084   0.791788  0.857772  0.159041  0.232821  8.006368\n",
       "1017  0.458299 -1.118438  -0.909197  0.217293 -1.651490 -0.521272  7.240650\n",
       "1018 -1.345330 -0.155084  -0.909197 -1.519579 -1.070559 -0.447073  6.998510\n",
       "1019 -1.345330 -0.155084  -0.909197 -0.019637 -0.618927 -0.483697  6.829794\n",
       "1020  1.360114  0.808270  -0.909197 -0.019637 -0.927611  0.591334  7.937375\n",
       "1021 -1.345330 -0.155084   0.791788  1.480304  1.296256 -1.311887  7.595890\n",
       "1022 -1.345330 -0.155084  -0.909197 -0.019637 -0.043042 -0.447073  7.085901\n",
       "1023  1.360114  0.808270  -0.909197  0.217293  0.109323  0.403203  7.673223\n",
       "1024 -0.443515 -0.636761  -0.909197 -1.519579 -1.259247  0.403203  7.306531\n",
       "1025  0.458299 -0.155084   1.786801  0.857772  1.013970  0.573584  8.779557\n",
       "1026  0.458299 -1.118438  -0.909197 -1.519579 -1.259247 -0.682168  7.204149\n",
       "1027 -1.345330 -0.155084   1.786801  1.480304  1.275384  0.255249  7.824046\n",
       "1028  0.458299 -0.155084   0.791788  0.857772 -0.270619 -0.863225  7.696213\n",
       "1029  0.458299 -0.155084   2.165087  0.857772  0.639107  0.989737  8.146130\n",
       "1030 -0.443515 -0.155084   0.791788 -0.019637 -0.266094 -0.769866  7.478170\n",
       "1031 -1.345330 -0.155084   0.085816 -0.019637  0.093129  0.232821  7.279319\n",
       "1032 -0.443515 -1.600115   0.791788  2.357714 -5.114004  0.403203  6.476972\n",
       "1033 -0.443515 -0.155084  -0.909197 -1.519579 -1.259247 -0.039990  7.130899\n",
       "1034 -1.345330 -0.155084  -0.909197 -1.519579 -1.171424  0.403203  6.774224\n",
       "1035  0.458299 -0.155084  -0.909197 -1.519579 -0.927611  0.299088  7.413367\n",
       "1036  1.360114  2.253301   0.791788  1.480304  0.231976  0.403203  8.555452\n",
       "1037  0.458299 -1.118438  -0.909197 -0.019637 -0.160267  0.403203  7.824046\n",
       "1038 -1.345330 -0.155084   0.085816 -1.519579 -0.160267  0.403203  7.313220\n",
       "1039 -1.345330 -0.155084  -0.909197 -0.019637 -0.353729 -0.447073  7.090077\n",
       "1040 -0.443515 -2.081792  -0.909197 -0.019637 -0.927611  0.403203  7.522941\n",
       "1041 -0.443515 -0.155084   0.791788  0.857772  0.469377  0.403203  7.899153\n",
       "1042 -1.345330 -2.563469   0.791788 -0.019637  0.155328 -2.072918  7.649693\n",
       "1043  1.360114  0.808270  -0.909197 -0.019637 -0.728157  0.403203  8.037543\n",
       "1044 -1.345330 -0.155084   1.339383 -0.019637  1.145716 -1.449760  7.937375\n",
       "1045  0.458299 -1.118438  -0.909197 -1.519579 -1.407677  0.921166  7.438384\n",
       "1046  0.458299 -0.155084   1.339383  0.857772  0.917091  0.709833  7.972466\n",
       "1047  0.458299 -0.155084   1.339383 -0.019637 -0.160267  0.403203  7.718685\n",
       "1048  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.210040  7.935587\n",
       "1049 -1.345330 -0.155084   0.791788  1.480304  0.212164 -0.153504  7.309881\n",
       "1050 -0.443515 -0.155084   0.791788 -0.019637  0.624219  0.115149  8.242756\n",
       "1051 -0.443515 -0.155084   1.786801  0.857772  0.563613  0.403203  8.292799\n",
       "1052  0.458299 -0.155084  -0.909197 -1.519579 -2.131557  0.255249  7.522941\n",
       "1053  0.458299 -0.155084   1.786801  1.963178  1.672405 -0.067544  8.242493\n",
       "1054  0.458299 -0.155084  -0.909197  0.217293 -1.980977  0.403203  7.351158\n",
       "1055  1.360114  0.808270  -0.909197  0.857772  0.890854  0.709833  8.160518\n",
       "1056  1.360114  2.253301   0.791788 -0.019637 -0.096675 -0.640236  8.611594\n",
       "1057 -1.345330 -0.155084  -0.909197 -1.519579 -0.867004  0.403203  7.090077\n",
       "1058 -0.443515 -2.081792  -0.909197  0.217293 -3.972421  0.403203  6.796824\n",
       "1059  0.458299 -0.155084  -0.909197 -0.019637 -1.171424  0.403203  7.429521\n",
       "1060 -1.345330 -0.155084   0.791788 -0.019637  0.456494 -0.521272  7.492760\n",
       "1061  1.360114  0.808270   0.791788  0.857772  0.480586  0.065262  8.160518\n",
       "1062  1.360114  2.253301  -0.909197 -1.519579 -3.710604  0.403203  7.279319\n",
       "1063 -0.443515 -1.600115  -0.909197 -0.019637 -1.087045  0.403203  7.435438\n",
       "1064  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.824046\n",
       "1065  0.458299 -0.155084   1.786801 -0.019637  0.367459 -0.309200  8.100161\n",
       "1066  1.360114 -0.155084  -0.909197 -1.519579 -1.587898  0.403203  7.901007\n",
       "1067  0.458299 -0.155084   1.786801 -0.019637  0.810168 -0.559850  7.955074\n",
       "1068  0.458299 -0.155084   0.085816 -0.019637  0.109323  0.403203  7.693937\n",
       "1069 -1.345330 -0.155084  -0.909197 -0.019637  0.151609  0.299088  7.377759\n",
       "1070  0.458299 -0.155084  -0.909197 -1.519579  0.109323  1.016280  7.313220\n",
       "1071 -1.345330 -0.155084  -0.909197 -0.019637  0.485372  0.362451  7.313220\n",
       "1072 -0.443515 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  7.536364\n",
       "1073 -1.345330 -0.155084  -0.909197  0.857772  0.712043  0.299088  7.495542\n",
       "1074 -1.345330 -0.155084   2.165087  0.857772  1.532089 -2.775240  8.229511\n",
       "1075 -1.345330 -0.155084   0.791788 -0.019637 -0.059403 -0.559850  7.467371\n",
       "1076  0.458299 -0.155084  -0.909197  0.857772  0.098858  0.500324  7.878913\n",
       "1077  1.360114  2.253301  -0.909197 -0.019637  0.109323  1.093107  7.823246\n",
       "1078  0.458299 -0.155084   0.791788 -0.019637 -0.254824 -0.725353  7.740664\n",
       "1079 -1.345330 -2.563469   0.791788 -0.019637  0.782586  0.709833  7.593374\n",
       "1080  0.458299 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.598399\n",
       "1081  0.458299 -0.155084   0.791788  0.857772  0.621229  0.442837  8.188689\n",
       "1082 -0.443515 -0.155084   0.791788 -0.019637 -0.055301 -1.965715  7.600902\n",
       "1083 -1.345330 -0.155084  -0.909197 -1.519579 -0.689293 -3.777927  7.495542\n",
       "1084 -1.345330 -0.155084  -0.909197 -0.019637  0.821103 -1.186019  7.522941\n",
       "1085 -0.443515 -2.081792   0.085816 -0.019637 -0.160267 -1.965715  7.824046\n",
       "1086  1.360114  0.808270  -0.909197 -0.019637 -0.160267  0.403203  8.100161\n",
       "1087  1.360114  1.289947   1.786801  1.480304  1.536005  0.362451  8.740337\n",
       "1088 -0.443515 -2.081792   0.791788  0.857772  0.969896 -3.455133  8.267449\n",
       "1089  1.360114 -0.155084   1.786801  0.857772  0.868299  0.382970  8.517193\n",
       "1090  0.458299 -0.155084   1.339383  0.857772  0.818374 -0.376494  8.101678\n",
       "1091  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.899153\n",
       "1092  0.458299 -1.118438  -0.909197  0.857772  0.737705  0.834346  7.937375\n",
       "1093 -0.443515 -0.155084   0.085816  0.857772  0.231976  0.773428  7.843849\n",
       "1094  0.458299 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.279319\n",
       "1095 -0.443515  0.326593  -0.909197 -1.519579 -0.386938  0.403203  7.882315\n",
       "1096 -0.443515 -2.081792  -0.909197  0.217293 -2.418834  0.403203  7.130899\n",
       "1097 -1.345330 -0.155084   1.786801  0.857772  1.631637  0.362451  8.455318\n",
       "1098 -1.345330 -0.155084   1.339383 -0.019637 -0.160267  0.403203  7.342779\n",
       "1099 -1.345330 -0.155084   0.791788  0.857772  0.982581  0.232821  7.824046\n",
       "1100 -0.443515 -1.600115  -0.909197 -1.519579 -0.034909 -2.968402  7.150701\n",
       "1101  0.458299 -0.155084  -0.909197 -0.019637 -0.958566 -5.783300  7.740664\n",
       "1102  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.244228\n",
       "1103  0.458299 -1.118438  -0.909197 -1.519579  0.109323  0.403203  7.435438\n",
       "1104 -1.345330 -0.155084   0.791788 -0.019637  0.704859 -0.039990  7.313220\n",
       "1105  1.360114 -0.155084   0.085816  0.857772  0.044781 -0.213787  8.006368\n",
       "1106  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.279319\n",
       "1107  0.458299 -0.155084   0.791788  0.857772  0.159041  0.210040  8.004700\n",
       "1108  0.458299 -0.155084  -0.909197 -0.019637 -0.386938 -1.379181  7.598399\n",
       "1109 -1.345330 -0.155084  -0.909197 -0.019637  0.453261  0.741980  7.598399\n",
       "1110 -0.443515 -0.636761  -0.909197 -0.019637 -0.613608 -1.523959  7.637716\n",
       "1111 -0.443515 -0.636761  -0.909197 -0.019637 -0.831444  0.403203  7.598399\n",
       "1112  1.360114  0.808270   0.085816  0.857772  0.109323  0.403203  8.101678\n",
       "1113  1.360114  2.253301  -0.909197  1.480304  0.404178  0.403203  8.612503\n",
       "1114  0.458299 -0.155084   1.339383 -0.019637  0.237348 -0.244897  7.937375\n",
       "1115  1.360114  1.771624  -0.909197  0.857772  0.404178  0.403203  8.465900\n",
       "1116 -0.443515 -0.155084   1.339383 -0.019637  0.782586 -1.965715  7.972121\n",
       "1117  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.244228\n",
       "1118 -0.443515 -0.155084  -0.909197 -0.019637 -0.509907  0.403203  7.313220\n",
       "1119  1.360114  0.808270  -0.909197  0.217293 -2.418834  0.403203  7.346010\n",
       "1120 -0.443515 -0.636761  -0.909197 -0.019637 -0.852116  0.403203  7.438384\n",
       "1121 -0.443515 -0.155084  -0.909197 -1.519579 -1.259247  0.403203  7.170120\n",
       "1122 -1.345330 -0.155084   1.339383  0.857772  0.792280 -0.067544  7.346010\n",
       "1123  0.458299 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.166266\n",
       "1124 -0.443515 -1.600115   0.791788 -0.019637  0.231976  0.403203  8.004700\n",
       "1125 -0.443515  0.326593  -0.909197 -0.019637  0.639107  0.976280  7.861342\n",
       "1126 -1.345330 -0.155084  -0.909197 -1.519579 -0.640334  0.403203  7.244228\n",
       "1127 -0.443515  0.326593   0.791788 -0.019637  0.683165 -3.455133  8.281471\n",
       "1128 -0.443515 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.522941\n",
       "1129  0.458299 -0.155084   0.791788  0.857772  0.604709  0.403203  8.292799\n",
       "1130  1.360114  2.253301  -0.909197 -1.519579 -1.259247  0.403203  7.824046\n",
       "1131  1.360114  1.771624  -0.909197  0.857772  0.109323  0.403203  8.292799\n",
       "1132  1.360114 -0.155084   0.791788  0.857772  0.109323  0.403203  8.241440\n",
       "1133  1.360114  1.771624  -0.909197  0.217293 -3.142714  0.403203  7.536364\n",
       "1134  1.360114  0.808270  -0.909197 -0.019637 -0.509907  0.403203  7.955074\n",
       "1135  0.458299 -0.155084  -0.909197 -0.019637 -0.214735  0.403203  7.509335\n",
       "1136 -0.443515 -0.155084  -0.909197 -0.019637  0.044781  0.403203  7.522941\n",
       "1137 -0.443515 -0.155084   1.339383  0.857772  0.601691  0.403203  7.813996\n",
       "1138 -0.443515  0.326593   0.791788 -0.019637  0.365775  0.403203  7.901007\n",
       "1139 -1.345330 -0.155084   1.339383 -0.019637  0.981316 -2.775240  7.824046\n",
       "1140  0.458299 -0.155084   0.791788  0.857772  0.810168  0.163372  8.229511\n",
       "1141  1.360114 -0.155084  -0.909197 -1.519579 -0.316394  1.080583  8.098643\n",
       "1142 -1.345330 -0.155084   1.339383  0.857772  1.128058 -1.015637  7.693937\n",
       "1143  1.360114 -0.155084  -0.909197  0.857772  0.857602  0.065262  8.400659\n",
       "1144  1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.822044\n",
       "1145  1.360114  2.253301  -0.909197 -1.519579 -1.259247  0.403203  7.793587\n",
       "1146  1.360114  2.253301  -0.909197  0.217293 -2.186025  0.403203  7.649693\n",
       "1147 -0.443515 -0.155084   0.791788 -0.019637  0.083546  0.403203  7.673223\n",
       "1148 -0.443515 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.240650\n",
       "1149 -0.443515 -2.081792   0.791788  0.857772  0.930090  1.105524  8.101678\n",
       "1150 -0.443515 -0.155084  -0.909197 -1.519579 -0.779181  0.090420  7.130899\n",
       "1151  0.458299 -0.155084  -0.909197  0.217293 -1.926509  0.403203  7.309881\n",
       "1152 -1.345330 -2.563469  -0.909197 -1.519579 -1.087045  0.115149  6.802395\n",
       "1153  1.360114  1.289947  -0.909197  0.217293 -2.750471  0.804208  7.495542\n",
       "1154 -1.345330 -0.155084   0.791788 -0.019637  1.446143  0.725996  8.465900\n",
       "1155 -1.345330 -0.155084   1.786801  0.857772  1.631637  0.362451  8.455318\n",
       "1156 -0.443515 -2.081792  -0.909197 -0.019637 -0.605655 -2.968402  7.647309\n",
       "1157  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  8.004700\n",
       "1158 -0.443515  0.326593  -0.909197 -0.019637  0.044781  0.186894  7.901007\n",
       "1159  1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.188411\n",
       "1160 -0.443515 -0.155084  -0.909197  0.217293  0.242706  0.403203  7.738488\n",
       "1161  0.458299 -0.155084  -0.909197 -0.019637  0.109323  0.403203  7.374629\n",
       "1162 -0.443515  0.326593  -0.909197 -1.519579 -1.171424  0.403203  7.090077\n",
       "1163 -0.443515 -0.155084   0.791788  1.480304  1.911887  0.255249  8.292799\n",
       "1164 -0.443515 -1.600115  -0.909197  1.480304  0.109323  0.403203  7.824046\n",
       "1165  1.360114  2.253301  -0.909197 -1.519579 -1.651490  0.403203  7.760041\n",
       "1166  1.360114  2.253301  -0.909197 -0.019637 -0.136731  1.154153  8.229511\n",
       "1167 -1.345330 -0.155084  -0.909197 -1.519579 -2.346309  0.403203  6.715383\n",
       "1168 -1.345330 -0.155084   0.791788  0.857772  0.754646  0.362451  7.696213\n",
       "1169 -0.443515  0.326593  -0.909197 -1.519579  0.109323  0.403203  7.166266\n",
       "1170  1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.822044\n",
       "1171  0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.403203  7.130899\n",
       "1172 -1.345330 -0.155084   1.339383  1.480304  1.094703  0.090420  7.374629\n",
       "1173  1.360114  1.289947  -0.909197 -0.019637  0.109323  0.403203  7.989560\n",
       "1174  1.360114  0.808270   1.339383 -0.019637  0.563613  0.403203  8.268732\n",
       "1175  0.458299 -0.155084   0.085816 -0.019637  0.228387  0.608868  8.779557\n",
       "1176 -0.443515 -0.155084   1.339383  1.480304  1.068127  0.403203  7.955074\n",
       "1177  0.458299 -0.155084   1.339383 -0.019637 -0.270619  0.403203  7.649693\n",
       "1178 -1.345330 -0.155084  -0.909197 -0.019637  0.021183 -0.483697  6.956545\n",
       "1179  1.360114  0.808270  -0.909197  0.857772  0.109323  1.042345  8.341649\n",
       "1180 -1.345330 -0.155084   0.791788  0.857772  0.151609 -0.213787  7.296413\n",
       "1181 -0.443515  0.326593  -0.909197  0.217293 -1.651490  0.921166  7.204149\n",
       "1182 -1.345330 -0.155084   0.791788  0.857772 -0.212530  0.362451  7.649693\n",
       "1183  1.360114  2.253301  -0.909197 -0.019637 -0.927611  0.186894  8.124151\n",
       "1184  0.458299 -0.155084   0.791788  0.857772  2.028552  0.039658  7.878534\n",
       "1185 -0.443515 -0.155084   0.791788 -0.019637 -0.168890 -0.725353  7.738488\n",
       "1186  1.360114  2.253301  -0.909197 -1.519579 -0.779181  1.105524  7.772753\n",
       "1187  0.458299 -0.155084   1.786801  1.963178  1.870434  0.115149  8.691146\n",
       "1188  0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.423156  7.435438\n",
       "1189 -0.443515 -1.600115   0.791788  0.857772  1.330956  0.481415  8.160518\n",
       "1190  0.458299 -0.155084  -0.909197  0.857772  0.231976  0.591334  8.006368\n",
       "1191 -0.443515 -0.155084   0.791788 -0.019637 -0.184063 -0.815793  7.598399\n",
       "1192  1.360114  0.808270   0.791788  0.857772  0.850889  0.403203  8.255828\n",
       "1193  1.360114 -0.155084  -0.909197 -0.019637 -0.377397  0.403203  8.174703\n",
       "1194  1.360114 -0.155084  -0.909197  0.217293 -0.708637  0.403203  7.972466\n",
       "1195  0.458299 -0.155084   0.791788 -0.019637  0.121624  0.403203  7.693937\n",
       "1196 -0.443515 -0.155084  -0.909197 -0.019637 -0.587211  0.403203  7.265430\n",
       "1197 -1.345330 -0.155084   1.339383 -0.019637  0.399208 -0.599484  7.435438\n",
       "1198  1.360114  1.289947   0.210938  0.217293 -0.736578  0.210040  7.647309\n",
       "1199  1.360114  1.289947  -0.909197 -0.019637  0.109323  0.403203  8.187299\n",
       "1200 -1.345330 -0.155084   2.165087  0.857772  1.532089 -2.775240  8.229511\n",
       "1201 -1.345330 -0.155084   0.791788 -0.019637  1.337401 -0.447073  8.070906\n",
       "1202 -0.443515 -2.081792   1.339383  0.857772  1.018951 -2.968402  8.160518\n",
       "1203 -1.345330 -2.563469   1.339383 -0.019637  1.560314 -3.777927  8.101678\n",
       "1204 -0.443515  0.326593  -0.909197 -0.019637 -0.689293  0.403203  7.435438\n",
       "1205 -0.443515 -0.155084  -0.909197 -1.519579 -0.767737 -0.213787  7.313220\n",
       "1206 -0.443515 -0.155084   2.165087  0.857772  0.968624 -3.777927  8.006368\n",
       "1207  1.360114  2.253301   0.085816  1.480304  0.712043  0.403203  8.433812\n",
       "1208  1.360114 -0.155084  -0.909197 -1.519579 -0.779181  0.403203  7.693937\n",
       "1209  0.458299 -0.155084  -0.909197 -0.019637 -0.722562  0.403203  7.546974\n",
       "1210  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.346010\n",
       "1211 -1.345330 -0.155084  -0.909197  0.217293 -1.546524 -0.309200  6.543912\n",
       "1212  1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.822044\n",
       "1213 -1.345330 -0.155084   0.791788  0.857772  1.811023  0.039658  8.070906\n",
       "1214 -0.443515 -0.636761   0.791788  0.857772  0.981316 -3.777927  8.411833\n",
       "1215 -0.443515 -0.155084  -0.909197 -0.019637  0.319800  0.403203  7.673223\n",
       "1216  0.458299 -0.155084   0.791788  0.857772  0.496498  0.320519  8.107720\n",
       "1217 -0.443515 -1.600115   0.791788  2.357714 -5.114004  0.403203  6.476972\n",
       "1218  0.458299 -1.118438   1.786801  0.857772  0.712043 -5.783300  8.055158\n",
       "1219 -1.345330 -0.155084   0.791788 -0.019637  0.350558  0.139461  7.309881\n",
       "1220 -1.345330 -0.155084   2.165087  1.480304  2.335379  0.341637  8.699515\n",
       "1221 -1.345330 -0.155084   0.791788  0.857772  0.021183 -0.521272  7.084226\n",
       "1222  0.458299 -0.155084   0.791788 -0.019637 -0.384549 -0.276691  7.600402\n",
       "1223  0.458299 -0.155084   1.339383  0.857772  1.330956  0.382970  8.433812\n",
       "1224 -0.443515 -0.155084   1.786801  1.480304  2.130331  0.341637  8.354674\n",
       "1225  1.360114  1.289947  -0.909197 -1.519579  0.109323  0.403203  7.803843\n",
       "1226 -1.345330 -0.155084  -0.909197 -1.519579  0.109323  0.403203  6.907755\n",
       "1227 -1.345330 -0.155084  -0.909197 -1.519579 -1.137274  0.299088  6.956545\n",
       "1228  0.458299 -0.155084  -0.909197 -0.019637  0.563613  0.403203  7.492760\n",
       "1229 -1.345330 -0.155084  -0.909197 -0.019637 -0.006680 -0.521272  7.438384\n",
       "1230  1.360114  2.253301  -0.909197  0.857772  0.231976  1.154153  8.556414\n",
       "1231 -1.345330 -0.155084   1.786801  1.963178  2.164918  0.065262  8.342840\n",
       "1232  1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  8.004700\n",
       "1233 -1.345330 -0.155084  -0.909197 -1.519579 -1.542430  0.403203  6.907755\n",
       "1234 -0.443515  0.326593  -0.909197 -0.019637 -0.779181  0.403203  7.374629\n",
       "1235  0.458299 -0.155084   0.791788  0.857772 -0.151679  0.403203  8.038835\n",
       "1236 -0.443515 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.296413\n",
       "1237  1.360114  2.253301  -0.909197  0.217293 -1.793782  0.403203  7.899153\n",
       "1238 -1.345330 -0.155084   0.791788 -1.519579  0.978784  0.403203  6.214608\n",
       "1239 -0.443515 -0.155084  -0.909197  0.857772 -0.096675  0.423156  7.843849\n",
       "1240  0.458299 -0.155084  -0.909197  0.217293 -1.651490  0.403203  7.170120\n",
       "1241 -1.345330 -0.155084  -0.909197 -1.519579  0.109323  0.403203  6.907755\n",
       "1242 -1.345330 -0.155084   0.791788  0.857772  0.932681  0.115149  7.824046\n",
       "1243  0.458299 -0.155084   2.165087  1.480304  0.109323 -2.775240  7.937375\n",
       "1244 -0.443515 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  7.693937\n",
       "1245  0.458299 -0.155084   0.791788  1.480304  0.563613  0.403203  7.972466\n",
       "1246  1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.899153\n",
       "1247  1.360114  1.771624  -0.909197 -1.519579 -2.750471  0.403203  7.408531\n",
       "1248  1.360114 -0.155084  -0.909197 -1.519579 -1.526132  0.403203  7.649693\n",
       "1249 -1.345330 -2.563469   0.791788 -0.019637  0.036944  0.573584  7.696213\n",
       "1250 -1.345330 -0.155084   0.791788 -0.019637  0.087384  0.039658  7.492760\n",
       "1251 -0.443515 -0.155084  -0.909197 -0.019637 -0.708637  0.277334  7.481556\n",
       "1252  1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.647309\n",
       "1253  1.360114  0.808270  -0.909197  0.217293  0.109323  0.403203  7.649216\n",
       "1254  0.458299 -0.155084   0.791788 -0.019637 -0.245851 -0.309200  7.863267\n",
       "1255  0.458299 -0.155084   1.786801  0.857772  0.850889 -0.725353  8.241440\n",
       "1256 -0.443515 -0.155084  -0.909197 -1.519579 -1.054199 -0.599484  7.313220\n",
       "1257  0.458299 -0.155084  -0.909197 -0.019637 -0.386938  0.013593  7.549609\n",
       "1258  0.458299 -0.155084   1.786801 -0.019637  0.474188 -0.483697  7.972466\n",
       "1259  0.458299 -0.155084  -0.909197 -1.519579 -0.867004  0.892801  7.438384\n",
       "1260  1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  8.101375\n",
       "1261  1.360114  1.771624  -0.909197 -0.019637  0.109323  0.403203  8.242756\n",
       "1262 -0.443515 -2.081792   1.339383 -0.019637  0.083546  0.403203  7.598399\n",
       "1263  0.458299 -0.155084  -0.909197 -0.019637 -0.316394  0.163372  7.649693\n",
       "1264  0.458299 -0.155084   0.791788 -0.019637  0.865630  0.403203  8.101678\n",
       "1265  0.458299 -0.155084  -0.909197 -0.019637  0.109323  0.403203  8.294050\n",
       "1266  1.360114 -0.155084  -0.909197  0.857772  0.109323 -0.725353  8.070906\n",
       "1267  0.458299 -0.155084   0.791788 -0.019637  0.231976  0.403203  7.882315\n",
       "1268 -0.443515 -0.636761  -0.909197 -1.519579 -1.259247  0.403203  7.435438\n",
       "1269 -1.345330 -0.155084   1.339383  0.857772  0.945587 -0.521272  7.438384\n",
       "1270  0.458299 -0.155084  -0.909197  0.217293 -1.878161 -1.965715  7.254885\n",
       "1271 -0.443515 -2.081792  -0.909197 -0.019637 -1.015434  0.403203  6.870053\n",
       "1272  0.458299 -0.155084   2.165087  1.963178  2.044052 -0.124280  8.809863\n",
       "1273  0.458299 -0.155084  -0.909197  0.857772 -0.143125  0.863869  7.901007\n",
       "1274  1.360114 -0.155084  -0.909197  0.217293  0.109323  0.403203  7.600902\n",
       "1275  1.360114 -0.155084   1.786801  0.857772  0.917091  0.403203  8.698681\n",
       "1276 -0.443515 -0.155084  -0.909197 -0.019637 -0.509907  0.403203  7.377759\n",
       "1277  0.458299 -0.155084  -0.909197 -1.519579 -0.878990  0.403203  7.893572\n",
       "1278 -1.345330 -2.563469   0.791788 -0.019637  0.155328 -2.072918  7.649693\n",
       "1279 -0.443515 -1.600115  -0.909197 -1.519579 -0.447544  0.403203  7.299797\n",
       "1280 -0.443515 -0.155084   0.791788  0.857772  0.768661  0.481415  7.937375\n",
       "1281 -1.345330 -0.155084   0.791788  0.857772  0.278091  0.299088  7.438384\n",
       "1282 -1.345330 -0.155084   0.791788 -0.019637  0.558998  0.255249  7.438384\n",
       "1283 -0.443515 -0.155084  -0.909197 -1.519579 -1.259247  0.039658  7.090077\n",
       "1284 -1.345330 -0.155084  -0.909197 -1.519579 -0.764886 -0.447073  7.003065\n",
       "1285  1.360114  1.289947  -0.909197 -0.019637  0.117847  0.403203  8.242756\n",
       "1286  1.360114  1.289947   0.791788  0.857772 -0.004678  0.423156  8.612503\n",
       "1287 -0.443515 -0.155084   0.791788 -0.019637  0.267537  0.403203  7.649693\n",
       "1288 -0.443515 -2.081792  -0.909197 -1.519579 -0.270619  0.403203  7.783224\n",
       "1289 -1.345330 -2.563469   0.791788  0.857772  0.634652  0.537418  7.803843\n",
       "1290 -1.345330 -0.155084   1.339383  1.480304  1.408082 -0.039990  7.970740\n",
       "1291  0.458299 -0.155084   1.786801  1.963178  1.043679  0.403203  8.116716\n",
       "1292 -1.345330 -2.563469   0.791788 -0.019637  0.134794  0.403203  7.207860\n",
       "1293  0.458299 -0.155084  -0.909197 -1.519579 -2.131557 -0.012950  7.598399\n",
       "1294  0.458299 -0.155084   0.791788  0.857772  0.114062 -5.783300  8.006368\n",
       "1295 -1.345330 -0.155084   1.339383  1.480304  1.201269 -1.865912  7.598399\n",
       "1296 -1.345330 -0.155084   1.339383 -0.019637  0.836048 -4.194079  7.861342\n",
       "1297  1.360114  0.808270   0.791788  0.857772  0.231976  0.403203  8.188689\n",
       "1298  0.458299 -0.155084   1.339383 -0.019637 -0.160267  0.403203  7.718685\n",
       "1299  1.360114  1.289947   0.791788 -0.019637 -0.927611  0.773428  7.919356\n",
       "1300 -1.345330 -0.155084   1.339383  0.857772  1.128058 -1.015637  7.693937\n",
       "1301 -1.345330 -0.155084  -0.909197 -0.019637  0.231976  0.819355  6.802395\n",
       "1302 -0.443515  0.326593  -0.909197 -1.519579 -1.295406  0.186894  7.313220\n",
       "1303 -1.345330 -2.563469   0.791788  0.857772  0.749014  0.773428  8.116716\n",
       "1304  0.458299 -0.155084   0.085816 -0.019637 -0.467304  0.403203  7.989560\n",
       "1305  0.458299 -0.155084  -0.909197 -1.519579 -2.131557  0.403203  7.696213\n",
       "1306 -0.443515  0.326593  -0.909197 -1.519579  0.109323  0.403203  7.150701\n",
       "1307  1.360114 -0.155084   0.791788  0.857772  0.563613 -0.447073  8.188689\n",
       "1308  1.360114 -0.155084   0.791788  0.857772  0.501249  0.163372  8.523175\n",
       "1309  0.458299 -0.155084   0.791788  1.480304  0.810168  0.186894  8.070906\n",
       "1310  0.458299 -0.155084   1.786801  0.857772  0.800555 -1.865912  8.188689\n",
       "1311 -0.443515 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.056175\n",
       "1312  1.360114  2.253301  -0.909197 -0.019637  0.850889  0.403203  8.512181\n",
       "1313 -0.443515 -0.636761   1.339383  0.857772  0.563613  0.403203  8.055158\n",
       "1314  0.458299 -0.155084  -0.909197 -0.019637  0.109323  0.403203  7.464510\n",
       "1315 -1.345330 -0.155084   1.339383  0.857772  1.612848 -0.342456  7.740664\n",
       "1316 -0.443515  0.326593  -0.909197 -1.519579 -1.878161  1.042345  7.467371\n",
       "1317 -1.345330 -0.155084   0.791788  0.857772  0.056484  0.186894  7.467371\n",
       "1318  1.360114 -0.155084  -0.909197  0.217293  0.109323  0.403203  7.435438\n",
       "1319 -0.443515 -0.155084   0.791788  0.857772  0.485372 -0.039990  7.937375\n",
       "1320  0.458299 -0.155084  -0.909197 -1.519579 -0.779181  0.403203  7.377759\n",
       "1321  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.803843\n",
       "1322  1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.317522\n",
       "1323  0.458299 -0.155084  -0.909197 -1.519579 -1.080436  0.403203  7.512071\n",
       "1324  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.989560\n",
       "1325  0.458299 -0.155084   0.791788  0.857772  0.109323  0.403203  7.937375\n",
       "1326  1.360114  2.253301  -0.909197  0.217293  0.109323  0.403203  7.693937\n",
       "1327 -0.443515 -0.636761  -0.909197 -1.519579 -1.878161  0.403203  7.166266\n",
       "1328 -1.345330 -0.155084   1.339383  0.857772  1.335255 -0.124280  7.598399\n",
       "1329  1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.004700\n",
       "1330 -0.443515 -0.155084  -0.909197 -0.019637 -0.304859  0.382970  7.600902\n",
       "1331 -0.443515  0.326593   0.791788 -0.019637  0.513868 -0.815793  8.146130\n",
       "1332 -1.345330 -0.155084   0.791788  1.480304  0.761665 -1.772553  7.549609\n",
       "1333 -1.345330 -0.155084   0.791788 -0.019637  0.783973 -0.640236  7.522941\n",
       "1334 -0.443515 -0.155084   0.791788 -0.019637  1.104286  0.921166  7.738488\n",
       "1335 -1.345330 -0.155084  -0.909197 -1.519579 -0.243613  0.403203  7.392648\n",
       "1336  0.458299 -0.155084   1.339383 -0.019637  0.324957 -2.072918  7.963808\n",
       "1337  1.360114  1.289947   1.339383  0.857772  0.701979 -1.449760  8.241440\n",
       "1338 -0.443515 -0.155084  -0.909197 -0.019637  0.404178  0.948986  7.901007\n",
       "1339 -0.443515  0.326593  -0.909197 -1.519579 -1.295406  0.186894  7.313220\n",
       "1340 -1.345330 -0.155084  -0.909197 -1.519579 -1.137274  0.299088  6.956545\n",
       "1341 -0.443515  0.326593   0.791788 -0.019637  0.823828 -2.072918  8.037543\n",
       "1342 -0.443515 -2.081792  -0.909197 -0.019637 -0.386938  0.403203  7.575585\n",
       "1343 -1.345330 -0.155084   0.791788 -0.019637  0.888213  0.481415  7.598399\n",
       "1344  0.458299 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.718685\n",
       "1345  0.458299 -1.118438  -0.909197  0.217293 -1.216644  0.403203  7.226209\n",
       "1346  1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  7.955074\n",
       "1347 -0.443515 -0.155084  -0.909197 -0.019637 -0.912298  0.403203  7.546974\n",
       "1348 -1.345330 -0.155084   0.791788  0.857772 -0.168890  0.382970  7.134891\n",
       "1349  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.279319\n",
       "1350  1.360114  1.289947  -0.909197 -1.519579  0.109323  0.403203  7.718685\n",
       "1351 -1.345330 -0.155084  -0.909197 -1.519579 -0.243613  0.403203  7.392648\n",
       "1352 -0.443515 -0.155084   0.085816 -0.019637  0.044781 -0.183332  7.520235\n",
       "1353 -0.443515 -0.155084   0.085816 -0.019637  0.231976  0.403203  7.649693\n",
       "1354  0.458299 -0.155084   0.791788 -0.019637  0.850889 -0.183332  7.783224\n",
       "1355 -0.443515  0.326593   0.791788 -0.019637  0.044781  0.403203  7.740664\n",
       "1356 -1.345330 -0.155084   0.791788  1.480304  0.712043 -3.777927  7.495542\n",
       "1357  1.360114  2.253301   0.791788  1.480304  0.109323  0.403203  8.593228\n",
       "1358 -0.443515 -0.155084   0.791788 -0.019637  0.267537  0.403203  7.649693\n",
       "1359 -0.443515  0.326593  -0.909197 -1.519579 -0.640334 -1.865912  7.546974\n",
       "1360 -1.345330 -0.155084   0.791788 -0.019637  0.081624 -0.447073  7.495542\n",
       "1361 -0.443515 -1.600115   0.791788 -0.019637  0.362402  0.403203  7.863267\n",
       "1362 -1.345330 -2.563469   0.085816 -0.019637 -0.140991 -0.483697  7.467371\n",
       "1363  1.360114  2.253301   0.791788  1.480304  1.104286  0.403203  8.612503\n",
       "1364 -1.345330 -0.155084   0.791788 -0.019637  0.162747  0.090420  7.495542\n",
       "1365  1.360114  2.253301   0.791788  0.857772  1.104286  1.080583  8.612503\n",
       "1366 -0.443515 -2.081792  -0.909197 -1.519579 -1.223687  0.115149  7.346010\n",
       "1367 -1.345330 -0.155084   1.339383  0.857772  1.561281 -1.684855  7.673223\n",
       "1368  1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.202482\n",
       "1369 -0.443515  0.326593   0.085816  0.857772  0.420664 -1.523959  7.901007\n",
       "1370  0.458299 -0.155084   0.085816 -0.019637  0.664184  0.362451  8.131531\n",
       "1371 -0.443515 -0.155084  -0.909197  0.857772  0.044781  0.299088  7.783224\n",
       "1372 -0.443515 -2.081792   1.339383  0.857772  0.563613 -2.604858  8.004700\n",
       "1373  1.360114  0.808270  -0.909197 -0.019637 -0.160267  0.403203  7.919356\n",
       "1374  0.458299 -0.155084   0.791788  0.857772  0.409137  0.139461  7.989560\n",
       "1375  1.360114  2.253301   0.791788  0.857772  0.109323  0.403203  8.647344\n",
       "1376 -1.345330 -0.155084   1.339383  0.857772  0.945587 -0.521272  7.438384\n",
       "1377 -0.443515  0.326593   1.786801  1.480304  1.845172  0.537418  8.101678\n",
       "1378  1.360114  0.808270   0.791788  1.480304  0.850889  0.403203  8.612503\n",
       "1379  1.360114  0.808270  -0.909197 -1.519579 -1.255665  0.163372  7.783224\n",
       "1380 -1.345330 -2.563469  -0.909197 -1.519579 -1.087045  0.186894  7.047517\n",
       "1381  1.360114  1.771624  -0.909197 -0.019637  0.109323  0.403203  8.330864\n",
       "1382  1.360114  1.289947   0.791788 -0.019637  0.044781  0.403203  8.070906\n",
       "1383  0.458299 -0.155084  -0.909197 -1.519579 -0.398923 -0.276691  7.783224\n",
       "1384 -1.345330 -0.155084  -0.909197 -1.519579 -1.223687  0.403203  6.983790\n",
       "1385 -1.345330 -0.155084   0.791788 -1.519579 -0.401328 -0.447073  6.932448\n",
       "1386 -1.345330 -0.155084   1.786801  1.480304  1.923353 -0.559850  7.970740\n",
       "1387  0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.255249  7.090077\n",
       "1388 -1.345330 -0.155084   1.339383 -0.019637  1.368280 -0.244897  7.843849\n",
       "1389 -0.443515 -0.155084  -0.909197 -1.519579 -1.259247  0.039658  7.090077\n",
       "1390  0.458299 -0.155084  -0.909197 -0.019637  0.109323  0.403203  7.600902\n",
       "1391  1.360114 -0.155084  -0.909197 -1.519579 -0.779181  0.403203  7.901007\n",
       "1392 -1.345330 -0.155084   0.085816 -1.519579 -0.675581 -0.521272  7.244228\n",
       "1393  0.458299 -0.155084   0.791788  0.857772  1.091098  0.255249  8.160518\n",
       "1394 -1.345330 -0.155084  -0.909197  0.857772 -0.016719  0.382970  7.244228\n",
       "1395 -1.345330 -0.155084   0.791788  1.480304  1.159740 -3.191392  7.467371\n",
       "1396  1.360114 -0.155084   0.791788  0.857772  0.404178  0.403203  8.318742\n",
       "1397 -1.345330 -0.155084   1.786801  0.857772  1.639107 -0.376494  8.242756\n",
       "1398 -0.443515 -2.081792   0.791788  0.857772  0.231976  0.403203  7.989560\n",
       "1399  0.458299 -0.155084  -0.909197  0.217293 -1.054199 -2.968402  7.408531\n",
       "1400  0.458299 -0.155084  -0.909197  0.217293 -0.055301  0.403203  7.435438\n",
       "1401 -1.345330 -0.155084   0.791788 -0.019637  0.563613  0.090420  7.279319\n",
       "1402  1.360114  1.771624  -0.909197 -1.519579 -1.192177  0.065262  7.928406\n",
       "1403 -0.443515 -0.155084   1.339383  0.857772  0.563613  0.403203  8.055158\n",
       "1404  0.458299 -0.155084  -0.909197 -0.019637 -0.912298  0.403203  7.475906\n",
       "1405 -1.345330 -0.155084   0.791788  0.857772  0.723486  0.500324  8.101678\n",
       "1406  0.458299 -0.155084  -0.909197 -1.519579 -1.651490  0.403203  7.130899\n",
       "1407 -0.443515 -0.636761  -0.909197 -1.519579 -1.505931  1.055204  7.346010\n",
       "1408 -0.443515 -0.155084  -0.909197 -0.019637  0.044781  0.773428  7.598399\n",
       "1409  0.458299 -0.155084  -0.909197 -0.019637 -0.648417  0.403203  7.522941\n",
       "1410  0.458299 -0.155084  -0.909197 -1.519579 -1.926509  1.093107  7.090077\n",
       "1411 -1.345330 -0.155084  -0.909197 -1.519579 -1.209625 -0.640236  6.678342\n",
       "1412  0.458299 -0.155084  -0.909197 -0.019637 -0.134604  0.403203  7.509335\n",
       "1413 -0.443515 -0.636761  -0.909197 -0.019637  0.231976  0.403203  8.146130\n",
       "1414  1.360114  2.253301   0.791788  0.857772  0.712043  0.403203  8.389360\n",
       "1415 -1.345330 -0.155084   0.085816 -1.519579 -0.675581 -0.521272  7.244228\n",
       "1416  1.360114 -0.155084  -0.909197 -1.519579 -2.418834  0.403203  7.549609\n",
       "1417  1.360114  1.289947  -0.909197  0.217293 -1.259247  0.804208  7.495542\n",
       "1418  1.360114 -0.155084   0.791788  0.857772  1.046137  0.362451  8.341649\n",
       "1419 -0.443515 -0.155084  -0.909197 -1.519579 -0.915352 -1.247584  7.549609\n",
       "1420 -0.443515  0.326593  -0.909197 -1.519579  0.109323  0.403203  7.150701\n",
       "1421  0.458299 -1.118438  -0.909197 -1.519579 -0.640334  0.403203  7.374629\n",
       "1422 -0.443515 -0.636761  -0.909197 -1.519579 -1.087045  0.403203  7.492760\n",
       "1423 -0.443515 -2.081792  -0.909197  0.217293 -1.237840 -1.772553  7.309881\n",
       "1424  1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  7.989560\n",
       "1425 -0.443515 -1.600115  -0.909197 -1.519579 -3.058335 -0.124280  6.802395\n",
       "1426 -0.443515 -0.155084  -0.909197 -1.519579 -1.037962  0.403203  7.309881\n",
       "1427  0.458299 -0.155084  -0.909197  0.857772  0.109323 -0.599484  7.783224\n",
       "1428 -0.443515  0.326593  -0.909197 -0.019637  1.080248  0.403203  7.764296\n",
       "1429 -0.443515 -2.081792  -0.909197 -0.019637 -0.386938  0.403203  7.546974\n",
       "1430  1.360114 -0.155084  -0.909197 -0.019637 -0.640334  0.403203  8.006368\n",
       "1431 -1.345330 -0.155084   0.791788  1.480304  0.761665 -1.772553  7.549609\n",
       "1432 -1.345330 -0.155084   1.786801  1.480304  1.635376 -0.153504  7.781139\n",
       "1433 -1.345330 -0.155084   0.791788 -0.019637  0.681710  0.403203  7.696213\n",
       "1434  1.360114  1.771624  -0.909197 -1.519579 -0.837330  0.403203  7.901007\n",
       "1435  1.360114 -0.155084   0.791788  0.857772  0.940434  0.442837  7.901007\n",
       "1436  0.458299 -0.155084   0.791788 -0.019637  0.714909  0.382970  8.101678\n",
       "1437  1.360114  1.289947  -0.909197  0.857772  0.109323  0.403203  8.160518\n",
       "1438 -1.345330 -0.155084   0.791788  0.857772  1.337401 -0.682168  7.696213\n",
       "1439 -1.345330 -0.155084  -0.909197 -1.519579 -0.831444 -0.483697  6.684612\n",
       "1440 -0.443515 -0.155084  -0.909197  0.217293 -1.849666 -1.126967  7.244228\n",
       "1441 -1.345330 -0.155084   0.085816 -1.519579 -0.437732  0.320519  7.279319\n",
       "1442  1.360114  1.771624   0.085816 -0.019637  0.151609  0.403203  8.411833\n",
       "1443 -0.443515 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.240650\n",
       "1444 -1.345330 -2.563469   1.786801  0.857772  0.575871 -0.183332  7.729735\n",
       "1445 -1.345330 -0.155084   0.791788  0.857772  0.982581  0.232821  7.824046\n",
       "1446 -0.443515 -0.155084   0.791788 -0.019637 -0.184063 -0.815793  7.598399\n",
       "1447 -1.345330 -0.155084   0.791788  0.857772  0.404178  0.039658  7.546974\n",
       "1448  1.360114 -0.155084   0.791788 -0.019637  1.104286 -0.376494  8.465900\n",
       "1449  0.458299 -1.118438   2.492774  1.963178  1.895401  0.163372  8.507143\n",
       "1450  0.458299 -0.155084   1.786801  1.480304  1.064478 -1.449760  8.070906\n",
       "1451 -0.443515 -0.155084   0.791788  0.857772  0.850889  0.403203  8.086410\n",
       "1452  0.458299 -0.155084   0.791788 -0.019637  0.091216  0.320519  7.575585\n",
       "1453 -1.345330 -0.155084  -0.909197 -1.519579 -0.243613  0.210040  7.346010\n",
       "1454 -1.345330 -0.155084  -0.909197 -1.519579 -0.779181  0.555614  6.902743\n",
       "1455 -1.345330 -0.155084   0.791788  0.857772  0.420664  0.139461  7.649693\n",
       "1456  1.360114  0.808270  -0.909197  0.857772  0.485372  0.403203  8.229511\n",
       "1457  0.458299 -0.155084   0.791788  0.857772  0.485372  0.210040  8.242756\n",
       "1458  0.458299 -0.155084  -0.909197 -0.019637  0.109323  0.403203  7.435438\n",
       "1459 -0.443515 -0.155084  -0.909197  0.217293 -2.977141  0.403203  7.090077\n",
       "1460  0.458299 -0.155084   1.786801  1.963178  1.163231 -3.777927  8.070906\n",
       "1461 -1.345330 -0.155084   1.786801  1.480304  1.923353 -0.559850  7.970740\n",
       "1462  1.360114 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.740664\n",
       "1463 -1.345330 -0.155084  -0.909197 -0.019637 -0.300262  0.115149  7.240650\n",
       "1464 -0.443515  0.326593  -0.909197 -1.519579 -0.640334 -1.865912  7.546974\n",
       "1465  1.360114  0.808270   0.085816  0.857772  0.404178  1.093107  8.389133\n",
       "1466  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.309881\n",
       "1467  1.360114  0.808270   1.786801  0.857772  0.958415  0.555614  8.610684\n",
       "1468  1.360114  2.253301  -0.909197  0.217293 -1.446442  0.403203  7.803843\n",
       "1469 -1.345330 -0.155084  -0.909197  0.857772  0.114062  0.921166  6.856462\n",
       "1470 -1.345330 -0.155084   0.791788  0.857772  0.864294 -0.039990  7.368340\n",
       "1471  1.360114  1.771624   0.791788  0.857772  0.109323  0.403203  8.410721\n",
       "1472  1.360114  2.253301   0.791788 -0.019637 -0.022765 -1.311887  8.342840\n",
       "1473  1.360114  1.771624  -0.909197 -1.519579 -0.640334  0.403203  8.070906\n",
       "1474  1.360114 -0.155084  -0.909197 -1.519579 -1.446442  0.065262  7.803843\n",
       "1475  1.360114  2.253301  -0.909197 -1.519579 -1.054199  0.403203  7.937375\n",
       "1476 -0.443515 -0.155084  -0.909197 -1.519579  0.231976  0.403203  7.696213\n",
       "1477 -0.443515  0.326593  -0.909197 -0.019637 -0.689293  0.403203  7.435438\n",
       "1478 -1.345330 -0.155084   0.791788  1.480304  0.996449 -3.455133  7.467371\n",
       "1479  1.360114 -0.155084  -0.909197 -0.019637 -0.270619  0.403203  8.160518\n",
       "1480 -0.443515 -0.155084   0.085816 -0.019637  0.319800  0.403203  7.549609\n",
       "1481  1.360114  1.771624  -0.909197  0.857772  0.850889  0.403203  8.575462\n",
       "1482  1.360114  0.808270  -0.909197 -0.019637 -0.055301  0.403203  8.159089\n",
       "1483  0.458299 -0.155084   0.791788  1.480304  0.712043 -1.865912  7.937375\n",
       "1484 -0.443515 -2.081792  -0.909197  0.217293 -1.237840 -1.772553  7.309881\n",
       "1485 -1.345330 -0.155084   2.165087  0.857772  1.748153 -1.311887  8.160518\n",
       "1486  1.360114  0.808270   0.085816 -0.019637 -0.722562  0.591334  7.783224\n",
       "1487 -1.345330 -0.155084   0.085816 -1.519579 -0.160267  0.403203  7.313220\n",
       "1488  0.458299 -0.155084   0.791788  0.857772  0.988896 -0.039990  8.160518\n",
       "1489  0.458299 -0.155084   0.791788 -0.019637 -0.277423 -0.342456  7.738488\n",
       "1490  0.458299 -1.118438  -0.909197 -0.019637 -0.160267  0.403203  7.549609\n",
       "1491  1.360114 -0.155084  -0.909197 -0.019637 -0.386938  0.403203  8.006034\n",
       "1492 -0.443515 -2.081792  -0.909197  0.857772  0.319800  0.403203  7.727535\n",
       "1493 -1.345330 -0.155084   1.339383  0.857772  1.311506 -0.067544  7.575585\n",
       "1494  1.360114  1.771624  -0.909197 -1.519579 -1.878161  0.863869  7.859413\n",
       "1495 -1.345330 -0.155084   0.791788 -0.019637  0.087384  0.039658  7.492760\n",
       "1496  1.360114  0.808270  -0.909197 -0.019637 -1.926509  0.403203  7.989560\n",
       "1497  1.360114 -0.155084   0.791788  0.857772  0.285099  0.403203  8.180321\n",
       "1498  0.458299 -0.155084   0.085816 -0.019637 -0.459873  0.403203  7.522941\n",
       "1499  1.360114  1.771624  -0.909197 -1.519579 -1.057461  0.403203  8.037543\n",
       "1500  0.458299 -0.155084   0.791788  0.857772  0.943012  0.277334  8.160518\n",
       "1501 -1.345330 -2.563469  -0.909197 -1.519579 -0.474761  0.757790  7.240650\n",
       "1502 -1.345330 -0.155084   1.339383  0.857772  1.561281 -1.684855  7.673223\n",
       "1503  1.360114  0.808270   0.791788 -0.019637  0.850889 -1.523959  8.342840\n",
       "1504  0.458299 -0.155084   0.791788  0.857772  0.874957  0.186894  8.146130\n",
       "1505  1.360114  2.253301  -0.909197  0.857772  0.712043  0.403203  8.432724\n",
       "1506 -1.345330 -0.155084   0.791788  0.857772  0.515440 -0.276691  7.279319\n",
       "1507 -0.443515 -0.155084  -0.909197 -1.519579 -1.266431 -0.411353  7.279319\n",
       "1508  1.360114  2.253301  -0.909197  0.217293  0.109323  0.989737  7.562681\n",
       "1509  0.458299 -0.155084   1.786801  1.480304  1.152739  0.232821  8.039157\n",
       "1510 -0.443515 -0.155084  -0.909197 -0.019637 -0.694802  0.537418  7.989560\n",
       "1511  1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.841886\n",
       "1512  0.458299 -0.155084  -0.909197 -1.519579 -1.087045  0.442837  7.495542\n",
       "1513  0.458299 -0.155084  -0.909197 -0.019637  0.109323  0.232821  7.590852\n",
       "1514 -1.345330 -0.155084  -0.909197 -1.519579 -0.993140 -1.965715  7.163947\n",
       "1515  0.458299 -0.155084   0.791788  1.480304  0.751831  0.403203  8.160518\n",
       "1516 -0.443515 -2.081792  -0.909197 -0.019637 -1.015434  0.403203  6.870053\n",
       "1517 -0.443515 -0.155084  -0.909197  0.217293 -2.645505  1.029371  7.154615\n",
       "1518  0.458299 -1.118438   1.339383  0.857772  0.109323  0.403203  7.781139\n",
       "1519  1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.781139\n",
       "1520  1.360114  1.771624  -0.909197 -0.019637  0.231976  0.403203  8.318742\n",
       "1521 -0.443515 -0.155084   0.085816 -0.019637 -0.160267 -0.521272  7.244228\n",
       "1522  0.458299 -0.155084  -0.909197 -1.519579 -1.174869  0.232821  7.313220\n",
       "1523  1.360114  2.253301  -0.909197  0.217293  0.109323  0.403203  7.762171\n",
       "1524  1.360114  2.253301  -0.909197  0.857772  0.276336  0.403203  8.330864"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>area</th>\n",
       "      <th>year</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.055301</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.177508</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.472273</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.822644</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.887744</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>6.745236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.601691</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>8.130059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.905324</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.214735</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>6.882437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.305986</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>1.142148</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.394121</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.388554</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.037543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.646513</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.698681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.708637</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.868759</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.536364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.513394</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.418834</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.636138</td>\n",
       "      <td>-4.780613</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.050942</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>6.745236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.025159</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.878990</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.431163</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.126891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.930090</td>\n",
       "      <td>-1.449760</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.274579</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.575871</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>7.729735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.995192</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.149231</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.056484</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.397548</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>6.902743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.394121</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.895401</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.496990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.407485</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.187299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.969896</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.267449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.768661</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.360802</td>\n",
       "      <td>0.090420</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.059403</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.065695</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.595096</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.005851</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.150701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.847712</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.171051</td>\n",
       "      <td>0.462254</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.723486</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.761665</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.423946</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>8.055158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.024660</td>\n",
       "      <td>-0.863225</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.993140</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.283349</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.055301</td>\n",
       "      <td>1.154153</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>1.142148</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.419381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>2.264768</td>\n",
       "      <td>-1.070231</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.485918</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>6.902743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.050641</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.882315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.750685</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.078434</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.961296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.562075</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.094073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>-0.183332</td>\n",
       "      <td>6.932448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.389360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.639107</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.377161</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>6.856462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.305986</td>\n",
       "      <td>0.481415</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.186682</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.569962</td>\n",
       "      <td>-0.682168</td>\n",
       "      <td>8.216088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.762171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.192168</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.990157</td>\n",
       "      <td>-1.379181</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.263208</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>1.080583</td>\n",
       "      <td>8.611594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.636138</td>\n",
       "      <td>-4.780613</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.073920</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.121624</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-1.126967</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.438633</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.225797</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.248051</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.277334</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.808059</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.330101</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.370823</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.321083</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.043679</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>2.119174</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.714909</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>8.188411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.139846</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.298441</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>8.330864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.410976</td>\n",
       "      <td>-0.769866</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.159041</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.083546</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.504411</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.807578</td>\n",
       "      <td>-1.247584</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.206729</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.694802</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.309881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.693412</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>8.214736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.605655</td>\n",
       "      <td>-2.968402</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.093129</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.071755</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.408531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.131557</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.423096</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>1.459357</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>7.183112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.864294</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.834346</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.020194</td>\n",
       "      <td>-0.309200</td>\n",
       "      <td>8.536996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.146772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.948986</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.546974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.086410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.808059</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.177508</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.693937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.683391</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.970740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.419021</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>7.803843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.262301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.588324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.523283</td>\n",
       "      <td>-0.309200</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.516193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.204149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.485372</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.236399</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.989737</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.675157</td>\n",
       "      <td>-1.247584</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.513868</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.779557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.240650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.216088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>8.607948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>1.312530</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>-0.045081</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.329750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.177141</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.580450</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.073846</td>\n",
       "      <td>-1.449760</td>\n",
       "      <td>7.522941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.743367</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.069342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.739314</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.972466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.930090</td>\n",
       "      <td>1.105524</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.976280</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.299088</td>\n",
       "      <td>7.158514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.362987</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>8.516193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.492774</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.732145</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.485918</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>6.902743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>2.164918</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.740337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.453261</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.369599</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.189168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.027638</td>\n",
       "      <td>0.382970</td>\n",
       "      <td>8.159089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.222993</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.703093</td>\n",
       "      <td>0.500324</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>7.166266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.242756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.173672</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.674197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.812907</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.214736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.146889</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.579560</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.976280</td>\n",
       "      <td>8.268732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.522600</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.762171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.026591</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.090077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.762490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.041006</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>7.588324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.166717</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.569751</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>8.053569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.962067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.632281</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.326466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.995634</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.963808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.453261</td>\n",
       "      <td>0.741980</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.873627</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.779181</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.598399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.248091</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.484369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.601691</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.441320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.603010</td>\n",
       "      <td>1.003069</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.864294</td>\n",
       "      <td>-2.188705</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.321253</td>\n",
       "      <td>0.232821</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.393263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.646513</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.496990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.484745</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>7.649693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.976595</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>8.389360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.618235</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.796461</td>\n",
       "      <td>-0.153504</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.455318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>2.357714</td>\n",
       "      <td>0.524848</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.352254</td>\n",
       "      <td>0.163372</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.445158</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.565983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.583498</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>7.362011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.673223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.054538</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.064251</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.399208</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.248091</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.532657</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.863267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.775473</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.906755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.220605</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.364089</td>\n",
       "      <td>-5.783300</td>\n",
       "      <td>8.101678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.538344</td>\n",
       "      <td>1.055204</td>\n",
       "      <td>7.822044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>2.019198</td>\n",
       "      <td>-1.772553</td>\n",
       "      <td>8.611594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>-0.095634</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-2.418834</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.998510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.899153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.837401</td>\n",
       "      <td>0.139461</td>\n",
       "      <td>8.154788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.140414</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.781139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.314630</td>\n",
       "      <td>-1.865912</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.138860</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.399208</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.509907</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>0.608868</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.401328</td>\n",
       "      <td>-0.521272</td>\n",
       "      <td>6.856462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>-2.604858</td>\n",
       "      <td>8.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.810168</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.726337</td>\n",
       "      <td>0.804208</td>\n",
       "      <td>8.507143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>0.115149</td>\n",
       "      <td>6.802395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-3.066626</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>7.047517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.456494</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.862882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.044781</td>\n",
       "      <td>-0.559850</td>\n",
       "      <td>7.647309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.768661</td>\n",
       "      <td>0.863869</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.816798</td>\n",
       "      <td>-0.483697</td>\n",
       "      <td>6.856462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.374182</td>\n",
       "      <td>0.989737</td>\n",
       "      <td>8.354674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.248091</td>\n",
       "      <td>0.518990</td>\n",
       "      <td>7.937375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>7.600402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>-0.411353</td>\n",
       "      <td>7.600902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.778418</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.435438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.339653</td>\n",
       "      <td>-0.213787</td>\n",
       "      <td>6.903747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.221192</td>\n",
       "      <td>-2.314574</td>\n",
       "      <td>8.109225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>2.165087</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.917091</td>\n",
       "      <td>-1.965715</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.438384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.437732</td>\n",
       "      <td>0.320519</td>\n",
       "      <td>7.279319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.512440</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>7.130899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.060822</td>\n",
       "      <td>-0.244897</td>\n",
       "      <td>8.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.526203</td>\n",
       "      <td>0.757790</td>\n",
       "      <td>8.039157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.270619</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.783224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.730961</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>8.229511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.662717</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.667396</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>0.453261</td>\n",
       "      <td>0.773428</td>\n",
       "      <td>8.131531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.317396</td>\n",
       "      <td>-0.599484</td>\n",
       "      <td>6.956545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.651490</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.625595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.085816</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>0.849182</td>\n",
       "      <td>8.444622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.906635</td>\n",
       "      <td>-0.376494</td>\n",
       "      <td>7.296413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.683165</td>\n",
       "      <td>-3.455133</td>\n",
       "      <td>8.281471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.981316</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.779557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.087045</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.316394</td>\n",
       "      <td>-2.072918</td>\n",
       "      <td>8.006368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.142196</td>\n",
       "      <td>0.186894</td>\n",
       "      <td>7.740664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-3.777927</td>\n",
       "      <td>8.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.515440</td>\n",
       "      <td>0.362451</td>\n",
       "      <td>7.501082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.548190</td>\n",
       "      <td>-2.775240</td>\n",
       "      <td>7.346010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.125693</td>\n",
       "      <td>-1.523959</td>\n",
       "      <td>8.699348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.231976</td>\n",
       "      <td>0.555614</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.694802</td>\n",
       "      <td>0.537418</td>\n",
       "      <td>7.989560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.005851</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.354674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>-0.248091</td>\n",
       "      <td>0.788900</td>\n",
       "      <td>7.002156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-2.081792</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.255665</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.398595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.217293</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.620073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.712043</td>\n",
       "      <td>-3.191392</td>\n",
       "      <td>7.313220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>1.104286</td>\n",
       "      <td>0.921166</td>\n",
       "      <td>7.738488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.726337</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.789455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-2.563469</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.062312</td>\n",
       "      <td>0.423156</td>\n",
       "      <td>8.294050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>8.216088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.404178</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.222993</td>\n",
       "      <td>-0.447073</td>\n",
       "      <td>7.377759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.289947</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.639107</td>\n",
       "      <td>0.643312</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.824046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.636761</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.492760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.464824</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.718685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.339383</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.850889</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>7.861342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.833338</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.575585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.963178</td>\n",
       "      <td>1.627892</td>\n",
       "      <td>0.878407</td>\n",
       "      <td>8.699515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.005851</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>6.989335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-1.600115</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.927611</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.374629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.538884</td>\n",
       "      <td>-0.640236</td>\n",
       "      <td>7.549609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.386938</td>\n",
       "      <td>-4.194079</td>\n",
       "      <td>7.495542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.467371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.259247</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.393263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.640334</td>\n",
       "      <td>1.029371</td>\n",
       "      <td>8.188689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-1.118438</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.430495</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.833996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.911887</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>8.292799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>1.786801</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.913529</td>\n",
       "      <td>0.892801</td>\n",
       "      <td>8.342840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.583498</td>\n",
       "      <td>0.725996</td>\n",
       "      <td>7.362011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.687041</td>\n",
       "      <td>0.065262</td>\n",
       "      <td>8.691146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.130059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-1.446442</td>\n",
       "      <td>0.255249</td>\n",
       "      <td>7.170120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.160267</td>\n",
       "      <td>1.067949</td>\n",
       "      <td>8.216088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>-0.610954</td>\n",
       "      <td>0.834346</td>\n",
       "      <td>8.202482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.365775</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.901007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>1.858682</td>\n",
       "      <td>0.210040</td>\n",
       "      <td>8.146130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.914482</td>\n",
       "      <td>0.626192</td>\n",
       "      <td>7.843849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>-0.339653</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.740230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>-1.345330</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.594129</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>7.207860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.458299</td>\n",
       "      <td>-0.155084</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>1.480304</td>\n",
       "      <td>1.330956</td>\n",
       "      <td>0.442837</td>\n",
       "      <td>8.070906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>-0.443515</td>\n",
       "      <td>0.326593</td>\n",
       "      <td>0.791788</td>\n",
       "      <td>-0.019637</td>\n",
       "      <td>0.423946</td>\n",
       "      <td>-0.342456</td>\n",
       "      <td>8.055158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>0.808270</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.501249</td>\n",
       "      <td>0.573584</td>\n",
       "      <td>8.255828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>1.771624</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>-1.519579</td>\n",
       "      <td>0.109323</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.160518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1.360114</td>\n",
       "      <td>2.253301</td>\n",
       "      <td>-0.909197</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.563613</td>\n",
       "      <td>0.403203</td>\n",
       "      <td>8.411833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city   zipcode  bathrooms  bedrooms      area      year     price\n",
       "0   -0.443515 -0.155084   0.791788 -0.019637 -0.055301 -1.965715  7.600902\n",
       "1    0.458299 -0.155084   0.791788 -0.019637  0.177508 -1.965715  7.937375\n",
       "2   -0.443515 -0.155084   0.085816 -1.519579 -0.472273  0.403203  7.783224\n",
       "3   -0.443515 -0.636761  -0.909197 -1.519579 -0.822644  0.403203  7.374629\n",
       "4   -1.345330 -0.155084  -0.909197 -1.519579 -1.887744 -0.559850  6.745236\n",
       "5    0.458299 -0.155084  -0.909197 -0.019637  0.319800  0.139461  8.188689\n",
       "6   -0.443515 -2.081792  -0.909197  0.857772  0.601691  0.819355  8.130059\n",
       "7   -1.345330 -0.155084   0.791788  0.857772  0.905324  0.403203  8.006368\n",
       "8    1.360114  2.253301  -0.909197 -1.519579 -0.927611  0.403203  7.781139\n",
       "9   -1.345330 -2.563469  -0.909197 -0.019637 -0.214735  0.232821  6.882437\n",
       "10   1.360114  1.289947  -0.909197 -1.519579 -1.446442  0.403203  7.738488\n",
       "11   0.458299 -1.118438  -0.909197 -1.519579 -0.160267 -0.411353  7.740664\n",
       "12   1.360114 -0.155084   0.791788 -0.019637  0.305986 -0.244897  8.159089\n",
       "13   0.458299 -0.155084   0.085816  1.480304  0.639107  0.277334  8.070906\n",
       "14   1.360114  2.253301  -0.909197 -1.519579 -0.927611  1.154153  7.822044\n",
       "15   1.360114  2.253301  -0.909197  0.857772  1.104286  1.142148  8.455318\n",
       "16  -1.345330 -0.155084   0.085816 -1.519579 -0.394121 -0.521272  7.207860\n",
       "17   0.458299 -0.155084  -0.909197 -1.519579 -1.388554  0.403203  7.279319\n",
       "18   1.360114 -0.155084  -0.909197 -1.519579 -0.779181  0.403203  8.037543\n",
       "19   0.458299 -0.155084  -0.909197 -0.019637 -0.779181  0.403203  7.492760\n",
       "20  -0.443515  0.326593  -0.909197 -1.519579 -0.386938  0.403203  7.882315\n",
       "21  -0.443515  0.326593  -0.909197 -0.019637  0.850889  0.555614  7.901007\n",
       "22  -1.345330 -0.155084   0.791788 -0.019637  0.646513 -0.095634  7.377759\n",
       "23   1.360114 -0.155084   1.786801  1.480304  1.330956  0.255249  8.698681\n",
       "24  -0.443515 -1.600115  -0.909197 -1.519579 -0.708637  0.403203  7.408531\n",
       "25   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.649693\n",
       "26  -0.443515 -2.081792   0.085816 -0.019637  0.563613  0.403203  7.696213\n",
       "27  -1.345330 -0.155084   0.791788  0.857772  1.868759  0.299088  8.004700\n",
       "28  -1.345330 -0.155084   0.085816  0.857772  0.093129  0.403203  7.536364\n",
       "29  -1.345330 -0.155084   0.791788  0.857772  1.513394 -2.188705  7.843849\n",
       "30   1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.783224\n",
       "31  -0.443515 -0.155084  -0.909197  0.217293 -2.418834  0.403203  6.998510\n",
       "32  -0.443515 -0.155084   0.791788  0.857772  0.636138 -4.780613  8.006368\n",
       "33   1.360114 -0.155084  -0.909197  0.857772  0.404178  0.773428  8.188689\n",
       "34  -1.345330 -0.155084  -0.909197 -1.519579 -1.050942 -0.483697  6.745236\n",
       "35   0.458299 -0.155084  -0.909197  0.217293 -2.131557  0.115149  7.207860\n",
       "36  -1.345330 -0.155084   0.085816  0.857772  1.025159  0.277334  7.600902\n",
       "37   0.458299 -0.155084  -0.909197  0.857772  0.013259  0.341637  8.006368\n",
       "38  -0.443515 -0.155084  -0.909197 -0.019637 -0.878990  0.403203  7.546974\n",
       "39   1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.861342\n",
       "40  -0.443515 -0.636761  -0.909197  0.217293 -2.431163  0.403203  7.126891\n",
       "41  -1.345330 -0.155084   0.791788  1.480304  0.930090 -1.449760  7.546974\n",
       "42  -1.345330 -0.155084   0.791788 -0.019637  0.274579 -4.194079  7.901007\n",
       "43  -1.345330 -2.563469   1.786801  0.857772  0.575871 -0.183332  7.729735\n",
       "44  -1.345330 -0.155084   0.085816 -0.019637  0.093129  0.232821  7.279319\n",
       "45   0.458299 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.309881\n",
       "46   0.458299 -0.155084   1.339383  0.857772  0.995192 -0.411353  8.006368\n",
       "47  -1.345330 -0.155084   0.791788  0.857772  1.149231  0.277334  7.882315\n",
       "48  -1.345330 -0.155084   0.791788  0.857772  0.056484  0.186894  7.467371\n",
       "49  -1.345330 -0.155084  -0.909197  0.857772  0.397548  0.788900  7.090077\n",
       "50  -1.345330 -2.563469  -0.909197 -1.519579 -1.446442  0.163372  6.902743\n",
       "51  -1.345330 -0.155084   0.085816 -1.519579 -0.394121 -0.521272  7.207860\n",
       "52  -1.345330 -0.155084   1.786801  1.480304  1.895401  0.341637  8.496990\n",
       "53  -0.443515 -1.600115   0.085816  0.857772 -0.160267  0.403203  8.146130\n",
       "54   1.360114  1.289947  -0.909197 -1.519579 -1.651490  0.819355  7.495542\n",
       "55  -0.443515 -0.155084   0.791788 -0.019637  0.407485 -4.194079  8.004700\n",
       "56   0.458299 -0.155084   0.791788  0.857772  0.404178  0.115149  7.822044\n",
       "57   1.360114 -0.155084  -0.909197 -0.019637  0.319800  0.403203  8.187299\n",
       "58  -0.443515 -2.081792   0.791788  0.857772  0.969896 -3.455133  8.267449\n",
       "59  -0.443515 -0.155084   0.791788  0.857772  0.768661  0.481415  7.937375\n",
       "60   0.458299 -0.155084   0.085816 -0.019637 -0.360802  0.090420  7.649693\n",
       "61  -1.345330 -0.155084   0.791788 -0.019637 -0.059403 -0.559850  7.467371\n",
       "62   0.458299 -0.155084   1.786801  1.480304  1.065695  0.232821  8.281471\n",
       "63   0.458299 -0.155084  -0.909197 -1.519579 -0.595096  0.403203  7.374629\n",
       "64   0.458299 -1.118438  -0.909197 -1.519579 -1.005851 -0.411353  7.150701\n",
       "65  -1.345330 -0.155084   1.339383  0.857772  1.847712 -0.411353  7.972466\n",
       "66  -1.345330 -0.155084  -0.909197  0.857772 -0.171051  0.462254  7.170120\n",
       "67  -1.345330 -0.155084   0.791788  0.857772  0.723486  0.500324  8.101678\n",
       "68   0.458299 -0.155084   0.791788  0.857772  0.761665  0.341637  8.411833\n",
       "69   1.360114  0.808270  -0.909197 -1.519579 -0.927611  0.403203  7.824046\n",
       "70  -0.443515  0.326593   0.791788 -0.019637  0.423946 -0.342456  8.055158\n",
       "71  -1.345330 -0.155084   2.165087  1.480304  2.024660 -0.863225  7.937375\n",
       "72   0.458299 -0.155084  -0.909197 -0.019637 -0.993140  0.403203  7.377134\n",
       "73  -1.345330 -0.155084   0.791788 -0.019637  0.283349 -0.447073  7.718685\n",
       "74   1.360114  2.253301  -0.909197 -1.519579 -1.446442  0.403203  7.738488\n",
       "75   1.360114  1.771624  -0.909197 -0.019637 -0.055301  1.154153  8.292799\n",
       "76  -0.443515 -0.155084  -0.909197 -0.019637 -0.509907  1.142148  7.377759\n",
       "77  -0.443515  0.326593  -0.909197 -1.519579 -0.779181  0.403203  7.377759\n",
       "78   0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.277334  7.419381\n",
       "79   0.458299 -0.155084   2.165087  1.963178  2.264768 -1.070231  8.699515\n",
       "80  -0.443515 -1.600115  -0.909197 -0.019637  0.712043 -3.191392  7.313220\n",
       "81  -1.345330 -0.155084  -0.909197 -1.519579 -1.485918  0.232821  6.902743\n",
       "82   0.458299 -1.118438   0.791788 -1.519579 -0.160267  0.403203  7.408531\n",
       "83   0.458299 -0.155084   0.791788  0.857772  0.050641 -0.095634  8.160518\n",
       "84   1.360114  2.253301  -0.909197 -1.519579 -0.779181  0.403203  7.882315\n",
       "85   0.458299 -0.155084  -0.909197 -1.519579 -0.750685  0.403203  7.492760\n",
       "86   1.360114 -0.155084   0.791788 -1.519579 -0.640334  0.139461  8.070906\n",
       "87   1.360114 -0.155084  -0.909197 -1.519579 -1.651490  0.403203  7.673223\n",
       "88  -1.345330 -2.563469  -0.909197  0.217293 -2.078434  0.403203  6.961296\n",
       "89   1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.972466\n",
       "90  -0.443515 -0.155084   1.339383  0.857772  0.712043  0.403203  7.861342\n",
       "91   0.458299 -0.155084   1.786801  0.857772  0.562075 -2.072918  8.094073\n",
       "92  -1.345330 -0.155084  -0.909197 -1.519579 -1.259247 -0.183332  6.932448\n",
       "93  -1.345330 -0.155084   0.085816 -1.519579  0.044781 -0.244897  7.346010\n",
       "94   1.360114  2.253301   0.791788  1.480304 -0.160267  0.403203  8.389360\n",
       "95  -1.345330 -0.155084   1.786801  0.857772  1.639107 -0.376494  8.242756\n",
       "96   0.458299 -0.155084  -0.909197 -0.019637  0.231976  0.626192  8.268732\n",
       "97  -1.345330 -0.155084  -0.909197 -1.519579 -1.377161 -0.559850  6.856462\n",
       "98  -1.345330 -0.155084  -0.909197  0.857772  0.305986  0.481415  7.575585\n",
       "99   0.458299 -0.155084   0.791788 -0.019637  0.186682  0.403203  7.546974\n",
       "100  0.458299 -0.155084   1.339383  0.857772  1.569962 -0.682168  8.216088\n",
       "101  1.360114  1.289947  -0.909197 -1.519579 -1.259247  0.403203  7.575585\n",
       "102  1.360114  2.253301  -0.909197 -1.519579 -1.259247  0.403203  7.762171\n",
       "103  0.458299 -0.155084   0.791788  0.857772  0.192168 -2.072918  7.863267\n",
       "104 -1.345330 -0.155084   1.339383 -0.019637  0.990157 -1.379181  7.937375\n",
       "105  0.458299 -0.155084   0.791788  1.480304  1.263208  0.039658  8.160518\n",
       "106 -0.443515 -0.636761   0.791788  1.480304  1.220605  1.080583  8.611594\n",
       "107 -0.443515 -0.155084   0.791788  0.857772  0.636138 -4.780613  8.006368\n",
       "108  1.360114  2.253301  -0.909197  0.217293  0.109323  0.403203  7.600902\n",
       "109 -0.443515 -0.155084  -0.909197 -1.519579 -0.927611  0.403203  7.207860\n",
       "110 -0.443515 -0.155084   0.791788 -0.019637  0.073920 -0.376494  7.492760\n",
       "111 -0.443515 -2.081792   1.339383  0.857772  0.005305 -2.072918  8.004700\n",
       "112 -0.443515 -2.081792  -0.909197 -0.019637  0.044781  0.403203  7.696213\n",
       "113 -0.443515 -0.155084   0.085816 -0.019637  0.121624  0.921166  7.738488\n",
       "114  1.360114 -0.155084  -0.909197 -0.019637 -0.386938 -1.126967  8.070906\n",
       "115  0.458299 -0.155084  -0.909197  0.217293 -1.438633  0.403203  7.166266\n",
       "116  0.458299 -0.155084   0.791788  0.857772  0.563613  0.403203  7.972466\n",
       "117  0.458299 -0.155084  -0.909197 -1.519579 -0.225797  0.403203  7.313220\n",
       "118  0.458299 -0.155084  -0.909197 -0.019637 -0.270619  0.403203  7.822044\n",
       "119  0.458299 -0.155084   0.085816 -0.019637  0.248051  0.382970  7.549609\n",
       "120  1.360114  0.808270   0.085816 -0.019637  0.044781  0.403203  8.101678\n",
       "121  0.458299 -0.155084  -0.909197 -1.519579 -2.131557  0.277334  7.435438\n",
       "122  1.360114  2.253301  -0.909197 -1.519579 -0.808059  0.403203  7.822044\n",
       "123 -1.345330 -0.155084   0.791788  0.857772  0.981316  0.299088  7.740664\n",
       "124 -1.345330 -0.155084  -0.909197 -0.019637  0.330101  0.403203  7.377759\n",
       "125 -1.345330 -2.563469  -0.909197 -0.019637  0.370823  0.741980  7.374629\n",
       "126 -0.443515 -2.081792  -0.909197 -1.519579 -1.321083  0.403203  7.492760\n",
       "127  0.458299 -0.155084   1.339383  0.857772  1.043679  0.403203  8.411833\n",
       "128 -1.345330 -0.155084   1.339383  1.480304  2.119174 -0.483697  7.696213\n",
       "129  0.458299 -0.155084   0.791788  0.857772  0.714909  0.403203  8.455318\n",
       "130 -0.443515 -1.600115   0.791788 -0.019637 -0.386938  0.403203  7.522941\n",
       "131  1.360114 -0.155084  -0.909197 -1.519579 -0.509907 -0.521272  8.188411\n",
       "132 -1.345330 -2.563469   1.339383 -0.019637  1.139846 -2.072918  7.937375\n",
       "133  0.458299 -0.155084   1.339383  1.480304  1.298441  0.139461  8.330864\n",
       "134 -0.443515 -0.155084  -0.909197 -1.519579 -0.410976 -0.769866  7.598399\n",
       "135  1.360114  2.253301  -0.909197 -1.519579 -1.087045  0.403203  7.972466\n",
       "136 -1.345330 -0.155084  -0.909197 -0.019637  0.159041  0.382970  7.279319\n",
       "137 -0.443515 -0.636761   1.339383  0.857772  1.220605  0.403203  8.455318\n",
       "138  1.360114  0.808270  -0.909197  0.217293 -1.087045  0.403203  7.843849\n",
       "139 -0.443515 -2.081792   1.339383 -0.019637  0.083546  0.403203  7.598399\n",
       "140  0.458299 -0.155084   0.791788  1.480304  0.504411  0.299088  8.188689\n",
       "141  0.458299 -0.155084  -0.909197 -0.019637  0.850889  0.186894  7.863267\n",
       "142 -0.443515 -2.081792  -0.909197 -0.019637 -0.927611  0.403203  7.522941\n",
       "143 -1.345330 -0.155084   1.786801  1.480304  1.807578 -1.247584  7.824046\n",
       "144 -1.345330 -0.155084   0.791788 -0.019637  0.206729  0.725996  7.492760\n",
       "145 -0.443515 -2.081792  -0.909197 -0.019637 -0.386938  0.403203  7.598399\n",
       "146 -0.443515 -0.155084  -0.909197 -1.519579 -0.694802 -0.376494  7.309881\n",
       "147 -0.443515 -1.600115   1.786801  1.963178  1.693412 -1.965715  8.214736\n",
       "148 -0.443515 -2.081792  -0.909197 -0.019637 -0.605655 -2.968402  7.647309\n",
       "149  0.458299 -0.155084   1.786801 -0.019637  0.639107 -1.965715  8.004700\n",
       "150  0.458299 -0.155084   0.085816 -0.019637  0.093129 -0.599484  7.647309\n",
       "151 -1.345330 -0.155084   0.791788 -0.019637 -0.071755  0.403203  7.408531\n",
       "152 -0.443515 -1.600115   0.791788 -0.019637 -0.386938  0.403203  7.522941\n",
       "153  0.458299 -0.155084  -0.909197  0.217293 -2.131557  0.403203  6.907755\n",
       "154  0.458299 -0.155084  -0.909197 -0.019637 -0.423096  0.255249  7.647309\n",
       "155  0.458299 -0.155084  -0.909197 -1.519579 -0.509907  0.403203  7.492760\n",
       "156  0.458299 -0.155084   1.339383  0.857772  1.220605 -0.153504  8.699515\n",
       "157 -0.443515 -0.155084  -0.909197  0.217293  1.459357 -4.194079  7.183112\n",
       "158 -1.345330 -2.563469   0.791788 -0.019637  0.864294 -2.188705  7.649693\n",
       "159 -0.443515 -0.155084  -0.909197  0.857772  0.850889  0.834346  8.146130\n",
       "160 -0.443515 -0.155084   0.085816  0.857772  0.231976  0.773428  7.843849\n",
       "161  1.360114 -0.155084   1.339383  0.857772  1.020194 -0.309200  8.536996\n",
       "162 -0.443515 -0.636761  -0.909197  0.217293  0.109323  0.403203  7.146772\n",
       "163 -0.443515 -0.155084   0.791788 -0.019637  0.850889  0.948986  7.822044\n",
       "164 -0.443515  0.326593  -0.909197 -0.019637  0.850889  0.555614  7.901007\n",
       "165  1.360114  1.771624  -0.909197 -0.019637  0.109323  0.403203  8.281471\n",
       "166  0.458299 -0.155084  -0.909197 -1.519579  0.109323 -0.411353  7.546974\n",
       "167  1.360114 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.600902\n",
       "168  1.360114  1.771624  -0.909197 -1.519579 -0.927611  0.403203  8.086410\n",
       "169 -1.345330 -0.155084  -0.909197 -1.519579 -0.808059  0.403203  7.130899\n",
       "170 -0.443515 -0.155084   1.339383 -0.019637  0.177508 -1.965715  7.693937\n",
       "171 -1.345330 -0.155084   1.786801  1.480304  1.683391  0.065262  8.070906\n",
       "172  1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  7.970740\n",
       "173 -1.345330 -0.155084   0.791788  0.857772  0.419021 -1.865912  7.240650\n",
       "174  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.163372  7.803843\n",
       "175  1.360114  0.808270  -0.909197 -1.519579 -0.386938  0.403203  7.937375\n",
       "176  1.360114  0.808270  -0.909197  0.857772  0.109323  0.403203  8.262301\n",
       "177 -0.443515 -0.155084  -0.909197 -0.019637 -0.160267  0.403203  7.588324\n",
       "178  0.458299 -0.155084   0.791788  1.480304  0.523283 -0.309200  7.989560\n",
       "179  1.360114  2.253301   0.085816  0.857772  0.712043  0.403203  8.516193\n",
       "180 -0.443515  0.326593  -0.909197  0.217293 -1.651490  0.921166  7.204149\n",
       "181  1.360114  0.808270  -0.909197 -0.019637  0.485372  0.741980  8.292799\n",
       "182 -1.345330 -0.155084   0.791788  0.857772  1.236399  0.341637  7.822044\n",
       "183  1.360114  1.771624  -0.909197 -1.519579 -0.779181  0.989737  8.070906\n",
       "184 -0.443515 -1.600115   0.791788  0.857772 -0.386938  0.186894  7.696213\n",
       "185 -1.345330 -0.155084   1.339383  0.857772  1.675157 -1.247584  7.783224\n",
       "186  0.458299 -0.155084   0.791788  0.857772  0.513868  0.442837  8.779557\n",
       "187  0.458299 -0.155084  -0.909197  0.217293  0.109323  0.403203  7.240650\n",
       "188  1.360114  0.808270   1.339383  1.480304  0.712043  0.403203  8.216088\n",
       "189  1.360114  1.771624   0.791788 -0.019637  0.044781 -0.244897  8.607948\n",
       "190  1.360114 -0.155084  -0.909197 -1.519579 -0.640334  1.312530  7.824046\n",
       "191 -1.345330 -0.155084  -0.909197  1.480304 -0.045081  0.186894  7.329750\n",
       "192 -1.345330 -0.155084   0.791788  0.857772  1.177141 -2.188705  8.411833\n",
       "193  0.458299 -0.155084   1.339383 -0.019637  0.580450 -0.376494  7.824046\n",
       "194 -0.443515 -0.155084  -0.909197 -1.519579 -1.073846 -1.449760  7.522941\n",
       "195  0.458299 -0.155084   0.791788 -0.019637  0.109323 -5.783300  7.783224\n",
       "196  1.360114 -0.155084   0.791788  0.857772  0.109323  0.403203  8.411833\n",
       "197  0.458299 -0.155084   0.791788  0.857772  0.743367  0.255249  8.069342\n",
       "198  1.360114 -0.155084  -0.909197 -1.519579 -1.739314  0.863869  7.972466\n",
       "199 -0.443515 -2.081792   0.791788  0.857772  0.930090  1.105524  8.101678\n",
       "200 -0.443515 -2.081792  -0.909197 -1.519579 -1.087045 -2.775240  7.377759\n",
       "201 -0.443515  0.326593  -0.909197 -0.019637  0.639107  0.976280  7.861342\n",
       "202 -1.345330 -0.155084  -0.909197  0.857772 -0.160267  0.299088  7.158514\n",
       "203  0.458299 -0.155084   1.786801  0.857772  1.362987  0.013593  8.516193\n",
       "204 -0.443515 -0.155084   2.492774  1.480304  1.732145  0.788900  8.342840\n",
       "205 -1.345330 -0.155084  -0.909197 -1.519579 -1.485918  0.232821  6.902743\n",
       "206 -1.345330 -0.155084   1.786801  1.963178  2.164918  0.065262  8.342840\n",
       "207  1.360114  2.253301   0.085816  0.857772  0.109323  0.403203  8.740337\n",
       "208 -0.443515 -0.155084   1.786801  0.857772  0.453261 -1.965715  7.824046\n",
       "209 -0.443515 -0.155084  -0.909197 -1.519579 -1.369599  0.403203  7.189168\n",
       "210  0.458299 -0.155084   0.791788  1.480304  1.027638  0.382970  8.159089\n",
       "211 -1.345330 -0.155084   1.339383 -0.019637  0.222993 -0.447073  7.377759\n",
       "212 -1.345330 -0.155084  -0.909197 -0.019637 -0.703093  0.500324  7.313220\n",
       "213 -0.443515 -0.636761  -0.909197 -1.519579 -0.927611  0.788900  7.166266\n",
       "214  1.360114  1.771624  -0.909197 -0.019637  0.044781  0.403203  8.242756\n",
       "215  1.360114  0.808270   1.786801  0.857772  1.173672 -2.072918  8.674197\n",
       "216  0.458299 -0.155084   1.339383  0.857772  0.812907  0.163372  8.294050\n",
       "217  1.360114  0.808270   0.791788  0.857772 -0.386938  0.403203  8.214736\n",
       "218 -1.345330 -0.155084   0.791788  0.857772  1.146889  0.065262  7.649693\n",
       "219 -0.443515 -1.600115  -0.909197 -1.519579 -1.579560  0.403203  6.998510\n",
       "220  0.458299 -0.155084   1.786801  1.963178  0.109323  0.403203  8.006368\n",
       "221 -0.443515 -1.600115   0.791788  1.480304  1.104286  0.976280  8.268732\n",
       "222  0.458299 -0.155084   0.791788 -0.019637 -0.522600  0.403203  7.762171\n",
       "223 -0.443515  0.326593  -0.909197  0.217293 -2.026591  0.403203  7.090077\n",
       "224  1.360114  2.253301   1.786801  0.857772  1.330956  0.403203  8.762490\n",
       "225  0.458299 -0.155084   0.791788  0.857772  1.220605  1.055204  7.937375\n",
       "226 -1.345330 -0.155084   0.085816 -0.019637 -0.041006 -0.521272  7.588324\n",
       "227 -1.345330 -0.155084   1.339383  1.480304  1.166717 -0.559850  7.647309\n",
       "228  0.458299 -0.155084   0.791788  0.857772  0.569751  0.320519  8.053569\n",
       "229  0.458299 -0.155084   0.791788  0.857772  0.109323  0.403203  7.962067\n",
       "230  0.458299 -0.155084  -0.909197 -0.019637 -0.632281  0.403203  7.824046\n",
       "231  1.360114  2.253301  -0.909197 -1.519579 -1.087045  0.403203  7.901007\n",
       "232 -0.443515 -0.155084  -0.909197 -1.519579 -0.779181  0.341637  7.326466\n",
       "233 -1.345330 -0.155084   0.791788  0.857772  1.995634  0.863869  7.963808\n",
       "234 -1.345330 -0.155084  -0.909197 -0.019637  0.453261  0.741980  7.598399\n",
       "235  0.458299 -0.155084   2.165087  0.857772  0.873627 -5.783300  8.131531\n",
       "236 -0.443515 -0.636761  -0.909197 -0.019637 -0.779181  0.403203  7.598399\n",
       "237 -0.443515  0.326593  -0.909197 -0.019637 -0.640334  0.403203  7.783224\n",
       "238 -0.443515 -2.081792  -0.909197  0.857772 -0.248091  0.403203  7.673223\n",
       "239  1.360114 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.484369\n",
       "240 -1.345330 -0.155084  -0.909197 -0.019637  0.601691  0.921166  7.435438\n",
       "241 -0.443515 -0.636761  -0.909197 -1.519579  0.109323 -3.191392  7.441320\n",
       "242  1.360114  2.253301  -0.909197 -0.019637 -0.603010  1.003069  8.039157\n",
       "243 -1.345330 -2.563469   0.791788 -0.019637  0.864294 -2.188705  7.649693\n",
       "244  0.458299 -0.155084   0.791788  1.480304  1.321253  0.232821  8.188689\n",
       "245 -0.443515 -0.155084  -0.909197 -1.519579 -1.087045  0.403203  7.393263\n",
       "246 -0.443515  0.326593  -0.909197 -1.519579 -0.509907  0.403203  7.600902\n",
       "247 -1.345330 -0.155084   0.791788 -0.019637  0.646513 -0.095634  7.377759\n",
       "248  1.360114  1.771624   0.791788 -0.019637 -0.160267  0.403203  8.496990\n",
       "249 -0.443515  0.326593  -0.909197 -0.019637 -0.484745  0.819355  7.374629\n",
       "250  1.360114  0.808270  -0.909197 -0.019637  0.639107  0.573584  7.649693\n",
       "251  1.360114  0.808270  -0.909197 -0.019637  1.976595 -0.411353  8.389360\n",
       "252  1.360114 -0.155084   1.786801  0.857772  0.618235  0.210040  8.517193\n",
       "253  1.360114 -0.155084  -0.909197 -1.519579 -0.796461 -0.153504  7.861342\n",
       "254  1.360114  1.771624   0.791788 -0.019637  0.109323  0.403203  8.455318\n",
       "255  1.360114  1.289947   1.786801  2.357714  0.524848  0.442837  8.612503\n",
       "256  0.458299 -0.155084   0.791788  0.857772  0.352254  0.163372  8.131531\n",
       "257  1.360114  1.771624   0.085816 -0.019637  0.445158  0.403203  8.565983\n",
       "258 -1.345330 -0.155084  -0.909197 -0.019637  0.583498  0.725996  7.362011\n",
       "259  0.458299 -0.155084   0.791788 -0.019637  0.109323  0.403203  7.673223\n",
       "260 -1.345330 -0.155084  -0.909197 -0.019637  0.054538  0.537418  8.160518\n",
       "261 -0.443515 -0.155084   0.085816 -0.019637  0.064251 -2.072918  7.824046\n",
       "262 -1.345330 -0.155084   1.339383 -0.019637  0.399208 -0.599484  7.435438\n",
       "263 -0.443515 -0.155084  -0.909197 -0.019637 -0.248091  0.442837  7.696213\n",
       "264 -0.443515 -0.155084   1.339383 -0.019637  0.532657 -2.072918  7.863267\n",
       "265  1.360114  2.253301  -0.909197 -1.519579  0.109323  0.403203  8.004700\n",
       "266 -1.345330 -0.155084  -0.909197 -1.519579 -1.775473  0.403203  6.906755\n",
       "267 -0.443515 -0.155084   0.085816 -0.019637  0.231976 -3.191392  7.824046\n",
       "268  0.458299 -1.118438   1.786801  0.857772  1.220605 -0.342456  7.989560\n",
       "269  0.458299 -0.155084   0.791788  0.857772  0.364089 -5.783300  8.101678\n",
       "270  1.360114  2.253301  -0.909197 -1.519579 -1.538344  1.055204  7.822044\n",
       "271 -1.345330 -2.563469   1.786801 -0.019637  2.019198 -1.772553  8.611594\n",
       "272 -1.345330 -0.155084   0.791788 -0.019637  0.005305 -0.095634  7.170120\n",
       "273 -0.443515 -0.155084  -0.909197  0.217293 -2.418834  0.403203  6.998510\n",
       "274  1.360114 -0.155084  -0.909197 -1.519579 -0.386938  0.403203  7.899153\n",
       "275 -0.443515 -0.636761   1.786801  0.857772  0.837401  0.139461  8.154788\n",
       "276 -0.443515  0.326593  -0.909197 -0.019637  0.140414 -0.244897  7.781139\n",
       "277 -1.345330 -2.563469   0.791788 -0.019637  0.314630 -1.865912  7.495542\n",
       "278  1.360114 -0.155084  -0.909197 -1.519579  0.850889 -0.244897  7.495542\n",
       "279 -0.443515  0.326593  -0.909197 -0.019637 -0.160267  0.403203  7.575585\n",
       "280  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  7.901007\n",
       "281 -1.345330 -0.155084   0.791788  0.857772 -0.138860  0.403203  7.207860\n",
       "282 -1.345330 -0.155084   0.791788 -0.019637  0.399208 -0.376494  7.313220\n",
       "283 -0.443515 -0.155084  -0.909197 -0.019637 -0.509907  0.403203  7.435438\n",
       "284 -0.443515  0.326593  -0.909197 -0.019637 -0.160267  0.608868  7.740664\n",
       "285 -1.345330 -0.155084   0.791788 -0.019637 -0.401328 -0.521272  6.856462\n",
       "286 -0.443515 -2.081792   1.339383  0.857772  0.563613 -2.604858  8.004700\n",
       "287  0.458299 -0.155084   0.791788  0.857772  0.810168  0.210040  8.229511\n",
       "288  1.360114 -0.155084  -0.909197 -0.019637  0.726337  0.804208  8.507143\n",
       "289 -1.345330 -2.563469  -0.909197 -1.519579 -1.087045  0.115149  6.802395\n",
       "290 -0.443515 -2.081792  -0.909197  0.217293 -3.066626 -4.194079  7.047517\n",
       "291 -1.345330 -0.155084   0.791788  0.857772  0.456494  0.341637  7.696213\n",
       "292  1.360114 -0.155084   0.085816 -0.019637  0.109323  0.403203  7.862882\n",
       "293 -1.345330 -0.155084  -0.909197 -1.519579 -1.259247 -0.559850  7.130899\n",
       "294 -0.443515 -0.155084   0.791788 -0.019637  0.044781 -0.559850  7.647309\n",
       "295 -0.443515 -0.155084  -0.909197 -0.019637  0.768661  0.863869  7.738488\n",
       "296 -1.345330 -0.155084  -0.909197 -1.519579 -0.816798 -0.483697  6.856462\n",
       "297  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.549609\n",
       "298  1.360114  0.808270  -0.909197  0.857772  0.374182  0.989737  8.354674\n",
       "299 -0.443515 -1.600115  -0.909197 -0.019637 -0.248091  0.518990  7.937375\n",
       "300 -0.443515 -0.155084  -0.909197 -0.019637  0.404178  0.442837  7.600402\n",
       "301  0.458299 -0.155084  -0.909197 -0.019637 -0.640334 -0.411353  7.600902\n",
       "302 -1.345330 -0.155084   1.786801  0.857772  0.778418  0.403203  7.435438\n",
       "303  1.360114 -0.155084  -0.909197 -0.019637  0.109323  0.403203  8.039157\n",
       "304 -1.345330 -0.155084  -0.909197 -0.019637 -0.339653 -0.213787  6.903747\n",
       "305  0.458299 -0.155084   1.786801 -0.019637  0.221192 -2.314574  8.109225\n",
       "306 -0.443515 -2.081792   2.165087  0.857772  0.917091 -1.965715  8.070906\n",
       "307  0.458299 -0.155084  -0.909197 -1.519579  0.109323  0.403203  7.438384\n",
       "308 -1.345330 -0.155084   0.085816 -1.519579 -0.437732  0.320519  7.279319\n",
       "309 -1.345330 -0.155084  -0.909197 -0.019637 -0.512440  0.341637  7.130899\n",
       "310  0.458299 -0.155084   1.339383  0.857772  1.060822 -0.244897  8.202482\n",
       "311 -0.443515 -0.636761   0.085816  0.857772  1.526203  0.757790  8.039157\n",
       "312 -0.443515 -2.081792  -0.909197 -1.519579 -0.270619  0.403203  7.783224\n",
       "313  1.360114  1.771624  -0.909197 -0.019637 -0.730961  0.039658  8.229511\n",
       "314 -1.345330 -2.563469   1.339383 -0.019637  0.662717 -2.072918  7.824046\n",
       "315  1.360114 -0.155084  -0.909197 -1.519579 -0.667396  0.403203  7.495542\n",
       "316  0.458299 -0.155084   1.786801  1.963178  0.453261  0.773428  8.131531\n",
       "317 -1.345330 -0.155084  -0.909197 -1.519579 -1.317396 -0.599484  6.956545\n",
       "318  1.360114 -0.155084  -0.909197  0.217293 -1.651490  0.403203  7.625595\n",
       "319  1.360114  0.808270   0.085816  0.857772  0.712043  0.849182  8.444622\n",
       "320 -1.345330 -0.155084   0.791788  0.857772  0.906635 -0.376494  7.296413\n",
       "321 -0.443515  0.326593   0.791788 -0.019637  0.683165 -3.455133  8.281471\n",
       "322  1.360114  0.808270   0.791788  0.857772  0.981316  0.403203  8.779557\n",
       "323 -0.443515 -2.081792  -0.909197 -1.519579 -1.087045 -2.775240  7.377759\n",
       "324 -0.443515 -2.081792   1.339383 -0.019637 -0.316394 -2.072918  8.006368\n",
       "325 -1.345330 -0.155084   0.791788  0.857772  1.142196  0.186894  7.740664\n",
       "326  1.360114  1.771624  -0.909197 -1.519579 -1.259247  0.403203  7.989560\n",
       "327 -0.443515 -0.155084   0.791788 -0.019637  0.712043 -3.777927  8.202482\n",
       "328 -1.345330 -0.155084   0.791788  0.857772  0.515440  0.362451  7.501082\n",
       "329 -1.345330 -0.155084   0.791788  0.857772  0.548190 -2.775240  7.346010\n",
       "330  0.458299 -0.155084   0.791788 -0.019637  1.125693 -1.523959  8.699348\n",
       "331  1.360114  0.808270   0.791788 -0.019637  0.231976  0.555614  8.411833\n",
       "332 -0.443515 -0.155084  -0.909197 -0.019637 -0.694802  0.537418  7.989560\n",
       "333  0.458299 -1.118438  -0.909197 -1.519579 -1.005851  0.403203  7.495542\n",
       "334  1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.354674\n",
       "335 -1.345330 -0.155084  -0.909197  0.857772 -0.248091  0.788900  7.002156\n",
       "336 -0.443515 -2.081792  -0.909197 -1.519579 -1.255665  0.403203  6.398595\n",
       "337 -1.345330 -0.155084  -0.909197  0.217293 -1.259247  0.403203  6.620073\n",
       "338 -0.443515 -1.600115  -0.909197 -0.019637  0.712043 -3.191392  7.313220\n",
       "339 -0.443515 -0.155084   0.791788 -0.019637  1.104286  0.921166  7.738488\n",
       "340 -1.345330 -0.155084   1.339383  0.857772  0.726337  0.403203  7.789455\n",
       "341 -1.345330 -2.563469  -0.909197 -0.019637  0.062312  0.423156  8.294050\n",
       "342 -0.443515 -0.155084   0.791788  1.480304  1.330956  0.573584  8.216088\n",
       "343  0.458299 -1.118438  -0.909197 -0.019637  0.404178  0.403203  7.495542\n",
       "344 -1.345330 -0.155084   1.339383 -0.019637  0.222993 -0.447073  7.377759\n",
       "345  1.360114  1.289947   0.791788  1.480304  0.639107  0.643312  8.612503\n",
       "346 -0.443515 -1.600115  -0.909197  1.480304  0.109323  0.403203  7.824046\n",
       "347  1.360114  0.808270  -0.909197  0.857772  0.109323  0.403203  8.160518\n",
       "348 -0.443515 -0.636761  -0.909197 -1.519579 -1.259247  0.403203  7.492760\n",
       "349  0.458299 -0.155084  -0.909197 -0.019637 -0.464824  0.403203  7.696213\n",
       "350  1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.718685\n",
       "351 -0.443515 -0.155084   1.339383  0.857772  0.850889  0.013593  7.861342\n",
       "352 -1.345330 -0.155084   0.791788  0.857772  0.833338  0.403203  7.575585\n",
       "353  1.360114 -0.155084   1.786801  1.963178  1.627892  0.878407  8.699515\n",
       "354 -1.345330 -0.155084  -0.909197 -1.519579 -1.005851  0.403203  6.989335\n",
       "355 -0.443515 -1.600115  -0.909197 -1.519579 -0.927611  0.403203  7.374629\n",
       "356 -1.345330 -0.155084   0.791788 -0.019637  0.538884 -0.640236  7.549609\n",
       "357  0.458299 -0.155084  -0.909197 -0.019637 -0.386938 -4.194079  7.495542\n",
       "358  0.458299 -0.155084  -0.909197 -0.019637 -0.640334  0.403203  7.467371\n",
       "359  0.458299 -0.155084  -0.909197 -1.519579 -1.259247  0.403203  7.393263\n",
       "360  1.360114  1.771624   0.791788 -0.019637 -0.640334  1.029371  8.188689\n",
       "361  0.458299 -1.118438   0.791788 -0.019637  0.430495  0.403203  7.833996\n",
       "362  1.360114  0.808270  -0.909197 -1.519579  0.109323  0.403203  7.696213\n",
       "363 -0.443515 -0.155084   0.791788  1.480304  1.911887  0.255249  8.292799\n",
       "364 -1.345330 -0.155084   1.786801  1.480304  1.913529  0.892801  8.342840\n",
       "365 -1.345330 -0.155084  -0.909197 -0.019637  0.583498  0.725996  7.362011\n",
       "366 -0.443515 -0.155084   0.791788  0.857772  1.687041  0.065262  8.691146\n",
       "367  1.360114  2.253301  -0.909197 -0.019637  0.109323  0.403203  8.130059\n",
       "368 -0.443515 -0.155084  -0.909197 -1.519579 -1.446442  0.255249  7.170120\n",
       "369  1.360114  1.771624  -0.909197 -0.019637 -0.160267  1.067949  8.216088\n",
       "370  1.360114  2.253301  -0.909197 -1.519579 -0.610954  0.834346  8.202482\n",
       "371 -0.443515  0.326593   0.791788 -0.019637  0.365775  0.403203  7.901007\n",
       "372 -1.345330 -0.155084   0.791788  0.857772  1.858682  0.210040  8.146130\n",
       "373 -1.345330 -0.155084   0.791788 -0.019637  0.914482  0.626192  7.843849\n",
       "374  0.458299 -0.155084   0.791788 -0.019637 -0.339653  0.403203  7.740230\n",
       "375 -1.345330 -0.155084   0.791788 -0.019637  0.594129  0.403203  7.207860\n",
       "376  1.360114  0.808270  -0.909197 -0.019637  0.109323  0.403203  8.411833\n",
       "377  0.458299 -0.155084   0.791788  1.480304  1.330956  0.442837  8.070906\n",
       "378 -0.443515  0.326593   0.791788 -0.019637  0.423946 -0.342456  8.055158\n",
       "379  1.360114  0.808270  -0.909197  0.857772  0.501249  0.573584  8.255828\n",
       "380  1.360114  1.771624  -0.909197 -1.519579  0.109323  0.403203  8.160518\n",
       "381  1.360114  2.253301  -0.909197  0.857772  0.563613  0.403203  8.411833"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       7.803843\n",
      "1       7.696213\n",
      "2       8.629629\n",
      "3       7.955074\n",
      "4       7.824046\n",
      "5       8.342840\n",
      "6       7.963808\n",
      "7       6.744059\n",
      "8       8.294050\n",
      "9       7.244228\n",
      "10      7.740664\n",
      "11      7.935587\n",
      "12      7.863267\n",
      "13      7.189168\n",
      "14      7.625595\n",
      "15      7.989560\n",
      "16      7.492760\n",
      "17      6.906755\n",
      "18      7.824046\n",
      "19      7.649693\n",
      "20      8.216088\n",
      "21      7.130899\n",
      "22      7.393263\n",
      "23      7.494986\n",
      "24      8.055158\n",
      "25      8.516193\n",
      "26      7.495542\n",
      "27      8.131531\n",
      "28      7.377759\n",
      "29      7.600902\n",
      "30      7.492760\n",
      "31      6.543912\n",
      "32      8.116716\n",
      "33      8.330864\n",
      "34      7.438384\n",
      "35      7.824046\n",
      "36      7.740664\n",
      "37      7.955074\n",
      "38      7.781139\n",
      "39      8.101678\n",
      "40      7.783224\n",
      "41      7.989560\n",
      "42      7.467371\n",
      "43      7.919356\n",
      "44      8.069342\n",
      "45      8.100161\n",
      "46      7.309881\n",
      "47      7.408531\n",
      "48      8.039157\n",
      "49      7.467371\n",
      "50      7.166266\n",
      "51      7.803843\n",
      "52      7.822044\n",
      "53      7.464510\n",
      "54      7.492760\n",
      "55      7.696213\n",
      "56      7.377759\n",
      "57      8.188689\n",
      "58      7.824046\n",
      "59      7.972466\n",
      "60      7.562681\n",
      "61      7.696213\n",
      "62      8.159089\n",
      "63      7.467371\n",
      "64      7.696213\n",
      "65      8.242756\n",
      "66      7.861342\n",
      "67      7.438384\n",
      "68      8.268732\n",
      "69      7.972466\n",
      "70      7.309881\n",
      "71      8.006368\n",
      "72      7.861342\n",
      "73      7.937375\n",
      "74      8.004700\n",
      "75      7.600902\n",
      "76      8.242756\n",
      "77      8.159089\n",
      "78      8.188689\n",
      "79      6.932448\n",
      "80      8.699515\n",
      "81      7.374629\n",
      "82      7.090077\n",
      "83      7.693937\n",
      "84      7.863267\n",
      "85      7.822044\n",
      "86      7.824046\n",
      "87      7.878913\n",
      "88      7.085901\n",
      "89      7.600902\n",
      "90      7.546974\n",
      "91      7.970740\n",
      "92      7.130899\n",
      "93      7.696213\n",
      "94      7.233455\n",
      "95      7.408531\n",
      "96      7.824046\n",
      "97      7.824046\n",
      "98      7.696213\n",
      "99      7.377759\n",
      "100     7.738488\n",
      "101     8.341649\n",
      "102     7.464510\n",
      "103     7.467371\n",
      "104     7.346010\n",
      "105     7.882315\n",
      "106     8.070906\n",
      "107     7.240650\n",
      "108     7.435438\n",
      "109     8.318742\n",
      "110     7.970740\n",
      "111     7.863267\n",
      "112     7.240650\n",
      "113     6.745236\n",
      "114     7.843849\n",
      "115     7.546974\n",
      "116     7.783224\n",
      "117     6.398595\n",
      "118     8.131531\n",
      "119     8.160518\n",
      "120     7.691657\n",
      "121     7.495542\n",
      "122     7.881937\n",
      "123     7.374629\n",
      "124     8.516193\n",
      "125     7.374629\n",
      "126     8.131531\n",
      "127     7.438384\n",
      "128     7.362011\n",
      "129     7.882315\n",
      "130     7.989560\n",
      "131     7.522941\n",
      "132     7.279319\n",
      "133     7.085901\n",
      "134     7.003065\n",
      "135     8.411833\n",
      "136     6.802395\n",
      "137     8.455318\n",
      "138     7.928406\n",
      "139     7.740664\n",
      "140     7.489971\n",
      "141     7.598399\n",
      "142     7.598399\n",
      "143     7.935587\n",
      "144     7.313220\n",
      "145     8.455318\n",
      "146     7.783224\n",
      "147     7.362011\n",
      "148     7.154615\n",
      "149     8.476371\n",
      "150     8.070906\n",
      "151     7.113956\n",
      "152     7.776954\n",
      "153     8.740337\n",
      "154     7.955074\n",
      "155     7.492760\n",
      "156     7.600902\n",
      "157     8.160518\n",
      "158     7.937375\n",
      "159     8.292799\n",
      "160     7.863267\n",
      "161     7.649693\n",
      "162     7.122867\n",
      "163     8.455318\n",
      "164     8.101678\n",
      "165     7.549609\n",
      "166     8.101678\n",
      "167     7.309881\n",
      "168     7.244228\n",
      "169     8.268732\n",
      "170     8.294050\n",
      "171     7.309881\n",
      "172     8.188689\n",
      "173     7.647309\n",
      "174     8.809863\n",
      "175     8.612503\n",
      "176     8.006368\n",
      "177     8.281471\n",
      "178     6.902743\n",
      "179     8.146130\n",
      "180     7.822044\n",
      "181     7.495542\n",
      "182     8.037543\n",
      "183     8.507143\n",
      "184     8.188689\n",
      "185     7.509335\n",
      "186     8.100161\n",
      "187     7.309881\n",
      "188     8.188689\n",
      "189     7.696213\n",
      "190     8.268732\n",
      "191     8.006368\n",
      "192     7.438384\n",
      "193     7.549609\n",
      "194     7.955074\n",
      "195     7.312553\n",
      "196     8.039157\n",
      "197     7.822044\n",
      "198     7.598399\n",
      "199     8.006034\n",
      "200     7.546974\n",
      "201     7.346010\n",
      "202     7.696213\n",
      "203     7.170120\n",
      "204     8.069342\n",
      "205     7.882315\n",
      "206     7.362011\n",
      "207     7.562681\n",
      "208     7.781139\n",
      "209     8.070906\n",
      "210     7.684784\n",
      "211     8.100161\n",
      "212     7.824046\n",
      "213     7.306531\n",
      "214     8.187299\n",
      "215     6.715383\n",
      "216     7.313220\n",
      "217     7.937375\n",
      "218     8.400659\n",
      "219     7.495542\n",
      "220     7.813996\n",
      "221     8.241440\n",
      "222     7.955074\n",
      "223     8.240121\n",
      "224     7.974189\n",
      "225     7.575585\n",
      "226     8.342840\n",
      "227     7.207860\n",
      "228     7.972466\n",
      "229     7.313220\n",
      "230     8.699515\n",
      "231     6.856462\n",
      "232     7.647309\n",
      "233     8.167636\n",
      "234     7.549609\n",
      "235     7.575585\n",
      "236     8.517193\n",
      "237     7.419381\n",
      "238     7.718685\n",
      "239     7.598399\n",
      "240     8.294050\n",
      "241     7.972466\n",
      "242     7.919356\n",
      "243     7.740664\n",
      "244     7.691657\n",
      "245     8.342840\n",
      "246     7.265430\n",
      "247     7.919356\n",
      "248     7.863267\n",
      "249     7.166266\n",
      "250     8.485496\n",
      "251     7.244228\n",
      "252     8.160518\n",
      "253     8.130059\n",
      "254     8.039157\n",
      "255     6.388561\n",
      "256     7.781139\n",
      "257     8.188689\n",
      "258     7.090077\n",
      "259     8.214736\n",
      "260     7.824046\n",
      "261     7.781139\n",
      "262     7.166266\n",
      "263     7.693937\n",
      "264     7.673223\n",
      "265     7.997999\n",
      "266     7.377759\n",
      "267     8.294050\n",
      "268     7.467371\n",
      "269     7.972466\n",
      "270     8.330864\n",
      "271     7.649693\n",
      "272     7.467371\n",
      "273     8.341649\n",
      "274     7.598399\n",
      "275     7.824046\n",
      "276     6.902743\n",
      "277     8.779557\n",
      "278     7.170120\n",
      "279     7.495542\n",
      "280     7.110696\n",
      "281     7.296413\n",
      "282     8.411833\n",
      "283     7.935587\n",
      "284     7.588324\n",
      "285     7.309881\n",
      "286     7.598399\n",
      "287     7.693937\n",
      "288     7.408531\n",
      "289     6.906755\n",
      "290     8.202482\n",
      "291     8.255828\n",
      "292     8.039157\n",
      "293     8.465900\n",
      "294     7.937375\n",
      "295     8.004700\n",
      "296     7.374629\n",
      "297     7.467371\n",
      "298     8.229511\n",
      "299     8.216088\n",
      "300     8.486734\n",
      "301     7.762171\n",
      "302     7.435438\n",
      "303     7.824046\n",
      "304     7.244228\n",
      "305     7.987864\n",
      "306     8.004700\n",
      "307     8.006368\n",
      "308     7.374629\n",
      "309     7.377759\n",
      "310     7.536364\n",
      "311     7.824046\n",
      "312     7.824046\n",
      "313     7.408531\n",
      "314     8.187299\n",
      "315     8.039157\n",
      "316     7.781139\n",
      "317     7.495542\n",
      "318     7.419381\n",
      "319     7.824046\n",
      "320     8.294050\n",
      "321     8.517193\n",
      "322     7.090077\n",
      "323     7.346010\n",
      "324     7.522941\n",
      "325     7.084226\n",
      "326     7.696213\n",
      "327     7.438384\n",
      "328     7.170120\n",
      "329     7.901007\n",
      "330     7.813996\n",
      "331     7.649693\n",
      "332     7.130899\n",
      "333     7.824046\n",
      "334     7.438384\n",
      "335     8.054840\n",
      "336     7.598399\n",
      "337     7.824046\n",
      "338     7.970740\n",
      "339     6.620073\n",
      "340     7.522941\n",
      "341     8.086410\n",
      "342     7.843849\n",
      "343     7.901007\n",
      "344     7.492760\n",
      "345     7.170120\n",
      "346     8.188689\n",
      "347     7.696213\n",
      "348     7.492760\n",
      "349     7.970740\n",
      "350     7.240650\n",
      "351     7.492760\n",
      "352     7.989560\n",
      "353     7.764296\n",
      "354     7.467371\n",
      "355     6.998510\n",
      "356     7.393263\n",
      "357     7.824046\n",
      "358     7.467371\n",
      "359     7.110696\n",
      "360     8.389360\n",
      "361     8.039157\n",
      "362     7.862882\n",
      "363     7.492760\n",
      "364     7.408531\n",
      "365     7.901007\n",
      "366     7.090077\n",
      "367     7.649693\n",
      "368     7.989560\n",
      "369     7.374629\n",
      "370     7.467371\n",
      "371     8.006368\n",
      "372     7.997999\n",
      "373     7.861342\n",
      "374     8.160518\n",
      "375     6.927558\n",
      "376     7.408531\n",
      "377     7.781139\n",
      "378     7.901007\n",
      "379     7.150701\n",
      "380     8.188689\n",
      "381     8.188689\n",
      "382     8.514590\n",
      "383     7.467371\n",
      "384     7.696213\n",
      "385     7.166266\n",
      "386     7.740664\n",
      "387     8.594154\n",
      "388     7.989560\n",
      "389     7.740664\n",
      "390     7.549609\n",
      "391     7.718685\n",
      "392     7.003065\n",
      "393     7.740664\n",
      "394     7.435438\n",
      "395     7.955074\n",
      "396     7.598399\n",
      "397     7.560080\n",
      "398     7.625595\n",
      "399     7.937375\n",
      "400     7.935587\n",
      "401     7.919356\n",
      "402     7.346010\n",
      "403     7.090077\n",
      "404     7.824046\n",
      "405     7.362011\n",
      "406     7.600902\n",
      "407     7.972466\n",
      "408     7.575585\n",
      "409     8.159089\n",
      "410     7.492760\n",
      "411     8.342840\n",
      "412     7.110696\n",
      "413     8.433812\n",
      "414     7.522941\n",
      "415     7.673223\n",
      "416     8.069342\n",
      "417     8.086410\n",
      "418     6.998510\n",
      "419     7.673223\n",
      "420     7.110696\n",
      "421     8.039157\n",
      "422     8.763272\n",
      "423     8.240121\n",
      "424     7.803843\n",
      "425     7.536364\n",
      "426     7.204149\n",
      "427     7.625595\n",
      "428     8.131531\n",
      "429     7.130899\n",
      "430     7.781139\n",
      "431     7.492760\n",
      "432     7.492760\n",
      "433     8.070906\n",
      "434     7.783224\n",
      "435     7.501082\n",
      "436     8.160518\n",
      "437     7.824046\n",
      "438     7.716461\n",
      "439     7.240650\n",
      "440     7.495542\n",
      "441     8.517193\n",
      "442     7.740664\n",
      "443     8.006368\n",
      "444     7.279319\n",
      "445     7.987864\n",
      "446     7.972466\n",
      "447     7.414573\n",
      "448     7.696213\n",
      "449     7.522941\n",
      "450     7.882315\n",
      "451     7.598399\n",
      "452     7.233455\n",
      "453     7.901007\n",
      "454     7.738052\n",
      "455     7.696213\n",
      "456     8.188689\n",
      "457     7.649693\n",
      "458     7.937375\n",
      "459     8.070906\n",
      "460     7.937375\n",
      "461     7.085901\n",
      "462     7.495542\n",
      "463     7.937375\n",
      "464     7.937375\n",
      "465     7.313220\n",
      "466     8.433812\n",
      "467     8.101678\n",
      "468     8.146130\n",
      "469     7.130899\n",
      "470     8.159089\n",
      "471     7.309881\n",
      "472     8.411833\n",
      "473     8.188689\n",
      "474     6.927558\n",
      "475     7.863267\n",
      "476     7.935587\n",
      "477     7.789455\n",
      "478     7.598399\n",
      "479     7.937375\n",
      "480     7.783224\n",
      "481     7.781139\n",
      "482     7.342779\n",
      "483     7.575585\n",
      "484     8.070906\n",
      "485     7.673223\n",
      "486     7.244228\n",
      "487     8.294050\n",
      "488     7.989560\n",
      "489     8.389360\n",
      "490     7.696213\n",
      "491     8.188689\n",
      "492     8.039157\n",
      "493     8.131531\n",
      "494     8.411833\n",
      "495     7.299797\n",
      "496     8.156223\n",
      "497     7.598399\n",
      "498     7.693937\n",
      "499     8.055158\n",
      "500     8.612503\n",
      "501     8.665613\n",
      "502     7.740230\n",
      "503     8.070906\n",
      "504     7.408531\n",
      "505     7.408531\n",
      "506     7.575585\n",
      "507     7.919356\n",
      "508     7.408531\n",
      "509     7.882315\n",
      "510     7.647309\n",
      "511     7.781139\n",
      "512     7.824046\n",
      "513     7.718685\n",
      "514     7.863267\n",
      "515     7.673223\n",
      "516     7.740664\n",
      "517     7.919356\n",
      "518     7.955074\n",
      "519     7.824046\n",
      "520     8.593228\n",
      "521     8.101678\n",
      "522     7.435438\n",
      "523     8.318742\n",
      "524     6.882437\n",
      "525     8.160518\n",
      "526     7.060476\n",
      "527     8.365207\n",
      "528     7.673223\n",
      "529     7.207860\n",
      "530     6.882437\n",
      "531     7.882315\n",
      "532     6.956545\n",
      "533     8.070906\n",
      "534     7.919356\n",
      "535     7.467371\n",
      "536     8.160518\n",
      "537     8.037543\n",
      "538     8.101678\n",
      "539     7.408531\n",
      "540     7.492760\n",
      "541     8.004700\n",
      "542     7.989560\n",
      "543     7.541683\n",
      "544     7.467371\n",
      "545     7.313220\n",
      "546     7.408531\n",
      "547     7.495542\n",
      "548     7.815611\n",
      "549     7.972121\n",
      "550     7.279319\n",
      "551     7.822044\n",
      "552     8.411833\n",
      "553     7.374629\n",
      "554     6.956545\n",
      "555     7.598399\n",
      "556     7.901007\n",
      "557     7.279319\n",
      "558     7.970740\n",
      "559     7.972466\n",
      "560     8.411833\n",
      "561     7.374629\n",
      "562     7.600402\n",
      "563     7.313220\n",
      "564     8.160518\n",
      "565     7.495542\n",
      "566     8.131531\n",
      "567     7.989560\n",
      "568     8.039157\n",
      "569     7.882315\n",
      "570     7.546974\n",
      "571     7.374629\n",
      "572     8.131531\n",
      "573     7.495542\n",
      "574     8.432724\n",
      "575     7.937375\n",
      "576     7.492760\n",
      "577     7.166266\n",
      "578     7.495542\n",
      "579     7.400010\n",
      "580     7.972466\n",
      "581     8.517193\n",
      "582     7.393263\n",
      "583     8.516193\n",
      "584     8.682708\n",
      "585     7.495542\n",
      "586     7.595890\n",
      "587     7.377759\n",
      "588     7.408531\n",
      "589     8.292799\n",
      "590     8.214736\n",
      "591     7.822044\n",
      "592     7.598399\n",
      "593     7.598399\n",
      "594     7.546974\n",
      "595     7.693937\n",
      "596     7.600902\n",
      "597     7.130899\n",
      "598     7.637716\n",
      "599     7.522941\n",
      "600     8.160518\n",
      "601     7.740664\n",
      "602     7.647309\n",
      "603     7.881937\n",
      "604     8.268732\n",
      "605     8.116716\n",
      "606     8.160518\n",
      "607     7.673223\n",
      "608     8.131531\n",
      "609     8.317522\n",
      "610     8.160518\n",
      "611     8.202482\n",
      "612     7.495542\n",
      "613     7.696213\n",
      "614     8.354674\n",
      "615     7.313220\n",
      "616     7.309881\n",
      "617     8.174703\n",
      "618     8.294050\n",
      "619     7.377759\n",
      "620     8.242756\n",
      "621     8.146130\n",
      "622     7.546974\n",
      "623     7.374629\n",
      "624     6.829794\n",
      "625     7.562681\n",
      "626     8.240121\n",
      "627     8.006368\n",
      "628     7.863267\n",
      "629     8.159089\n",
      "630     7.278629\n",
      "631     7.441320\n",
      "632     6.796824\n",
      "633     7.464510\n",
      "634     7.467371\n",
      "635     7.803843\n",
      "636     8.187299\n",
      "637     7.882315\n",
      "638     8.039157\n",
      "639     7.824046\n",
      "640     8.131531\n",
      "641     7.578145\n",
      "642     7.313220\n",
      "643     7.309881\n",
      "644     7.170120\n",
      "645     7.346010\n",
      "646     8.130059\n",
      "647     7.781139\n",
      "648     7.368340\n",
      "649     6.998510\n",
      "650     7.647309\n",
      "651     6.956545\n",
      "652     7.313220\n",
      "653     7.150701\n",
      "654     7.600902\n",
      "655     7.762171\n",
      "656     7.309881\n",
      "657     8.229511\n",
      "658     7.696213\n",
      "659     7.935587\n",
      "660     8.281471\n",
      "661     8.160518\n",
      "662     8.242756\n",
      "663     7.718685\n",
      "664     7.803843\n",
      "665     7.158514\n",
      "666     7.600902\n",
      "667     7.625595\n",
      "668     8.444622\n",
      "669     7.492760\n",
      "670     8.411833\n",
      "671     7.025538\n",
      "672     7.997999\n",
      "673     7.598399\n",
      "674     7.740664\n",
      "675     7.882315\n",
      "676     8.101678\n",
      "677     8.389360\n",
      "678     8.101678\n",
      "679     8.031060\n",
      "680     6.989335\n",
      "681     7.492760\n",
      "682     8.389360\n",
      "683     7.244228\n",
      "684     7.069023\n",
      "685     7.495542\n",
      "686     7.824046\n",
      "687     8.611594\n",
      "688     7.244228\n",
      "689     8.159089\n",
      "690     8.366370\n",
      "691     7.090077\n",
      "692     8.410721\n",
      "693     8.292799\n",
      "694     8.003029\n",
      "695     7.377759\n",
      "696     8.432724\n",
      "697     7.989560\n",
      "698     8.621553\n",
      "699     7.762171\n",
      "700     8.377931\n",
      "701     7.649693\n",
      "702     7.824046\n",
      "703     8.495970\n",
      "704     8.229511\n",
      "705     7.919356\n",
      "706     7.647309\n",
      "707     7.495542\n",
      "708     8.556414\n",
      "709     7.972466\n",
      "710     7.312553\n",
      "711     8.242756\n",
      "712     8.131531\n",
      "713     7.781139\n",
      "714     7.972466\n",
      "715     8.187299\n",
      "716     7.588324\n",
      "717     8.084871\n",
      "718     8.556414\n",
      "719     7.901007\n",
      "720     7.408531\n",
      "721     7.549609\n",
      "722     8.070906\n",
      "723     8.006368\n",
      "724     7.824046\n",
      "725     6.388561\n",
      "726     8.294050\n",
      "727     7.419381\n",
      "728     8.006368\n",
      "729     7.309881\n",
      "730     7.536364\n",
      "731     7.740664\n",
      "732     7.244228\n",
      "733     8.281471\n",
      "734     8.465900\n",
      "735     7.882315\n",
      "736     7.279319\n",
      "737     7.481556\n",
      "738     7.090077\n",
      "739     7.346010\n",
      "740     7.207860\n",
      "741     8.116716\n",
      "742     7.170120\n",
      "743     6.902743\n",
      "744     7.492760\n",
      "745     7.309881\n",
      "746     6.856462\n",
      "747     7.937375\n",
      "748     8.377931\n",
      "749     7.435438\n",
      "750     7.536364\n",
      "751     7.207860\n",
      "752     7.313220\n",
      "753     6.214608\n",
      "754     8.116716\n",
      "755     7.989560\n",
      "756     8.342840\n",
      "757     7.279319\n",
      "758     7.495542\n",
      "759     6.956545\n",
      "760     7.279319\n",
      "761     7.085901\n",
      "762     7.843849\n",
      "763     7.693937\n",
      "764     7.279319\n",
      "765     7.047517\n",
      "766     7.313220\n",
      "767     8.779557\n",
      "768     7.034388\n",
      "769     7.575585\n",
      "770     7.377759\n",
      "771     7.647309\n",
      "772     8.004700\n",
      "773     7.562681\n",
      "774     7.997999\n",
      "775     7.878913\n",
      "776     7.600902\n",
      "777     7.047517\n",
      "778     8.003029\n",
      "779     7.696213\n",
      "780     7.972466\n",
      "781     7.598399\n",
      "782     7.822044\n",
      "783     7.279319\n",
      "784     7.090077\n",
      "785     7.718685\n",
      "786     6.961296\n",
      "787     7.673223\n",
      "788     8.411833\n",
      "789     8.070906\n",
      "790     7.979339\n",
      "791     7.600402\n",
      "792     6.998510\n",
      "793     8.229511\n",
      "794     7.600402\n",
      "795     7.822044\n",
      "796     7.264730\n",
      "797     8.242756\n",
      "798     7.861342\n",
      "799     7.329750\n",
      "800     7.549609\n",
      "801     7.762171\n",
      "802     7.598399\n",
      "803     7.901007\n",
      "804     7.970740\n",
      "805     7.467371\n",
      "806     7.522941\n",
      "807     7.207860\n",
      "808     8.556414\n",
      "809     7.146772\n",
      "810     6.684612\n",
      "811     8.268732\n",
      "812     8.160518\n",
      "813     8.281471\n",
      "814     8.086410\n",
      "815     8.094073\n",
      "816     6.802395\n",
      "817     8.444622\n",
      "818     8.160518\n",
      "819     8.330864\n",
      "820     8.101678\n",
      "821     7.509335\n",
      "822     7.549609\n",
      "823     7.240650\n",
      "824     7.863267\n",
      "825     6.956545\n",
      "826     7.937375\n",
      "827     7.575585\n",
      "828     7.696213\n",
      "829     7.575585\n",
      "830     8.294050\n",
      "831     8.294050\n",
      "832     8.594154\n",
      "833     8.070906\n",
      "834     7.495542\n",
      "835     8.160518\n",
      "836     7.972466\n",
      "837     7.783224\n",
      "838     7.377759\n",
      "839     8.005701\n",
      "840     7.600902\n",
      "841     7.264730\n",
      "842     7.003065\n",
      "843     7.522941\n",
      "844     6.774224\n",
      "845     7.240650\n",
      "846     7.522941\n",
      "847     8.612503\n",
      "848     7.309881\n",
      "849     8.131531\n",
      "850     6.956545\n",
      "851     8.131531\n",
      "852     8.006368\n",
      "853     7.987864\n",
      "854     8.006368\n",
      "855     7.374629\n",
      "856     8.116716\n",
      "857     8.188689\n",
      "858     7.492760\n",
      "859     8.612503\n",
      "860     7.279319\n",
      "861     7.313220\n",
      "862     8.037543\n",
      "863     7.492760\n",
      "864     8.187299\n",
      "865     7.522941\n",
      "866     8.037543\n",
      "867     8.732305\n",
      "868     7.901007\n",
      "869     7.598399\n",
      "870     7.130899\n",
      "871     7.495542\n",
      "872     8.486734\n",
      "873     8.242756\n",
      "874     7.673223\n",
      "875     8.428362\n",
      "876     7.600902\n",
      "877     8.318742\n",
      "878     7.166266\n",
      "879     7.989560\n",
      "880     7.346010\n",
      "881     8.691146\n",
      "882     8.070906\n",
      "883     7.824046\n",
      "884     7.467371\n",
      "885     7.575585\n",
      "886     7.598399\n",
      "887     7.495542\n",
      "888     7.863267\n",
      "889     7.481556\n",
      "890     8.070906\n",
      "891     7.362011\n",
      "892     7.989560\n",
      "893     6.796824\n",
      "894     7.937375\n",
      "895     8.698681\n",
      "896     7.261927\n",
      "897     7.637716\n",
      "898     7.696213\n",
      "899     7.649693\n",
      "900     8.242756\n",
      "901     7.863267\n",
      "902     7.783224\n",
      "903     7.901007\n",
      "904     8.699515\n",
      "905     8.476371\n",
      "906     7.309881\n",
      "907     8.101678\n",
      "908     8.341649\n",
      "909     7.937375\n",
      "910     8.748305\n",
      "911     7.600902\n",
      "912     7.598399\n",
      "913     7.781139\n",
      "914     7.824046\n",
      "915     7.342779\n",
      "916     8.070906\n",
      "917     8.507143\n",
      "918     7.377759\n",
      "919     7.696213\n",
      "920     6.796824\n",
      "921     7.783224\n",
      "922     8.004700\n",
      "923     8.366370\n",
      "924     7.647309\n",
      "925     8.101678\n",
      "926     7.090077\n",
      "927     7.244228\n",
      "928     7.110696\n",
      "929     7.090077\n",
      "930     7.126891\n",
      "931     7.649693\n",
      "932     7.863267\n",
      "933     7.279319\n",
      "934     8.267449\n",
      "935     8.160518\n",
      "936     7.937375\n",
      "937     7.822044\n",
      "938     7.972121\n",
      "939     7.625595\n",
      "940     8.366370\n",
      "941     7.419381\n",
      "942     8.779557\n",
      "943     7.309881\n",
      "944     7.090077\n",
      "945     8.086103\n",
      "946     7.824046\n",
      "947     7.901007\n",
      "948     8.465900\n",
      "949     8.131531\n",
      "950     7.309881\n",
      "951     7.783224\n",
      "952     7.598399\n",
      "953     7.718685\n",
      "954     7.863267\n",
      "955     8.699515\n",
      "956     7.207860\n",
      "957     7.822044\n",
      "958     7.313220\n",
      "959     7.989560\n",
      "960     8.086410\n",
      "961     7.541683\n",
      "962     7.522941\n",
      "963     7.935587\n",
      "964     8.292799\n",
      "965     7.170120\n",
      "966     8.131531\n",
      "967     8.465900\n",
      "968     8.242756\n",
      "969     7.989560\n",
      "970     7.823246\n",
      "971     8.629629\n",
      "972     8.202482\n",
      "973     7.522941\n",
      "974     8.006368\n",
      "975     8.006368\n",
      "976     8.160518\n",
      "977     7.593374\n",
      "978     7.699842\n",
      "979     7.150701\n",
      "980     8.255828\n",
      "981     7.970740\n",
      "982     7.740664\n",
      "983     7.549609\n",
      "984     7.600902\n",
      "985     7.824046\n",
      "986     7.900266\n",
      "987     7.598399\n",
      "988     7.937375\n",
      "989     7.863267\n",
      "990     8.006368\n",
      "991     8.098643\n",
      "992     7.882315\n",
      "993     8.229511\n",
      "994     7.989560\n",
      "995     7.374629\n",
      "996     8.160518\n",
      "997     7.781139\n",
      "998     7.244228\n",
      "999     7.522941\n",
      "1000    7.408531\n",
      "1001    6.840547\n",
      "1002    7.824046\n",
      "1003    6.543912\n",
      "1004    7.374629\n",
      "1005    7.901007\n",
      "1006    7.575585\n",
      "1007    7.240650\n",
      "1008    7.575585\n",
      "1009    6.840547\n",
      "1010    8.737132\n",
      "1011    6.882437\n",
      "1012    7.598399\n",
      "1013    7.600902\n",
      "1014    8.070906\n",
      "1015    8.188689\n",
      "1016    8.006368\n",
      "1017    7.240650\n",
      "1018    6.998510\n",
      "1019    6.829794\n",
      "1020    7.937375\n",
      "1021    7.595890\n",
      "1022    7.085901\n",
      "1023    7.673223\n",
      "1024    7.306531\n",
      "1025    8.779557\n",
      "1026    7.204149\n",
      "1027    7.824046\n",
      "1028    7.696213\n",
      "1029    8.146130\n",
      "1030    7.478170\n",
      "1031    7.279319\n",
      "1032    6.476972\n",
      "1033    7.130899\n",
      "1034    6.774224\n",
      "1035    7.413367\n",
      "1036    8.555452\n",
      "1037    7.824046\n",
      "1038    7.313220\n",
      "1039    7.090077\n",
      "1040    7.522941\n",
      "1041    7.899153\n",
      "1042    7.649693\n",
      "1043    8.037543\n",
      "1044    7.937375\n",
      "1045    7.438384\n",
      "1046    7.972466\n",
      "1047    7.718685\n",
      "1048    7.935587\n",
      "1049    7.309881\n",
      "1050    8.242756\n",
      "1051    8.292799\n",
      "1052    7.522941\n",
      "1053    8.242493\n",
      "1054    7.351158\n",
      "1055    8.160518\n",
      "1056    8.611594\n",
      "1057    7.090077\n",
      "1058    6.796824\n",
      "1059    7.429521\n",
      "1060    7.492760\n",
      "1061    8.160518\n",
      "1062    7.279319\n",
      "1063    7.435438\n",
      "1064    7.824046\n",
      "1065    8.100161\n",
      "1066    7.901007\n",
      "1067    7.955074\n",
      "1068    7.693937\n",
      "1069    7.377759\n",
      "1070    7.313220\n",
      "1071    7.313220\n",
      "1072    7.536364\n",
      "1073    7.495542\n",
      "1074    8.229511\n",
      "1075    7.467371\n",
      "1076    7.878913\n",
      "1077    7.823246\n",
      "1078    7.740664\n",
      "1079    7.593374\n",
      "1080    7.598399\n",
      "1081    8.188689\n",
      "1082    7.600902\n",
      "1083    7.495542\n",
      "1084    7.522941\n",
      "1085    7.824046\n",
      "1086    8.100161\n",
      "1087    8.740337\n",
      "1088    8.267449\n",
      "1089    8.517193\n",
      "1090    8.101678\n",
      "1091    7.899153\n",
      "1092    7.937375\n",
      "1093    7.843849\n",
      "1094    7.279319\n",
      "1095    7.882315\n",
      "1096    7.130899\n",
      "1097    8.455318\n",
      "1098    7.342779\n",
      "1099    7.824046\n",
      "1100    7.150701\n",
      "1101    7.740664\n",
      "1102    7.244228\n",
      "1103    7.435438\n",
      "1104    7.313220\n",
      "1105    8.006368\n",
      "1106    7.279319\n",
      "1107    8.004700\n",
      "1108    7.598399\n",
      "1109    7.598399\n",
      "1110    7.637716\n",
      "1111    7.598399\n",
      "1112    8.101678\n",
      "1113    8.612503\n",
      "1114    7.937375\n",
      "1115    8.465900\n",
      "1116    7.972121\n",
      "1117    7.244228\n",
      "1118    7.313220\n",
      "1119    7.346010\n",
      "1120    7.438384\n",
      "1121    7.170120\n",
      "1122    7.346010\n",
      "1123    7.166266\n",
      "1124    8.004700\n",
      "1125    7.861342\n",
      "1126    7.244228\n",
      "1127    8.281471\n",
      "1128    7.522941\n",
      "1129    8.292799\n",
      "1130    7.824046\n",
      "1131    8.292799\n",
      "1132    8.241440\n",
      "1133    7.536364\n",
      "1134    7.955074\n",
      "1135    7.509335\n",
      "1136    7.522941\n",
      "1137    7.813996\n",
      "1138    7.901007\n",
      "1139    7.824046\n",
      "1140    8.229511\n",
      "1141    8.098643\n",
      "1142    7.693937\n",
      "1143    8.400659\n",
      "1144    7.822044\n",
      "1145    7.793587\n",
      "1146    7.649693\n",
      "1147    7.673223\n",
      "1148    7.240650\n",
      "1149    8.101678\n",
      "1150    7.130899\n",
      "1151    7.309881\n",
      "1152    6.802395\n",
      "1153    7.495542\n",
      "1154    8.465900\n",
      "1155    8.455318\n",
      "1156    7.647309\n",
      "1157    8.004700\n",
      "1158    7.901007\n",
      "1159    8.188411\n",
      "1160    7.738488\n",
      "1161    7.374629\n",
      "1162    7.090077\n",
      "1163    8.292799\n",
      "1164    7.824046\n",
      "1165    7.760041\n",
      "1166    8.229511\n",
      "1167    6.715383\n",
      "1168    7.696213\n",
      "1169    7.166266\n",
      "1170    7.822044\n",
      "1171    7.130899\n",
      "1172    7.374629\n",
      "1173    7.989560\n",
      "1174    8.268732\n",
      "1175    8.779557\n",
      "1176    7.955074\n",
      "1177    7.649693\n",
      "1178    6.956545\n",
      "1179    8.341649\n",
      "1180    7.296413\n",
      "1181    7.204149\n",
      "1182    7.649693\n",
      "1183    8.124151\n",
      "1184    7.878534\n",
      "1185    7.738488\n",
      "1186    7.772753\n",
      "1187    8.691146\n",
      "1188    7.435438\n",
      "1189    8.160518\n",
      "1190    8.006368\n",
      "1191    7.598399\n",
      "1192    8.255828\n",
      "1193    8.174703\n",
      "1194    7.972466\n",
      "1195    7.693937\n",
      "1196    7.265430\n",
      "1197    7.435438\n",
      "1198    7.647309\n",
      "1199    8.187299\n",
      "1200    8.229511\n",
      "1201    8.070906\n",
      "1202    8.160518\n",
      "1203    8.101678\n",
      "1204    7.435438\n",
      "1205    7.313220\n",
      "1206    8.006368\n",
      "1207    8.433812\n",
      "1208    7.693937\n",
      "1209    7.546974\n",
      "1210    7.346010\n",
      "1211    6.543912\n",
      "1212    7.822044\n",
      "1213    8.070906\n",
      "1214    8.411833\n",
      "1215    7.673223\n",
      "1216    8.107720\n",
      "1217    6.476972\n",
      "1218    8.055158\n",
      "1219    7.309881\n",
      "1220    8.699515\n",
      "1221    7.084226\n",
      "1222    7.600402\n",
      "1223    8.433812\n",
      "1224    8.354674\n",
      "1225    7.803843\n",
      "1226    6.907755\n",
      "1227    6.956545\n",
      "1228    7.492760\n",
      "1229    7.438384\n",
      "1230    8.556414\n",
      "1231    8.342840\n",
      "1232    8.004700\n",
      "1233    6.907755\n",
      "1234    7.374629\n",
      "1235    8.038835\n",
      "1236    7.296413\n",
      "1237    7.899153\n",
      "1238    6.214608\n",
      "1239    7.843849\n",
      "1240    7.170120\n",
      "1241    6.907755\n",
      "1242    7.824046\n",
      "1243    7.937375\n",
      "1244    7.693937\n",
      "1245    7.972466\n",
      "1246    7.899153\n",
      "1247    7.408531\n",
      "1248    7.649693\n",
      "1249    7.696213\n",
      "1250    7.492760\n",
      "1251    7.481556\n",
      "1252    7.647309\n",
      "1253    7.649216\n",
      "1254    7.863267\n",
      "1255    8.241440\n",
      "1256    7.313220\n",
      "1257    7.549609\n",
      "1258    7.972466\n",
      "1259    7.438384\n",
      "1260    8.101375\n",
      "1261    8.242756\n",
      "1262    7.598399\n",
      "1263    7.649693\n",
      "1264    8.101678\n",
      "1265    8.294050\n",
      "1266    8.070906\n",
      "1267    7.882315\n",
      "1268    7.435438\n",
      "1269    7.438384\n",
      "1270    7.254885\n",
      "1271    6.870053\n",
      "1272    8.809863\n",
      "1273    7.901007\n",
      "1274    7.600902\n",
      "1275    8.698681\n",
      "1276    7.377759\n",
      "1277    7.893572\n",
      "1278    7.649693\n",
      "1279    7.299797\n",
      "1280    7.937375\n",
      "1281    7.438384\n",
      "1282    7.438384\n",
      "1283    7.090077\n",
      "1284    7.003065\n",
      "1285    8.242756\n",
      "1286    8.612503\n",
      "1287    7.649693\n",
      "1288    7.783224\n",
      "1289    7.803843\n",
      "1290    7.970740\n",
      "1291    8.116716\n",
      "1292    7.207860\n",
      "1293    7.598399\n",
      "1294    8.006368\n",
      "1295    7.598399\n",
      "1296    7.861342\n",
      "1297    8.188689\n",
      "1298    7.718685\n",
      "1299    7.919356\n",
      "1300    7.693937\n",
      "1301    6.802395\n",
      "1302    7.313220\n",
      "1303    8.116716\n",
      "1304    7.989560\n",
      "1305    7.696213\n",
      "1306    7.150701\n",
      "1307    8.188689\n",
      "1308    8.523175\n",
      "1309    8.070906\n",
      "1310    8.188689\n",
      "1311    7.056175\n",
      "1312    8.512181\n",
      "1313    8.055158\n",
      "1314    7.464510\n",
      "1315    7.740664\n",
      "1316    7.467371\n",
      "1317    7.467371\n",
      "1318    7.435438\n",
      "1319    7.937375\n",
      "1320    7.377759\n",
      "1321    7.803843\n",
      "1322    8.317522\n",
      "1323    7.512071\n",
      "1324    7.989560\n",
      "1325    7.937375\n",
      "1326    7.693937\n",
      "1327    7.166266\n",
      "1328    7.598399\n",
      "1329    8.004700\n",
      "1330    7.600902\n",
      "1331    8.146130\n",
      "1332    7.549609\n",
      "1333    7.522941\n",
      "1334    7.738488\n",
      "1335    7.392648\n",
      "1336    7.963808\n",
      "1337    8.241440\n",
      "1338    7.901007\n",
      "1339    7.313220\n",
      "1340    6.956545\n",
      "1341    8.037543\n",
      "1342    7.575585\n",
      "1343    7.598399\n",
      "1344    7.718685\n",
      "1345    7.226209\n",
      "1346    7.955074\n",
      "1347    7.546974\n",
      "1348    7.134891\n",
      "1349    7.279319\n",
      "1350    7.718685\n",
      "1351    7.392648\n",
      "1352    7.520235\n",
      "1353    7.649693\n",
      "1354    7.783224\n",
      "1355    7.740664\n",
      "1356    7.495542\n",
      "1357    8.593228\n",
      "1358    7.649693\n",
      "1359    7.546974\n",
      "1360    7.495542\n",
      "1361    7.863267\n",
      "1362    7.467371\n",
      "1363    8.612503\n",
      "1364    7.495542\n",
      "1365    8.612503\n",
      "1366    7.346010\n",
      "1367    7.673223\n",
      "1368    8.202482\n",
      "1369    7.901007\n",
      "1370    8.131531\n",
      "1371    7.783224\n",
      "1372    8.004700\n",
      "1373    7.919356\n",
      "1374    7.989560\n",
      "1375    8.647344\n",
      "1376    7.438384\n",
      "1377    8.101678\n",
      "1378    8.612503\n",
      "1379    7.783224\n",
      "1380    7.047517\n",
      "1381    8.330864\n",
      "1382    8.070906\n",
      "1383    7.783224\n",
      "1384    6.983790\n",
      "1385    6.932448\n",
      "1386    7.970740\n",
      "1387    7.090077\n",
      "1388    7.843849\n",
      "1389    7.090077\n",
      "1390    7.600902\n",
      "1391    7.901007\n",
      "1392    7.244228\n",
      "1393    8.160518\n",
      "1394    7.244228\n",
      "1395    7.467371\n",
      "1396    8.318742\n",
      "1397    8.242756\n",
      "1398    7.989560\n",
      "1399    7.408531\n",
      "1400    7.435438\n",
      "1401    7.279319\n",
      "1402    7.928406\n",
      "1403    8.055158\n",
      "1404    7.475906\n",
      "1405    8.101678\n",
      "1406    7.130899\n",
      "1407    7.346010\n",
      "1408    7.598399\n",
      "1409    7.522941\n",
      "1410    7.090077\n",
      "1411    6.678342\n",
      "1412    7.509335\n",
      "1413    8.146130\n",
      "1414    8.389360\n",
      "1415    7.244228\n",
      "1416    7.549609\n",
      "1417    7.495542\n",
      "1418    8.341649\n",
      "1419    7.549609\n",
      "1420    7.150701\n",
      "1421    7.374629\n",
      "1422    7.492760\n",
      "1423    7.309881\n",
      "1424    7.989560\n",
      "1425    6.802395\n",
      "1426    7.309881\n",
      "1427    7.783224\n",
      "1428    7.764296\n",
      "1429    7.546974\n",
      "1430    8.006368\n",
      "1431    7.549609\n",
      "1432    7.781139\n",
      "1433    7.696213\n",
      "1434    7.901007\n",
      "1435    7.901007\n",
      "1436    8.101678\n",
      "1437    8.160518\n",
      "1438    7.696213\n",
      "1439    6.684612\n",
      "1440    7.244228\n",
      "1441    7.279319\n",
      "1442    8.411833\n",
      "1443    7.240650\n",
      "1444    7.729735\n",
      "1445    7.824046\n",
      "1446    7.598399\n",
      "1447    7.546974\n",
      "1448    8.465900\n",
      "1449    8.507143\n",
      "1450    8.070906\n",
      "1451    8.086410\n",
      "1452    7.575585\n",
      "1453    7.346010\n",
      "1454    6.902743\n",
      "1455    7.649693\n",
      "1456    8.229511\n",
      "1457    8.242756\n",
      "1458    7.435438\n",
      "1459    7.090077\n",
      "1460    8.070906\n",
      "1461    7.970740\n",
      "1462    7.740664\n",
      "1463    7.240650\n",
      "1464    7.546974\n",
      "1465    8.389133\n",
      "1466    7.309881\n",
      "1467    8.610684\n",
      "1468    7.803843\n",
      "1469    6.856462\n",
      "1470    7.368340\n",
      "1471    8.410721\n",
      "1472    8.342840\n",
      "1473    8.070906\n",
      "1474    7.803843\n",
      "1475    7.937375\n",
      "1476    7.696213\n",
      "1477    7.435438\n",
      "1478    7.467371\n",
      "1479    8.160518\n",
      "1480    7.549609\n",
      "1481    8.575462\n",
      "1482    8.159089\n",
      "1483    7.937375\n",
      "1484    7.309881\n",
      "1485    8.160518\n",
      "1486    7.783224\n",
      "1487    7.313220\n",
      "1488    8.160518\n",
      "1489    7.738488\n",
      "1490    7.549609\n",
      "1491    8.006034\n",
      "1492    7.727535\n",
      "1493    7.575585\n",
      "1494    7.859413\n",
      "1495    7.492760\n",
      "1496    7.989560\n",
      "1497    8.180321\n",
      "1498    7.522941\n",
      "1499    8.037543\n",
      "1500    8.160518\n",
      "1501    7.240650\n",
      "1502    7.673223\n",
      "1503    8.342840\n",
      "1504    8.146130\n",
      "1505    8.432724\n",
      "1506    7.279319\n",
      "1507    7.279319\n",
      "1508    7.562681\n",
      "1509    8.039157\n",
      "1510    7.989560\n",
      "1511    7.841886\n",
      "1512    7.495542\n",
      "1513    7.590852\n",
      "1514    7.163947\n",
      "1515    8.160518\n",
      "1516    6.870053\n",
      "1517    7.154615\n",
      "1518    7.781139\n",
      "1519    7.781139\n",
      "1520    8.318742\n",
      "1521    7.244228\n",
      "1522    7.313220\n",
      "1523    7.762171\n",
      "1524    8.330864\n",
      "Name: price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# capture the target\n",
    "y_train = X_train['price']\n",
    "y_test = X_test['price']\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['price'])\n",
    "X_test = X_test.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multiple linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Lasso regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Polynomial regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# SVR\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Decision Tree regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Random Forest regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# to evaluate the model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 411280\n",
      "train rmse: 641\n",
      "train r2: 0.6726833560560685\n",
      "\n",
      "test mse: 488845\n",
      "test rmse: 699\n",
      "test r2: 0.6609573882947357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# transform target and predictions (log) back to original\n",
    "\n",
    "# we evaluate using the mean squared error,\n",
    "# the root of the mean squared error, and r2\n",
    "\n",
    "# make predictions for train set\n",
    "pred = lm.predict(X_train)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('train mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print('train rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\n",
    "print('train r2: {}'.format(\n",
    "    r2_score(np.exp(y_train), np.exp(pred))))\n",
    "print()\n",
    "\n",
    "# make predictions for test set\n",
    "pred = lm.predict(X_test)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('test mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print('test rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\n",
    "print('test r2: {}'.format(\n",
    "    r2_score(np.exp(y_test), np.exp(pred))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Evaluation of Linear Regression Predictions')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the predictions respect to the real values\n",
    "plt.scatter(y_test, lm.predict(X_test))\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Evaluation of Linear Regression Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x204b3c7c908>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQ0ElEQVR4nO3de4xcd3nG8e9LQi5lS2wT2LpJih3hUqJYDXiVRkUq63ALpCKWmlKjQO02yIJCRYWRMKV/tKgVSas0VVUk6hKKexEbMEVxiSIUEq9QJRKKy8WEKNgJEU3i2qUkoUvTFNO3f8xZOqxnPbM7Z2b3he9HWu3Muc3j384+PnvOnJnITCRJ9TxjpQNIkpbHApekoixwSSrKApekoixwSSrqzHE+2Pnnn58bNmwYahvf/e53edazntVOoBGrkrVKTqiTtUpOMOsotJ3z0KFD38rM554yIzPH9rVly5Yc1sGDB4fexrhUyVolZ2adrFVyZpp1FNrOCXwhe3Sqh1AkqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqaixXkovjduGPbcPtNzDN1w94iRS+9wDl6SiLHBJKsoCl6SiLHBJKsqTmCpp0JOT0o8y98AlqSgLXJKKssAlqSiPgWtV6Xdse/fmk+z0+LcEuAcuSWUNXOARcUZEfDEiPtXc3xgR90bEkYi4NSLOGl1MSdJCS9kDfwdwf9f9G4GbM3MT8DhwfZvBJEmnN1CBR8SFwNXAh5r7AVwJ7G8W2QdsG0VASVJvkZn9F4rYD7wf+EngXcBO4J7MfEEz/yLgjsy8tMe6u4BdAJOTk1tmZmaGCjw3N8fExMRQ2xiXKllXU87Djz552vmT58Lxp9p/3M0XnNfq9lbTmPZj1va1nXPr1q2HMnNq4fS+r0KJiF8GTmTmoYiYnp/cY9Ge/xNk5l5gL8DU1FROT0/3Wmxgs7OzDLuNcamSdTXl7PcKk92bT3LT4fZfPPXwddOtbm81jWk/Zm3fuHIO8pvwUuB1EfFa4Bzg2cCfAWsi4szMPAlcCDw2upiSpIX6HgPPzPdk5oWZuQHYDtydmdcBB4Frm8V2ALeNLKUk6RTDvA783cA7I+Io8BzglnYiSZIGsaSDiZk5C8w2tx8CLm8/kjR+fvSaKvJKTEkqygKXpKIscEkqygKXpKJ8O1mNhR+BJrXPPXBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6Si+hZ4RJwTEZ+PiC9HxH0R8QfN9I0RcW9EHImIWyPirNHHlSTNG2QP/Gngysz8eeAy4KqIuAK4Ebg5MzcBjwPXjy6mJGmhvgWeHXPN3Wc2XwlcCexvpu8Dto0koSSpp8jM/gtFnAEcAl4AfAD4E+CezHxBM/8i4I7MvLTHuruAXQCTk5NbZmZmhgo8NzfHxMTEUNsYlypZx5Hz8KNPtrKdyXPh+FOtbGpZNl9w3kDLVfnZg1lHoe2cW7duPZSZUwunnznIypn5feCyiFgDfBJ4Ua/FFll3L7AXYGpqKqenpwfN3NPs7CzDbmNcqmQdR86de25vZTu7N5/kpsMDPW1H4uHrpgdarsrPHsw6CuPKuaRXoWTmE8AscAWwJiLmf5MuBB5rN5ok6XQGeRXKc5s9byLiXOAVwP3AQeDaZrEdwG2jCilJOtUgf4uuB/Y1x8GfAXwsMz8VEV8DZiLiD4EvAreMMKckaYG+BZ6ZXwFe3GP6Q8DlowglSerPKzElqSgLXJKKssAlqSgLXJKKWrkrIlTehpYuzpG0PO6BS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRvhuhtASDvgPj7s0nmR5tFMk9cEmqygKXpKIscEkqygKXpKIscEkqygKXpKIscEkqygKXpKIscEkqygKXpKIscEkqygKXpKIscEkqygKXpKIscEkqygKXpKIscEkqygKXpKIscEkqygKXpKL6FnhEXBQRByPi/oi4LyLe0UxfFxF3RsSR5vva0ceVJM0bZA/8JLA7M18EXAG8LSIuAfYAd2XmJuCu5r4kaUz6FnhmHsvMf2lu/ydwP3ABcA2wr1lsH7BtVCElSaeKzBx84YgNwGeBS4FvZuaarnmPZ+Yph1EiYhewC2BycnLLzMzMUIHn5uaYmJgYahvjUiXrcnMefvTJEaQ5vclz4fhTY3/YJZs8F5637ryVjjGQKs9TqJO17Zxbt249lJlTC6efOegGImIC+ATwO5n5nYgYaL3M3AvsBZiamsrp6elBH7Kn2dlZht3GuFTJutycO/fc3n6YPnZvPslNhwd+2q6Y3ZtP8voCP3uo8zyFOlnHlXOgV6FExDPplPffZ+Y/NJOPR8T6Zv564MRoIkqSehnkVSgB3ALcn5l/2jXrALCjub0DuK39eJKkxQzyt+hLgTcBhyPiS8203wVuAD4WEdcD3wR+dTQRJUm99C3wzPwnYLED3i9vN47042fDgOcSHr7h6hEnUTVeiSlJRVngklSUBS5JRVngklTU6r8iQmM36Ek1SSvLPXBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsorMaUR8YpWjZp74JJUlAUuSUVZ4JJUlAUuSUV5EvPHyGIn1XZvPslOT7hJ5bgHLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklF+W6EPwL86C7px5N74JJUlAUuSUVZ4JJUlAUuSUVZ4JJUVN8Cj4gPR8SJiPhq17R1EXFnRBxpvq8dbUxJ0kKD7IF/BLhqwbQ9wF2ZuQm4q7kvSRqjvgWemZ8Fvr1g8jXAvub2PmBby7kkSX0s9xj4ZGYeA2i+P6+9SJKkQURm9l8oYgPwqcy8tLn/RGau6Zr/eGb2PA4eEbuAXQCTk5NbZmZmhgo8NzfHxMTEUNsYl3FlPfzok0OtP3kuHH+qpTAjViXrKHJuvuC8djfY8HeqfW3n3Lp166HMnFo4fbmX0h+PiPWZeSwi1gMnFlswM/cCewGmpqZyenp6mQ/ZMTs7y7DbGJdxZd055KX0uzef5KbDNd5VoUrWUeR8+LrpVrc3z9+p9o0r53IPoRwAdjS3dwC3tRNHkjSoQV5G+FHgc8ALI+KRiLgeuAF4ZUQcAV7Z3JckjVHfv/Ey8w2LzHp5y1m0gO8yKOl0vBJTkoqywCWpKAtckoqywCWpqNX/glpJSzLoye+Hb7h6xEk0au6BS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRfiZmixZ+FuHuzSfZOeDnE0r9DPpZl0vdXr/nqZ+duXq5By5JRVngklSUBS5JRXkMXNJpDXrs3WPl4+ceuCQVZYFLUlEWuCQVZYFLUlGexJQ0dp4YbYd74JJUlAUuSUVZ4JJUlAUuSUWVOYk56DunSVoZbb9bYq9tLvb7P+jJzrZPni62vYU5R3Uy1j1wSSpqqAKPiKsi4oGIOBoRe9oKJUnqb9kFHhFnAB8AXgNcArwhIi5pK5gk6fSG2QO/HDiamQ9l5v8AM8A17cSSJPUTmbm8FSOuBa7KzDc3998E/EJmvn3BcruAXc3dFwIPLD8uAOcD3xpyG+NSJWuVnFAna5WcYNZRaDvn8zPzuQsnDvMqlOgx7ZT/DTJzL7B3iMf54QeN+EJmTrW1vVGqkrVKTqiTtUpOMOsojCvnMIdQHgEu6rp/IfDYcHEkSYMapsD/GdgUERsj4ixgO3CgnViSpH6WfQglM09GxNuBTwNnAB/OzPtaS7a41g7HjEGVrFVyQp2sVXKCWUdhLDmXfRJTkrSyvBJTkoqywCWpqFVZ4BGxLiLujIgjzfe1PZbZGhFf6vr674jY1sz7SER8o2veZSuVs1nu+11ZDnRN3xgR9zbr39qcDB6JAcf0soj4XETcFxFfiYhf65o30jHt97YMEXF2M0ZHmzHb0DXvPc30ByLi1W3mWmbWd0bE15oxvCsint81r+dzYQWz7oyIf+/K9OaueTua58uRiNixwjlv7sr49Yh4omve2MY0Ij4cESci4quLzI+I+PPm3/GViHhJ17z2xzMzV90X8MfAnub2HuDGPsuvA74N/ERz/yPAtaslJzC3yPSPAdub2x8E3rqSWYGfBTY1t38aOAasGfWY0jkJ/iBwMXAW8GXgkgXL/Bbwweb2duDW5vYlzfJnAxub7ZwxwnEcJOvWrufiW+eznu65sIJZdwJ/0WPddcBDzfe1ze21K5VzwfK/TedFEysxpr8EvAT46iLzXwvcQec6mSuAe0c5nqtyD5zOJfn7mtv7gG19lr8WuCMz/2ukqU611Jw/EBEBXAnsX876y9A3a2Z+PTOPNLcfA04Ap1z9NQKDvC1Dd/79wMubMbwGmMnMpzPzG8DRZnsrljUzD3Y9F++hc43EShjm7S5eDdyZmd/OzMeBO4GrVknONwAfHVGW08rMz9LZWVzMNcDfZMc9wJqIWM+IxnO1FvhkZh4DaL4/r8/y2zn1B/pHzZ8wN0fE2aMIyeA5z4mIL0TEPfOHeYDnAE9k5snm/iPABSPKuZSsAETE5XT2hh7smjyqMb0A+Neu+73G4gfLNGP2JJ0xHGTdNi318a6ns0c2r9dzYVQGzforzc91f0TMX5w3znEd+LGaw1Ebgbu7Jo9zTPtZ7N8ykvFcsQ90iIjPAD/VY9Z7l7id9cBmOq9Hn/ce4N/oFNBe4N3A+1Yw589k5mMRcTFwd0QcBr7TY7mhXtPZ8pj+LbAjM/+3mdzamPZ6yB7TFo7FYssM9JYOLRr48SLijcAU8LKuyac8FzLzwV7rt2CQrP8IfDQzn46It9D5K+fKAddty1IeazuwPzO/3zVtnGPaz1ifpytW4Jn5isXmRcTxiFifmceaMjlxmk29HvhkZn6va9vHmptPR8RfA+9ayZzN4Qgy86GImAVeDHyCzp9XZzZ7lEO/FUEbWSPi2cDtwO81fwLOb7u1Me1hkLdlmF/mkYg4EziPzp+y435Lh4EeLyJeQec/zpdl5tPz0xd5LoyqbPpmzcz/6Lr7V8CNXetOL1h3tvWE//9Yg/4MtwNv654w5jHtZ7F/y0jGc7UeQjkAzJ+l3QHcdpplTzke1hTU/HHmbUDPM8Yt6JszItbOH26IiPOBlwJfy86ZjYN0jt8vuv6Ys54FfJLOMbyPL5g3yjEd5G0ZuvNfC9zdjOEBYHt0XqWyEdgEfL7FbEvOGhEvBv4SeF1mnuia3vO5sMJZ13fdfR1wf3P708CrmsxrgVfxw3/ljjVnk/WFdE4Afq5r2rjHtJ8DwK83r0a5Aniy2fkZzXiO6+ztUr7oHNu8CzjSfF/XTJ8CPtS13AbgUeAZC9a/GzhMp2T+DphYqZzALzZZvtx8v75r/YvplM1R4OPA2Ss5psAbge8BX+r6umwcY0rn7P3X6ew5vbeZ9j46JQhwTjNGR5sxu7hr3fc26z0AvGYMz89+WT8DHO8awwP9ngsrmPX9wH1NpoPAz3Wt+5vNeB8FfmMlczb3fx+4YcF6Yx1TOjuLx5rfk0fonON4C/CWZn7Q+aCbB5s8U6McTy+ll6SiVushFElSHxa4JBVlgUtSURa4JBVlgUtSURa4JBVlgUtSUf8HTwnaLGX6vwQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the distribution of the errors:\n",
    "# they should be fairly normally distributed\n",
    "\n",
    "errors = y_test - lm.predict(X_test)\n",
    "errors.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Feature Importance')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finally, just for fun, let's look at the feature importance\n",
    "\n",
    "importance = pd.Series(np.abs(lm.coef_.ravel()))\n",
    "importance.index = ['city', 'zipcode', 'bathrooms', 'bedrooms', 'area', 'year']\n",
    "importance.sort_values(inplace=True, ascending=False)\n",
    "importance.plot.bar(figsize=(18,6))\n",
    "plt.ylabel('Lm Coefficients')\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.005, random_state=0)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = Lasso(alpha=0.005, random_state=0)\n",
    "\n",
    "l1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 414118\n",
      "train rmse: 643\n",
      "train r2: 0.6704244126197878\n",
      "\n",
      "test mse: 495973\n",
      "test rmse: 704\n",
      "test r2: 0.656013826121537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# transform target and predictions (log) back to original\n",
    "\n",
    "# we evaluate using the mean squared error,\n",
    "# the root of the mean squared error, and r2\n",
    "\n",
    "# make predictions for train set\n",
    "pred = l1.predict(X_train)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('train mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print('train rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\n",
    "print('train r2: {}'.format(\n",
    "    r2_score(np.exp(y_train), np.exp(pred))))\n",
    "print()\n",
    "\n",
    "# make predictions for test set\n",
    "pred = l1.predict(X_test)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('test mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print('test rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\n",
    "print('test r2: {}'.format(\n",
    "    r2_score(np.exp(y_test), np.exp(pred))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Evaluation of Lasso Regression Predictions')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO29e5xdZX3v//7MZIfMBMsEiBdGAqhtKBQhkCPUnJ+CqEgRSMVToGDFVpFatSLN70DLkYtY06JFW88p4qXWojHczIGCgm1AKzZoIAkYJZVrwgQxGiZIMsAk+Z4/1lrDmjXrumfvPXvP/r5fr/2avZ91e9bas5/v83yvMjMcx3EcJ0nPVHfAcRzHaU9cQDiO4zipuIBwHMdxUnEB4TiO46TiAsJxHMdJxQWE4ziOk4oLiGmApLskvbdJ5/5LSV9sxrkLrvv7kjZJelbSglZfvxuRdJakO6a6H1WRZJJeE76/WtL/qvM8z0p6VWN719m4gGghkh6TNBL+I0avz011vyIkHSvpiXibmf21mTVF+BTwKeCDZranma1JbowPCu1E2K/t4Xc7JOnvJPVOdb/KYGZfM7O3Nvq84f/V7vCZ/FrSBknvafR1AMzsPDP7eIk+TZhUhf9rjzSjX53KjKnuQBdyspn921R3ogM4AFg/1Z2ok8PN7KFQgH0X+CnwhUZeQNIMM9vZyHM2mc1m9kpJAk4FbpB0j5n9JL5TB97XtMZXEG2ApD0kDUv6nVjb3HC18VJJcyT9q6Qtkp4O378y41yXSro29vnAcFY7I/z8Hkk/DWdyj0h6f9g+G/gWsF9sdbNfyvlOkbQ+7O9dkn47tu0xSX8h6X5J2yQtlzQro589ki6W9LikX0j6qqS9wmfxLNALrJP0cMVn+WpJKyX9StIvJX1N0kBs+/8MZ/bRTPb4sP11klZLekbSU5L+rsw952FmDwF3A0fEzvV2SWvDc/1A0mtj246UtCbs2/Xh87si3HaspCfC/v8c+KfwGV4o6eHwfq+TtHe4/yxJ14btw5J+JOll4bZzwu/+15IelXRWrP37sf68PjxuW/j39bFtd0n6uKS7w/PcIWnfEs/EzGwF8DRwSHjNuyVdJWkrcGn4P/ApSRvD7+JqSX2xay+R9KSkzZL+OH5+SV+Jnln4+dTweT8TPqe3SfoE8P8Bn1NsFa/xqqq9wv/JLeH/6MWSeuLPKezj0+EzPDF2zdTn25GYmb9a9AIeA96cse3LwCdin/8M+Hb4fh/gNKAfeAlwPbAitu9dwHvD95cC18a2HQgYMCP8fBLwakDAG4EdwJHhtmOBJxL9Gjsf8FvAduAtQA34/4GHgJmx+/shsB+wN8HM+byM+/3j8NhXAXsCNwH/EttuwGtynmXqduA1Yf/2AOYC3wM+E26bD2wC9os9m1eH7/8TeFf4fk/gmDL3nNcv4GDgSeD88PORwC+AowkE4LvDZ7YHMBN4HPjz8DrvAF4Aroh9NzuBvwn37wM+AqwCXhm2fR5YFu7/fuAWgv+ZXuAo4DeA2cAzwPxwv1cAh4bvzwG+H77fm2AQfxeBpuHM8PM+sf+5h8Pn0xd+XprxTI4l/L8imJT+PjAafh/nhPf1ofA6fcBngJvDPrwkvI9Phse/DXgK+J3wXr6eeOZfiT2z1wHbwu+uBxgEDk7+ZjK+u68C/ze8/oHAfwF/EntOo8D7wmf7p8Bmgt9U5vPtxNeUd6CbXuFg8CwwHHu9L9z2ZuCR2L53A3+UcZ4jgKdjn8f+2SkQECnnWgH8efh+7Icc2z52PuB/AdfFtvUAQ8Cxsfs7O7b9b4GrM67778AHYp/nhz+6SJDVJSBS9lsMrAnfv4ZggH4zUEvs9z3gMmDfRHvuPWf06xkCoWLAMmCPcNs/Ah9P7L+BQFC/ITyvYtu+z3gB8QIwK7b9p8Dxsc+viJ4hgQD+AfDaxPVmh/93pwF9iW3n8KKAeBfww8T2/wTOif3PXRzb9gHCCU3KMzkW2B1edyuwFjgjds2NsX0VPrtXx9p+F3g0fP9lYoKIQEBlCYjPA1dl9OkuMgQEwaD/PHBIbNv7gbtifX4otq0/PPblec+3E1+uYmo9i81sIPaKdNMrgT5JR0s6gEAIfBNAUr+kz4dL3WcIBrMB1WH8lHSipFWStkoaBn4PKFQNhOxHMMsFwMx2E8zIB2P7/Dz2fgfBbLzwXOH7GcDLSvYlFQUquW+EaqRngGsJ788Clc9HCITeL8L99gsP/ROCwebBUJ3y9rR+ZtxzkiMJ7vt0gtXC7LD9AOCCUOUzHD7//cNr7AcMWTjihGxKnHeLmT0X+3wA8M3YuX4K7CJ4hv8C3A58I1TF/K2kmpltD/t1HvCkpFslHZxyD8nvh/BzPd81BDaIATPb28yOMLNvZNznXIIB997YfX07bI/6Fd8/2cc4+xOscqqyLy+u6OLXSb13M9sRvt2zwvPtCFxAtAnhwHMdwVL+D4F/NbNfh5svIJhhH21mv0Ew24RgtpVkO8EPLOLl0RtJewA3EngIvczMBoDbYucpSu27mWBQis4ngh/hUNH9FZ0LmEeganiqjnPF+STBfbw2fFZnE3tOZvZ1M/vv4bWNQGWDmf3MzM4EXhq23aDALlPXPVvAdQSz7o+FzZsI1IjxCUK/mS0jUEUNhueP2D952sTnTcCJifPNMrMhMxs1s8vM7BDg9cDbgT8K+3a7mb2FYMXxIOkG9OT3A8F3VM93XUT8vn4JjBCoZaJ72svMIuHzJOOfy7yc824iUKcWXTPJLwlWYsn/z1L3XvL5dgQuINqLrxPMPs4K30e8hOBHMxwaIS/JOcda4A2S5knaC7gotm0mga56C7AzNKzF3RqfAvYJj0vjOuAkScdLqhEIrucJVBlVWQacL+kgSXsCfw0st2oeLDNDY2z06iV4Vs8SPKtBYEm0s6T5kt4UCsrnCJ7prnDb2ZLmhoJ6ODxkVwPueSlwrqSXEwwU54WrREmaLekkSS8hECS7gA9KmiHpVAIdeh5XA58IV5yRY8Op4fvjJB0WPpNnCAa8XZJepsDoPju8j2ejZ5DgNuC3JP1h2J/TgUOAfy1533URPv8vAFdJeml4L4OSTgh3uQ44R9IhkvrJ/y18CXhP+N31hOeJZvNPEdi/0voQfe+fkPSS8Pl+lGA1mkuF59sRuIBoPbdofBzEN6MNZnYPwQpgPwKPoojPEBjvfklglPx21snN7DvAcuB+4F5iP+hwRfJhgn/+pwlWKjfHtj9IMHA/Ei7v94udGjPbQDAj/4ewLycTuO2+UPUhEOiS/4VAXfYowYD9oYrnWE8wyEev9xDYEY4kME7eSmD8jtiDYMD+JYGK4KXAX4bb3gasV+BB9VkCHflzk71nM3uAwNV1iZmtJjBsfo7g+T9EoM8mPN87CFRdw+E1/5VgkMniswTf3x2Sfk3wv3F0uO3lwA0EwuGnYR+uJfjNX0CwQthKYP/4QEq/f0Ww6rgA+BWBcf7tZvbLMvc9Sf4nwbNZFaoJ/41gBY2ZfYvg97Ay3Gdl1knM7IcE/xNXEfw/fJcXVwWfBd4ZeiH9fcrhHyL4LT5CYAv6OsH/bBGlnm+noPEqT8dx2gVJ9xAY+f9pqvvidCe+gnCcNkHSGyW9PFTpvBt4LTmrRcdpNh5J7Tjtw3wC9d+eBN437zSzJ6e2S0434yomx3EcJ5WmriAknQ+8l8Cl7AHgPXE/bklXAceFH/uBl4aul0jaFR4DQSDNKc3sq+M4jjOepq0gQhfD7xNEI45Iug64zcy+krH/h4AFZvbH4ednY77Ppdh3333twAMPnFzHHcdxuoh77733l2Y2N21bs20QMwiig0cJVgibc/Y9k3yf5kIOPPBAVq9ePZlTOI7jdBWSMqPRm+bFZGZDBBG7GwmiH7eZWWoxkjAQ5SDG+zTPUpBdc5Wkxc3qp+M4jpNO0wSEpDkEed8PIgj8mi3p7IzdzwBuCCMYI+aZ2UKCYK7PSEoNmZd0bihIVm/ZsqWBd+A4jtPdNDMO4s0EGRi3mNkoQUTr6zP2PYMggncMM9sc/n2EIPNiatlJM7vGzBaa2cK5c1PVaI7jOE4dNFNAbASOUZCJVMDxBCH/45A0H5hDkIsmapsT5stBQRGSRcBPksc6juM4zaOZNoh7CHLB3EfgrtoDXCPpcklxl9UzgW8k0hz/NrBa0jrgToL87y4gHMdxWsi0CpRbuHChuReT4zidzoo1Q1x5+wY2D4+w30AfS06Yz+IFeSVI6kfSvaG9dwKeasNxHKeNWLFmiItueoCR0cBnZ2h4hItuCmKGmyUksvBkfY7jOG3ElbdvGBMOESOju7jy9g0t74sLCMdxnDZi8/BIpfZm4gLCcRynjdhvoK9SezNxAeE4jtNGLDlhPn213nFtfbVelpwwv+V9cSO14zhOGxEZolvlxZSHCwjHcZw2Y/GCwSkRCElcxeQ4juOk4gLCcRzHScVVTI7jOG3GxSseYNk9m9hlRq/EmUfvzxWLD2t5P1xAOI7jtBEXr3iAa1dtHPu8y2zscyQkWpWKw1VMjuM4bcSyezbltkepOIaGRzBeTMWxYs1Qw/viAsJxHKeN2JWRQDVqb2UqDhcQjuM4bUSvlNveylQcLiAcx3HaiDOP3j+3vZWpOFxAOI7jtAEr1gyxaOlKvrZqI7Nn9hKtI3olzj5m3piBupWpONyLyXEcpwXkeR4la0Bsf2EXfbVePvmOwyZ4J7UyFYcLCMdxnCZTVAQoz/CcNvC3KhVHU1VMks6XtF7SjyUtkzQrsf0cSVskrQ1f741te7ekn4Wvdzezn47jOM2kyPOonWpAxGmagJA0CHwYWGhmvwP0Amek7LrczI4IX18Mj90buAQ4GngdcImkOc3qq+M4TjMpEgDtVAMiTrON1DOAPkkzgH5gc8njTgC+Y2Zbzexp4DvA25rUR8dxnKZSJADaqQZEnKYJCDMbAj4FbASeBLaZ2R0pu54m6X5JN0iK/LsGgXg44RNhm+M4TsdRJAAWLxjkk+84jMGBPgQMDvSlGqhbTdOM1KFK6FTgIGAYuF7S2WZ2bWy3W4BlZva8pPOAfwbeBKRFiqSGF0o6FzgXYN68eQ28A8dxnMZQxvOoXWpAxGmmF9ObgUfNbAuApJuA1wNjAsLMfhXb/wvA34TvnwCOjW17JXBX2kXM7BrgGoCFCxemx6g7juNMMe0oAIpopoDYCBwjqR8YAY4HVsd3kPQKM3sy/HgK8NPw/e3AX8cM028FLmpiXx3HaUNalbV0MnRCH+ulaQLCzO6RdANwH7ATWANcI+lyYLWZ3Qx8WNIp4fatwDnhsVslfRz4UXi6y81sa7P66jhO+1EUO9AOdEIfJ4MsI3NgJ7Jw4UJbvXp18Y6O47Q9i5auZCjFPXRwoI+7L3zTFPRoIp3QxyIk3WtmC9O2eS4mx3HaknYNHovTCX2cDJ5qw3GctmS/gb7U2flUB49FrFgzRI+UWr+hVX1stv3DBYTjOG3JkhPmj9PvQ+OCxyY7sEa2hzThkNXHRg/mrbB/uIrJcZy2pFnBY40o2ZmWWwmC1NxpfWxGmdBWVJbzFYTjOG1LM2IHqmZOTSPLxrDbLPUcjbhm2T400v7hKwjHcbqKRgysVZPrNWMwb0WCPxcQjuN0FY0YWKsm12vGYN6KBH8uIBzH6SoaMbBWtY80YzBvRYI/D5RzHKfrmIr0GO2akiMvUM4FhOM4TheTJyDci8lxHKci7boaaDQuIBzHaVumciDOuvZ0T9AXxwWE4zhtyWQH4nqFy4o1Q1x2y3qe3jE61ha/djNiGtoVFxCO47QlZQbiRs/yk8elXTstP1R0jemGCwjHcdqSouCyPCFQ7yw/K4VG/Nq9GQn6epVWKbmz8TgIx3HakqLgsjwhUG/kctH2/Qb6UoUDkNneyfgKwnGcUlTR6U/GuBwdOzQ8goD4sBsPLssTAvWmCs86Ln7tLDXTYJukIW8kLiAcxymkik6/qv4/Lkz26qux/YWdjO4KxILBmJAYTAiaPCFQlCo8S4ClHQcw0Ffj0lMOHbt2s9KQtxsuIBzHKaSKTr9o3/jgPNBf49nndjK6OxAIwyOjJImEQ7KEZ54QiPpUrwE7b/VTZp/J0i5xFk0VEJLOB95L8B0/ALzHzJ6Lbf9ouH0nsAX4YzN7PNy2KzwGYKOZndLMvjqOk00VnX7evsnBOe5KWvX6RQN1cntUJ6FIgJVJMd6MNOQR7RRn0TQBIWkQ+DBwiJmNSLoOOAP4Smy3NcBCM9sh6U+BvwVOD7eNmNkRzeqf4zjlqaLTz9u3yEso7/pp5A3UWQNt1vWHhkdYtHTllEdFt1OcRbO9mGYAfZJmAP3A5vhGM7vTzHaEH1cBr2xyfxzHqYMq2Ujz9q2n/kG9+v2sgTbPHbURld4mSysKAZWlaQLCzIaATwEbgSeBbWZ2R84hfwJ8K/Z5lqTVklZJWpx1kKRzw/1Wb9mypSF9dxxnPFVSS+ftW6b+QQ/QX3txaJpVq2+YyhpQd5lNEGBxGl22sypZz2ivvhqLlq7koAtvZdHSlS0RYk3L5ippDnAjgcpoGLgeuMHMrk3Z92zgg8Abzez5sG0/M9ss6VXASuB4M3s475qezdVxJkcZ4+hkDKgXr3iAa1dtzN1HwIxejXkyQbCKqFrrYNHSlZnuqHnuqhGPLT2p9LXiTNbAnBbNXesRiEk/kzTysrk2U8X0ZuBRM9tiZqPATcDrUzr3ZuCvgFMi4QBgZpvDv48AdwELmthXx+l6ooFpaHgEI13dUmafPO58sHiVb4wfCCGY1V9w3bpKs+Y8VdfiBYMTvKKSrFgzVHnGPtnnA+krsD1nzUh9Js1e6TTTi2kjcIykfmAEOB4YN72XtAD4PPA2M/tFrH0OsMPMnpe0L7CIwIDtOE6TKGMcnawBdTJ69F1mXHTTA6x+fCt3PrilcIY+WXfUejyJsp7PZbesr9SPpPH9oAtvTd2v2XaJpgkIM7tH0g3AfQRurGuAayRdDqw2s5uBK4E9gesVGI4id9bfBj4vaTfBKmepmf2kWX11HKeccXSyBtS8SOUyjIzu4murNo5FVxcN3JNxR61HEGY9h6d3jI659NbjtlpvZPhkaaoXk5ldYmYHm9nvmNm7zOx5M/tYKBwwszeb2cvM7IjwdUrY/gMzO8zMDg//fqmZ/XQcpzj3Udl98khT+1QlaTWNZuj1GHD7KxrAy+RqKkNV9VAzalqXwZP1OY4DlBuE6hmo4rr8K2/fwGlHDY7p1+f01yYMQrUecfYx8yplR316x2ih7SRNgPz1O15LT+IyPQpSa6RRJACqCMAq6qEqXmSNxFNtOM4U0i4pFaB8monVj29l2T2b2GVGr8RpR1ULVrvx3qFxg1vyGRx38FzufHDLpLKjJlN7VE2tAen5lo47eC5HXHbHWEqQOf01Ljn5xRxNaefb/vzO1BQiVdVDzYzezqJpbq5Tgbu5Op1Emjtjo1wXm0XVPue5mqZ5EeUV7IFgdt/bowkePWkIeHTpSZX7EO9LUnAt/+GmsbxREbVeceU7Dy8tJKG9vuepcnN1HCeHPI+gdqVqn6satYtScew22LnLmNNfG1O19GXYEfYK1UT1GtYjV9hHl57E3Re+iTsf3DJBOEDgkpv3nU2VeqgRuIrJcaaIdkqpUIYVa4YyPZCy+lzV+6bMvRvw3Ohurjr9CBYvGGTB5XcwMrp7wn6RCaOoD2XVfHl9KyNsqgbLtYPq0QWE40wRrXZdnGwRn0hvn0a8z8l03rUejZt5Czju4Lmp+/dklPNMEgXOnb987QSvpojIrTQvLXiefQLG2xIG+muZ2Wcb+Z11RTZXx3HyKSpq00gmO+gUqX6iAT8tnXdvwk3IgBvvHWLhAXsD443BVQzTRftGXlB5xvdFS1emqswuvXk9z+/cPe551XpEjwI1V5xarxr6nbVTNlcXEI4zRbSi8ExEsyOgoxQaadfZlaK3j9st6kn/XYa4AMlS8WTdV5rX0ehuG3N/zfJiagTtpHp0AeE4TSZPtdMq18XJDDor1gwVqn6iYkBVoqSbPeCVqRFdNbJ728goj9aZxK8sUxU1nYZ7MTlOE2lE8rZGUG8EdNT/InXOXn21XBtF1rXLDnoDfbUxL6AyAXRlVXVZgX9z+usLlGsEVYIR60koWIXSKwhJs81se0Ov7jgdRlVD71Tpk9N8+G+8d6iyvaNMBbi+Wi9SNVWRCITlnBQjdpJar7j0lBfVOFmxEhKYvZjOO17/emh4hN5wFRTfXjVQrtmpLaC86rEVxuxCASHp9cAXCZLqzZN0OPB+M/tAQ3rgOB1CPT/IqdAnZ0Uvn3bUYKksqGX7KRg7z/nL15bun3gxn9LTO0ap9YqBvlqq3h9gRo8mRHMDXHbL+nFeRWbj03knn0O0Ckp+b3lqvqlwNS07CWnF5KPMCuIq4AQgSrC3TtIbGnJ1x+kg6vlBToU+Oaufdz64pbAGQpKs/iejkIuK7wBjM/jkWmF0lzF7jxlsGxlNdVkdGd3NijVD41YEkUvsxH1f/D7yVj/JVBxpA/JUpLaoMglpxeSjlA3CzDYlmprjduA4bUw9P8ipyMKZNVCXMcYmddrHHTy3VP+PO3guRZaBIiN3ntC88vYNE+w5WeeLvo+igTIyrLeDjSiiSqT6ZDPrlqGMgNgUqplM0kxJfwH8tGE9cJwOoZ4f5FSkWcgy4hYZd9MGy0g1ldf/FWuGuPHeoQmz/75aD4JCwQEw0F/LFZpDwyNccN26UnaOHokVa4YKB8r9BvraLt1JlUlIKyYfZVRM5wGfBQaBJ4A7gD9rWA8cp0OoN7Ct1aqKrJl1kSdSvaqpLFXO3rP34O4L38SBGdXQ4pgFzylpV6jS//h+F930AKcdNTjBMB8RfW9ZtpOpSndSRSXZijiaQgFhZr8EzmrYFR2nQ2llYNtkGMwYZAb6aixaujKz7/XqtBuhC98WGqgvOfnQ3GyuZYkE2yffcViuF1OW7WQqYg6g+iSk2ZOPMl5M/wz8uZkNh5/nAJ82sz9uWq8cp02ZCsNlVdIGmVqP2P7Ci3UJ0oyf9RrUi46bk5PDKLlvXAiXsZn01Xozhcnm4ZHC76soT1MjJwNlztduk5AyNojXRsIBwMyeBhaUObmk8yWtl/RjScskzUps30PSckkPSbpH0oGxbReF7RsknVDudhzHSbN77DlrxoQaCklde5pOu9YjdrywMzcQq+g4YELVtjjJGXKUZjvPZhK3h2RFTJdZBWTZiICGGq+rGMOTacanckJSWDBI0jrg2FAwIGlv4LtmdljBcYPA94FDzGxE0nXAbWb2ldg+HyAQQOdJOgP4fTM7XdIhwDLgdcB+wL8Bv2VmuetOLxjkTBcaMXuNnyPrVx4V1Uk7Zq++Gttf2DlOsGQVuik6rtYrZs8MXFkH+muYBWqleAW55L3m2S4eS/Q5qyAP1DcbzyoyNKe/Rv/MGQ07X1HRolaQVzCojJH608APJN0Qfv4fwCdKXnsG0CdpFOgHNie2nwpcGr6/AficJIXt3zCz54FHJT1EICz+s+R1HadjaUSEbFFltojkLDuuklm0dOWE4LWsuI+i46JYh7WXvDW3n/F7zbKlRCuGZFrxPWb0jAmdtGjoKs8xy37y9I7RMXVZI87XrrU/IsoYqb8qaTXwJoIJxzvM7CcljhuS9ClgIzAC3GFmdyR2GwQ2hfvvlLQN2CdsXxXb74mwbQKSzgXOBZg3b15Rtxyn7WlEhOxlt6wvlSIjzwMra/AaGh6ZYOyO+p23Wkk736U3T+xndK9V6jg8vWOUvlrvWBEhIDOVd5nnWDaJ32TPN1XG8LJkCghJv2Fmz4QqpZ8DX49t29vMtuadODRmnwocBAwD10s628yuje+WcqjltE9sNLsGuAYCFVNenxynE6g620zLN5RHPEUGkOnZlDdIRu1DwyMsuX4diMI60cnBcMWaocz0GpGBGarVcYgP1pOZtacJpyzqPV+rcjtNhrwVxNeBtwP3Mn5wjlKpvKrg3G8GHjWzLQCSbgJeD8QFxBPA/sATkmYAewFbY+0Rr2Siespx2op67AZpx2QNzEYwmMfPm5VvKA8Dfr7tOa5fvZH7Nm4bp4JZcsM6Lr15PdtGRsdqOheRl2gvIq2oTl4wWnTtqnUc4u2TmbWnCaftz+9MFWj1nq8dXaSTZAoIM3t7aA94o5ltrOPcG4FjJPUTqJiOB5IW5JuBdxPYFt4JrDQzk3Qz8HVJf0dgpP5N4Id19MFxWkI9doO0Y/JKaKadt0y21TR2mXH3wxOVAKO7bGwQzJrd18OulNVFbiLAgvDrMoP/ZGftSeGUZQyv93ydQK6bqwUuTt+s58Rmdg+B4fk+4IHwWtdIulzSKeFuXwL2CY3QHwUuDI9dD1wH/AT4NvBnRR5MjtNs8nLv59kNso5LO6aMjjTunlql2M1UspvA3hAnb+Y9XBA3USbNRKPTnExF2pSppoyb6/8GvmJmP2pNl+rH3VydZpHnSrl4wSAHXXhr5uCeDOaKjitaLeQRuae++qLbKtVxnmqS7qlZz6CM+2ejA9m6lcm6uR4HnCfpMWA7oQ3CzF7buC46TntT5FmUpfLolTKPq1ruMk40+84TDpGxMMtdtCx9tR5e2GnsMqNX4phXzeGxX42MDcz1nnvxgkFWP76Vr63aOE5IpKlt8lJyO82jjIA4sem9cJw2p8gouuSE+Sy5ft04g21epbTNwyNcdfoRdeUdig+gWYO/CDKkFqlqonNEsQTPPrdzXJ+zAuPiZAWBleGKxYex8IC9c1cCraic5qST5+b6UuAvgdcQ2BA+aWbPtKpjjtNOlPKISRpWlZ2HaL+Bvgl5h+KV1qLTGcE54pHH8QE0K+8SYlxAVx5xVU7aTB2yXWGz+pCGBAddeOuEcxStBLJWbx+9bu2Yx5WrmJpD3griqwQurv9A4O7698A5LeiT47QdRR4xV96+YUIcwOguGyuDmXVcfHAs0qlH289fvnYskCzNfXLHCzsLk+NFZOUxilj9+NZxKbPTZu/JPgz019i2Y5TdiXNF2rCqK4Cs1dtuIzf5oDN5Mo3Uktaa2RGxz/eZ2ZEt61kduJHaaSZ5A3iWkVrAVacfkXpcFZqTBa8AACAASURBVCNrkZE8Tp7BPE7y+LLpOaDYiJwsDZpmKymbh6iKCqsdcht1GvUaqRVGQ0cL597456JIaseZbuSpQvJUUGnHVdWrF1U+iwuagQy1VlGiuSoxFUPDI2PJ9Ba9em++9r7fHbc9fs8HZSTdK5uHqNFRzVNFJ3pd5QmIvQhUTHHN6n3h3zKR1I7TNVQNyqqabykvL1JS0NR6RK1XE7KwXnLyobkDUr2D690Pb+WsL/znBCERkSU89yooYBQRtV1w3bpCl952zW3UqYb2zEA5MzvQzF5lZgelvFw4OF1JVtBb1SCqqnmCstJeCCYImtHdxuyZM8b6MifMdHr+8rWZNR3yrlGGtKjsiKx6Edtf2Fm63sLiBYN8+g8On3CeOO2c26jdal+XpYybq+O0jHZehhfNAqv45VfNE5SVeiJrPj08MsrsPWZgBFHJ0X7JPsefd1F6i3opa0jPUpklYx7ixvAs7652Y9qm+3acVtHuy/BGpOGOqKqSKhPPEEe86N6aFCLxgTjeh2YGZCeFZ5ZdIk1ldv7ytax+fCtXLD4s1etrWwNzRjWLTk33XabkqOO0hKJleF4upFbQyFlgVZVU3kCSEn5R6MU0NDxSqmYEBNHgUR8bRdb9pEWeG/C1VRvHfd9pJTzPX76Wi1c80LA+NpIyuaPakUwBIWnvvFcrO+l0B3kDcJWavs0ia1BrxSwwbYCJiBdQGRzoK53fqUysRF+tl0//weFj9ZHn9KfbKbLas8gaMLOM0Mb49OBZiQ6TgqRd6NREf3kriHsJ0nPfC2wB/gv4Wfj+3uZ3zek28gbgdjDyNXIWWFXgxQeYNKKcS3df+KZJz/TjK4bkIHbJyYdS6x2/Zqn1iktOPrTSNbIGzN4cQ0hcRZM1mUgKknZi8YJB7r7wTWPCtt2FA+TXgzgIQNLVwM1mdlv4+USCYkCO01Dy9PLnL1+bekwrjXyNLPqSJfAuu2V95vki/XtWIFw8L9RHr1tLiTo+EyjKvVT2GZRxNkgz6n8k43sGxgmPvCSB7W747STK2CD+WyQcAMzsW8Abm9clp1vJW4ZPpXqnGWQNYk/vGC1UkRQ9i9WPb61LOJRVexTNhCejDsxb/cTVT0tOmJ9alxg693+iHSkjIH4p6WJJB0o6QNJfAb9qdsec7iRr8GkHI18j7SB5g1iRiiTtWUReS4uWruTr91QvAClomNpjMurAvIE/LjwWLxjkrGPmTdi3Ewy/nUQZN9czgUsIKssZ8L2wzXFaRjNr+paNvWi0m2uWOqVIRZKXBXay9SXKkHxexx08lzsf3MLm4RFm1XoYGU2m6XuxbyvWDBUmJyxbI6JMqnBnchRWlBvbUdrTzJ5tcn8mhSfrc6rSiCR4UXW3qhxx2R2pdZ8H+mrM3iM7Z1KcydRiiIgXFioaYFesGWLJDesmZK4tS/Rsgdzn3s4Bk9ONSVWUk/R64IvAnsA8SYcD7zezDxQcNx9YHmt6FfAxM/tMbJ8lwFmxvvw2MNfMtoYV7H4N7AJ2Zt2A40yGKquCRgc7XXrKoam1HLa/sLN0GutGGGTjq48lN6ybcK1ktHU99o2IuKop77l7tbj2oIwN4irgBEK7g5mtA95QdJCZbTCzI8KU4UcBOwjUVPF9roztcxHw3USW2OPC7S4cnKZQJfit0XaQNKP8nrNmTJid5+nvG22QHd1lXHbL+rHPSbvLZIRDxObhkY5NPdFtlEq1YWabNN4/uVqNRDgeeNjMHs/Z50xgWcXzOs6kyFoVGIH6Jln5DLLtIGXVIkV1JdLIGjirpMIuSzyArkoK8LJEQq1dUk+4OiubMgJiU6hmMkkzgQ8DP614nTPIGfwl9QNvAz4YazbgDkkGfN7Mrsk49lzgXIB58+ZV7JbT7eQNsFnV07IG/TJ5pNL2O3/5Wj6yfC2DA33s1VdLtUvsN9CXO5DF2zeHs/1G0OgZfXzFVSUXVbNo9/xfU02hkVrSvsBnCYLjBNwBfLhswaBQqGwGDjWzpzL2OR0428xOjrXtZ2abw9rY3wE+ZGbfy7uWG6mdeogG3ixjb7xKWdYgnWUsjgy/RdXVImq9AgtSdkf01Xo57ajBcaU/Idu4fGDGKqQsA3011l7yVqCcEbxH8BuzamwbGWVWrYfnd+5mtwWBbce8ag6P/WpkUiuuZpL3vXVLZbpJGamB+WZ2VrxB0iLg7pLXPxG4L0s4hExYYZjZ5vDvLyR9E3gdgYut4zSUogjlaADJm23mFfQ5f/nasfMWFbwZ3WVjeY0iVc8eM3q49f4nU3MPJfvRiAH20P1eMvY+a4UlBdlfy3g+ZdEOhmi3heRTxkj9DyXbssi1LUjaiyAy+//G2mZLekn0Hngr8OMK13ScymTWYuDF2W6W582sWvZPqaq65+kdozz73M6xz8Mjo4WJ9UZGd3HBdes46MJbJ13XYdUjT4+9jwzpA4liQmYvqoSqDvJTnZU3znSL0G80edlcf1fSBcBcSR+NvS4Fsss6jT9HP/AW4KZY23mSzovt9vvAHWa2Pdb2MuD7ktYBPwRuNbNvl74rx6mDrCjeKAFclqplaHiE53emB4fVgxivYirLLjOMydd1SK5yFi8YZPYeE5UN9SRLbIesvHHaIUK/nclbQcwkiH2YAbwk9noGeGeZk5vZDjPbx8y2xdquNrOrY5+/YmZnJI57xMwOD1+Hmtknyt+S061Mdma6eMFg5mx/8/BIZqbRXqkh7p8Rja7bI0F/uMLpKbm6SD7DPBValefcDll543RqGu5WkZfN9bvAdyV9pcA91XGmnEZ5owzmuL1m2Q92mdFbYHyeSsxgZHQ3tR6VXplEs/sl1weBc3nZU6s853bU+beDLaRdKWOD+KKkgeiDpDmSbm9inxynMlkz049UrDKWV5gni8GBPs48ev9Kx2Qxp79WufhOXg2FCCNdbTV7Zm/u8aO7jUtvXp/7XKqsAFzn31mUERD7mtlw9MHMngZe2rwuOU518mag167aWFpIFBXmSRLpq69YfBhnHzOv1GAdkVV456TXvqL0OQA+/QeHVxZqETte2MXDn/w9HsvJJTU8Mjr2XLIoWgFE6r8ouWAc1/m3L2UExG5JYxFokg6g8WpSx5kURTPQZfdsKn2uKOV40VDfK3HaUS+qJ65YfBgPf/L3Co+LuPKdh4/TfV/5zsNZvGCQOx/cUrqvUX/jevQqVJm5L14wmCk4884TN0zD+BKpUQ3qK2/f0JalQrudMnEQf0XgUfTd8PMbCCOXHaddKEo5UY99IE/vHp1z+Q83sfCAvcfpsIuOg8BYnKX7rqKPjwba+LnKBsolZ+5z+mup7rRxlddxB8/l2lUT600cd/DczOtk1Y8WL34vHsHcnhSuIEL30iMJMrNeBxxlZm6DcNqKIhVIPZSxR0Q6+qrH5dmKq8zqs9KPZxGpwOqtN521uslb9eTVj44zld5MTjp5cRAHh3+PBOYRpMsYIkj5fWRruuc45SmaeVZ1fy2ruknmTipjx0gGnkG+nr4KeWulXWaZAW6LFwxmqr0i6vFCqiLwPIK5vchTMV0AvA/4dMo2A7ojUYnTUWS5qULMdfOGdVx683q2jYwW5gAqq7qJV0qLH7fg8jtS1TZJW3bSTTdSwUS5lrZufz61UluaoMl7BpBfBa/I5bOemhhp6r94Fbyy53FaT+YKwszeF/49LuXlwsFpOWUC4UqphXYZwyOjlSN589xPP7J8La++6LYJ3lLDGSkyku1ZevqI0456JbVElFutR1x6yqEkKfMMqszU4899+/M7J6ihiryQ0oLRzjpmnkcwdwCZKwhJ78g70MxuytvuOEVUyeZZNhAumf66jGm6bF3pS04+NLOONATqm8iAe8XiwB5SdsadN2APDY9w471DnP66/cdqP8efV9pzPO2oQZbdsynTOF92pp587sMjo9R6xJz+GsM7ildgEWkrE68n3f7kqZii1NsvBV4PrAw/HwfcRSy/kuNUpWrkc5XSoPHBqGzN5kbqvpfds2lMQKSpV+Iz5YtXPMCyezYVCrKR0V3c+eCWCSmo057jkuvXgbI9t6rM1NOe++huo3/mDNZ87K2lzpGFRzC3P3kqpveY2XsIVrqHmNlpZnYaMHFN6zgVqZqTp94UDWUjo6MZdZ4aq6yHTXxgzsv1c/GKB7h21cbSLrhp95o1gCfLlkZUzTXUjqkxnNZRJg7iQDN7Mvb5KeC3mtQfp0soM/DEVSdZhXaKVCVJlVNvj9iZ4mPaP7OncFVTdlBMRlNnzZSrBO9B+r1WjZmoWgSnHqO0M30oE0l9l6TbJZ0j6d3ArcCdTe6XM80pysmTTAudJhzKqkqiyOhHl56UKhwAfvaL7YWrmrKD4i6zUu60eSuHsgbcKgN1PYO6p8PubsoEyn0QuBo4HDgCuMbMPtTsjjnTm7SBRwSz9kVLV3LZLetTo6J7paalZS5a1Sw5Yf4ED54epQemlfGOysvbtMeMHub01wrvNe051npU2dMoC0+H3d2UUTEB3Af82sz+TVK/pJeY2a+b2TFnehNX/USBYfESmlnsNuPRnMRyk6GUOiUx6e+VuPIPDk8tKBRlk73y9g2pHjpnHr1/atoKCLyFkrECeV5fyfa0tnoHdTcmdy+yAgOZpPcR5F7a28xeLek3gavN7PhWdLAKCxcutNWrV091N5yKlPU0gnLF5PMG0rxgt8+cfsQEj6Nar5g9cwbbRkYz7SCDA32lXGoH+mpcesqh4wbbyIupyFBdC5cqceNzX63XZ/POpJF0r5ktTNtWxgbxZ8AigkpymNnP8HTfTgMpa2gtoya5eMUDnL98bV0lLZPqlDn9NTDGguqyBvFIEBUxPDI6oS9lM8CmeSZ57iKn2ZQREM+b2QvRB0kzKJHuW9J8SWtjr2ckfSSxz7GStsX2+Vhs29skbZD0kKQLq9yU01lkDa4DfbVKuu8Va4b42qqNuUng8sqGwniDdv/MGaUqsEWrlDLutFmDer1eQZN1N51smVZnelPGBvFdSX8J9El6C/AB4Jaig8xsA4FRG0m9BIn+vpmy63+Y2dvjDeH+/xt4C/AE8CNJN5vZT0r01+kwsoLJkuqYIq68fUNuTWnI1vunVYQrM/jWejTBFlCkLks7b1G68iwm427aqDKtzvSlzArifwJbgAeA9wO3ARdXvM7xwMMValu/DnjIzB4JVy/fAE6teE2nA4jsBSOjuyakogYqzW7LZBRNVn7rlTj7mHljkc9px+SRDIq7+8I38ZnTj8hdTaSdt2olO5i8u2mWW+8F163zFYUDFBipJfUA95vZ70zqItKXgfvM7HOJ9mOBGwlWCZuBvzCz9ZLeCbzNzN4b7vcu4OjQ5TZ57nMJCxjNmzfvqMcfLyuDnKkmOYOFFw2vQOa2rNltlrFbwFWnH1F5VpzWvzT6az3Mmb3HBC+iy25ZPyGTaxnD8kEX3loqh9Rn6rinqtdxQ/j0p24jtZntBtbFS47WcfGZwCnA9Smb7wMOMLPDgX8AVkSHpXUno4/XmNlCM1s4d252VSun/cgLTKuaigOyYyvOOmbeuAGurN69bD2IHaO7JxjFAdZ87K185vQjKscQlFm5DA70TXrQLnMdN4R3N2VsEK8A1kv6IbA9ajSzU0pe40SC1cNTyQ1m9kzs/W2S/o+kfQlWFHGl8CsJVhjONCJLJZSnw89TI2XFBCSFw5Ib1o15BEX1IaLjV6wZGjfzj7umli3lGU8imOxTNNjmDe5F9ohGRTKXtXt43qXupYyAuGyS1zgTWJa2QdLLgafMzCS9jmBF8ytgGPhNSQcRGLfPAP5wkv1wpoC8mIS82s31FpQpCuq67Jb1E9xFR3cZ5y9fy0U33T+hKM/wyGiQHbUi0aBajyE4KVQG+muYUarAURWS16k335Uzfcm0QUiaBZwHvIbAQP0lM9tZ6eRSP7AJeJWZbQvbzgMws6slfRD4U2AnMAJ81Mx+EO73e8BngF7gy2b2iaLreaBce5FnY4hm63kz2KSQaIQ+vOwqoEx/8hjoqzF7jxmZArBMwF9Z4sF2vRJnHr1/qtG9iKLvy5me5Nkg8lYQ/wyMAv9BoCY6BPjzKhc2sx3APom2q2PvPwd8LnlcuO02Ao8pp0MpquEQDTpZRXiicpvtUlAmTTj0KPCEisdL1HrE9hd2TqhVHadRapsoZXhEWtGispRR0TndRZ6AOMTMDgOQ9CXgh63pkjNdKJPSe/GCwczYgcmm1UiSLAdaL70Su80y8x7teGFnah3qOI1S22SlDI8XLapCnoquyrN2pgd5AmLsP9zMdion86TT2TTrh1+2lkBWoNxxB89l0dKV+Qbn69eNzd7HqqnxosE5uq9ZtZ4J9oV6SUsYGO/XQQVqrMjI3IjnnpX+o2wRorJ4UF13kufmeniYHuMZSb8GXhu9l/RMznFOB5Gsu1Ald1ERZWsJpKWUPu2oQW68dyi3X5fevH5CKozR3calN6+fcF+NEg5QPPsv2n7kvL0AGvLci1KHNIp63I6dzidzBWFmxYllnI6nSq3nqlTRaSdVG4uWrizsV5aOf3hkNPW+GkGei2m0IkimL09y98NbuW/j8AShVc9zr5I6ZDJ46dHupGw9CGea0uwffr21BCbbryrlQT8dq+eQHNjj6b7zBFxSBWPkez1lrWiqPvfIzlDFi6ke1ZaXHu1OXEB0CVmDQrv+8Mv0a05/LdUYPKe/Rv/MbBfTODNnBKqYyBher10gbcVSjxWgnud+xeLDShuk67UlZNmJvPTo9MYFRBeQNyi06w+/TL8uOfnQcVHREMz4Lzn5UGBiLqcegkE7PnCPjO4eN0A2esWThYBZtd6GPPcqQq1elaK7wHYnLiC6gLxBIZo5t9sPv8yAVGaf5Las0qD12lyiwbnqamFWrYdPvuOwST/3qiuCyajuvPRo9+ECogsoGhSm+oefl/9oMv1KOz4rKK9sydM4VSPB44yM7m7Ic6+6ImhXlaLTnpSpB+F0OFk//nYYFKLkeXFbQpT/qMjlsx4X3Ua6heZ5Sg0O9HHWMdlJkBvlhlp1RVDW9dhxwAVEV9DoQaGRZSqvvH3DhOR5EMQzFPnY1+Ob38jAsqxBWARG7zzDcaMC2aoK/7SYk6nOteRlT9sXVzF1AY00MDY6ojZP912kF89LF37Qhbem3udghoqlSiW3iDLqmkZeL416nAymWqUYxyO02xtfQXQJUTnMR5eexN0XvqnuH1+jI2rz1FyTiVjOUjktOWE+td7x6p1ar+paTZVZmTVbpdOOK4IqeIR2e+MrCKcSjQ6sW3LC/AmuqhBkRC0aRMsUvBkZ3cVHr1vLpTevZ9vIKAP9NXYlVVp1ansa5Wk1WdppRVAVj9Bub1xAOJVotBdMNLBleTGVOTYafLPG+d32YlqOtMC60d3GBdet4/zlaysP4GUG504ewJtNo/6fPNNsc8gsGNSJeMGg5tPORWUWLV1Zl7tqksg9ddAHmqbTiP+ndv6f7ATyCga5DcKpRCt13lW9W9L0/fUQTZkamdnWSacR/09ux2germJyKtMMlUlSRXDcwXNZ/sNNmbUesvoFcMF16xrmRtqozLZONpP9f3I7RvNo2gpC0nxJa2OvZyR9JLHPWZLuD18/kHR4bNtjkh4Ij3W9UZNoBx/0tIC3a1dtzKz1kMfiBYN8+g8Ob8hKIsIHmvamnQNBO52mrSDMbANwBICkXmAI+GZit0eBN5rZ05JOBK4Bjo5tP87MftmsPnYzK9YMcenN68fVVJgqH/QqtRvy6jxHJI3XA/01zGBbeGzVtUXeQOPG0amnXRNOTgdapWI6HnjYzB6PN5rZD2IfVwGvbFF/upq8HEJToVJpxgw9S22x4PI7CutFxykqEORBXlOPZ5ptHq0SEGcAywr2+RPgW7HPBtwhyYDPm9k1aQdJOhc4F2DevOzcN86LFM3YW61SyXJ1TGNOf21S1xouEA5RLYlmps52Go+7EjeHpgsISTOBU4CLcvY5jkBA/PdY8yIz2yzppcB3JD1oZt9LHhsKjmsgcHNtaOenKUUCoNm62zSD9I33Do0bbGu9mhA81yPGaj3US5EwGt4xypqPvbXUudw46kx3WuHmeiJwn5k9lbZR0muBLwKnmtmvonYz2xz+/QWB7eJ1LehrV5AnAJqtu00zSN947xCnHTU4ztXxdQfOmXDsboPVj2+dcL4qRvYD96k/fUfyej0ZGVndOOpMF1qhYjqTDPWSpHnATcC7zOy/Yu2zgR4z+3X4/q3A5S3oa1eQlaJiTn+NS04ujmCuQnK1sOOFnalqmTsf3DJWvAjg1Rfdlnq+ZfdsGsuSWo8NYNUjT2f2tUg4Jq+X5krrxlFnOtFUASGpH3gL8P5Y23kAZnY18DFgH+D/KJiN7Qwj+l4GfDNsmwF83cy+3Yw+dqMXSquMemkDeBZxtcyKNUOl0nLXYwPIi48oCtDKst30Suw265r/H6d7aKqAMLMdBAIg3nZ17P17gfemHPcIcHiyvdF0sxdKK4x6VdxX9+qrjaXKyCulEy+0k5fue9HSlanCr1dKFRK9UuHzyLrebjMeXXpS7rGO04l0daoND9FvLmWNtbUesf2FnWMrjDxPgzOP3n/sfZauX5BZZS5+fNZ5s/CALKfb6GoB4V4ozSVr4Bzoq40zSO85a0ZqVbkkZx8zb1yVtrTcS2l1oONC/4rFh3H2MfPGViK90oTzZnHcwXMrtTtOp9PVuZi8gHtzyYpwTabyPujCWwvPNTjQN2EQT7OlZNk54kL/isWHlRIISe58cEuldsfpdLpaQHiIfnMpawwvik3I+06StpSslN+NEPq+4nS6ja4WEB6iX52qXl9ljOFpgrremgzNFPq+4nS6ja4WEOAh+lVoltdXIwV1M4W+rzidbsMryjmlyVLfDA70jQtym850Y9yMM73JqyjX9SsIpzxZdoKqZT47eZD1FafTTbiAaDGdPDg2gm4OTnScTqOr4yBaTVqium6reezBiY7TObiAaCGdPjj2ZmQvzWpPw11FHadzcAHRQjp9cJxMmooIT1fhOJ2DC4gWMpWDY9W6CWlMJk1FxJIT5lPrGb/iqPXIXUUdpw1xI3ULmSo/+kYahutNUzGOpEaqvIbKcZwW4iuIFrJ4wSCffMdh4xLVFdUgaATtZPu48vYNExLzje6yjrHDOE434SuIFjMVfvTtZPtop744jpOPC4guoFE5hBoRw+H5jBync3AVU4dRj7E5rW5CVdtHo2I4GtEXx3FaQ9MEhKT5ktbGXs9I+khiH0n6e0kPSbpf0pGxbe+W9LPw9e5m9bOTqHeQboTto1F2jKmywziOU52mqZjMbANwBICkXmAI+GZitxOB3wxfRwP/CBwtaW/gEmAhQdbneyXdbGZPN6u/nUDeIF00wE7W9tFI24HnM3KczqBVKqbjgYfN7PFE+6nAVy1gFTAg6RXACcB3zGxrKBS+A7ytRX1tW6bSwOsBbo7TfbTKSH0GsCylfRDYFPv8RNiW1T4BSecC5wLMmzevEX1tW4oMvI0wImedo5ExHN2esNBxOoWmryAkzQROAa5P25zSZjntExvNrjGzhWa2cO7c6V08Ps/A2wgjcto5zl++lgMvvJUrb9/AaUcN5toOyhjQPWGh43QOrVhBnAjcZ2ZPpWx7Aogn8nklsDlsPzbRfleT+tcx5FVLW7R0Zd32iYg0G0cklYeGR7jx3qFMg3LZaO3J2FEcx2ktrRAQZ5KuXgK4GfigpG8QGKm3mdmTkm4H/lrSnHC/twIXNb+r7U+WgbfIPlFGrVNky8gbyMsO/B4o5zidQ1NVTJL6gbcAN8XazpN0XvjxNuAR4CHgC8AHAMxsK/Bx4Efh6/Kwzckgz4hcVq1TxuBcdYBPtrux23E6h6auIMxsB7BPou3q2HsD/izj2C8DX25m/6YTeUbkvNn96se3suyeTewyQ4Iewe6cMuV5A3yZCOmpSljoOE51PJJ6mpAXgJY1ux8aHuHaVRvZZYFEMAuEQ38t+LdIegrkDeRlI6Q9UM5xOgfPxTSNyLJPZM3us3h+p/HY0pNYsWaIy25Zz9M7RgHYY0b2fCLPgF62n47jtBcuILqALLVOUu0UEa0oAJ4b3T32fnhkNLeOhA/8jjO9cBVTF5Cl1unJKNQTtbdTHQnHcVqPryC6hLTZ/UU33c9IbIUQEamS3CXVcbobX0F0Mc+lCAdgTGi4S6rjdDcuILqYrIFeBIF1XrvBcbobFxAdRj0Fg7JYcsL8zKRXUQS0u6Q6TvfiNogOomy+o7IsXjDIR5avTd0W2RncM8lxuhdfQXQQzfAqGnQ7g+M4GbiA6CCa4VXkdgbHcbJwAdFBNMOryO0MjuNk4TaIDqJZie7czuA4ThouIDqIKvmOHMdxJosLiA7DZ/uO47QKt0E4juM4qbiAcBzHcVJxAeE4juOk4gLCcRzHScUFhOM4jpOKzHIq1HcYkrYAj7fwkvsCv2zh9doNv3+//269/+l07weY2dy0DdNKQLQaSavNbOFU92Oq8Pv3++/W+++We3cVk+M4jpOKCwjHcRwnFRcQk+Oaqe7AFOP339108/13xb27DcJxHMdJxVcQjuM4TiouIBzHcZxUXECUQNKApBskPSjpp5J+N7H9WEnbJK0NXx+bqr42GknzY/e1VtIzkj6S2EeS/l7SQ5Lul3TkVPW3kZS892n73QNIOl/Sekk/lrRM0qzE9j0kLQ+/+3skHTg1PW0OJe7/HElbYt//e6eqr83A032X47PAt83snZJmAv0p+/yHmb29xf1qOma2ATgCQFIvMAR8M7HbicBvhq+jgX8M/3Y0Je8dpul3L2kQ+DBwiJmNSLoOOAP4Smy3PwGeNrPXSDoD+Bvg9JZ3tgmUvH+A5Wb2wVb3rxX4CqIASb8BvAH4EoCZvWBmw1PbqynjeOBhM0tGq58KfNUCVgEDkl7R+u41lax7n+7MAPokGJaY4AAABEtJREFUzSCYGG1ObD8V+Ofw/Q3A8ZLUwv41m6L7n9a4gCjmVcAW4J8krZH0RUmzU/b7XUnrJH1L0qEt7mOrOANYltI+CGyKfX4ibJtOZN07TNPv3syGgE8BG4EngW1mdkdit7Hv3sx2AtuAfVrZz2ZR8v4BTgtVqzdI2r+lnWwyLiCKmQEcCfyjmS0AtgMXJva5jyCfyeHAPwArWtvF5hOq1k4Brk/bnNI2bfynC+592n73kuYQrBAOAvYDZks6O7lbyqHT4rsvef+3AAea2WuBf+PF1dS0wAVEMU8AT5jZPeHnGwgExhhm9oyZPRu+vw2oSdq3td1sOicC95nZUynbngDiM6dXMr2W4pn3Ps2/+zcDj5rZFjMbBW4CXp/YZ+y7D9UwewFbW9rL5lF4/2b2KzN7Pvz4BeCoFvexqbiAKMDMfg5skjQ/bDoe+El8H0kvj/Sukl5H8Fx/1dKONp8zyVax3Az8UejNdAzBUvzJ1nWt6WTe+zT/7jcCx0jqD+/xeOCniX1uBt4dvn8nsNKmT/Rt4f0nbG2nJLd3Ou7FVI4PAV8LVQ2PAO+RdB6AmV1N8MP4U0k7gRHgjGn0I0FSP/AW4P2xtvj93wb8HvAQsAN4zxR0symUuPdp+92b2T2SbiBQo+0E1gDXSLocWG1mNxM4b/yLpIcIVg5nTFmHG0zJ+/+wpFPC7VuBc6aqv83AU204juM4qbiKyXEcx0nFBYTjOI6TigsIx3EcJxUXEI7jOE4qLiAcx3GcVFxAOA4gaZ9YRs6fSxqKfZ7ZoGvcJWlDmJbj7lhsTXK/L0o6pBHXdJzJ4G6ujpNA0qXAs2b2qVjbjDDX0GTOexfwF2a2WtK5wNvN7JTEPr1mtmsy13GcRuErCMfJQNJXJP2dpDuBv5F0qaS/iG3/cVT/QNLZkn4Yrjg+H6YHz+N7wGvCY5+VdLmkewgS/90laWG47W2S7gtXHf8ets2W9GVJPwoTSJ7a+Lt3HBcQjlPEbwFvNrMLsnaQ9NsENRAWmdkRwC7grILzngw8EL6fDfzYzI42s+/HzjuXIL/PaWEywP8RbvorgpQW/w04DrgyI8Ow40wKT7XhOPlcX0LlczxBkrYfhWmZ+oBfZOz7NUkjwGMEKVwgECg3pux7DPA9M3sUwMyiJHhvBU6JrWZmAfOYZnmAnKnHBYTj5LM99n4n41fdUflJAf9sZheVON9ZZrY60fZchhAS6amzRbCq2FDieo5TN65icpzyPEaY6l1B3e2DwvZ/B94p6aXhtr0lHdCA6/0n8EZJB0XnDdtvBz4UyyK7oAHXcpwJuIBwnPLcCOwtaS3wp8B/AZjZT4CLgTsk3Q98B5h0yVUz2wKcC9wkaR2wPNz0caAG3C/px+Fnx2k47ubqOI7jpOIrCMdxHCcVFxCO4zhOKi4gHMdxnFRcQDiO4zipuIBwHMdxUnEB4TiO46TiAsJxHMdJ5f8BPVEBFqtTvSEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the predictions respect to the real values\n",
    "plt.scatter(y_test, l1.predict(X_test))\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Evaluation of Lasso Regression Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization: Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.005, random_state=0)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = Ridge(alpha=0.005, random_state=0)\n",
    "l2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 411280\n",
      "train rmse: 641\n",
      "train r2: 0.6726833763462601\n",
      "\n",
      "test mse: 488846\n",
      "test rmse: 699\n",
      "test r2: 0.6609569241656668\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# transform target and predictions (log) back to original\n",
    "\n",
    "# we evaluate using the mean squared error,\n",
    "# the root of the mean squared error, and r2\n",
    "\n",
    "# make predictions for train set\n",
    "pred = l2.predict(X_train)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('train mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print('train rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\n",
    "print('train r2: {}'.format(\n",
    "    r2_score(np.exp(y_train), np.exp(pred))))\n",
    "print()\n",
    "\n",
    "# make predictions for test set\n",
    "pred = l2.predict(X_test)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('test mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print('test rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\n",
    "print('test r2: {}'.format(\n",
    "    r2_score(np.exp(y_test), np.exp(pred))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Evaluation of Rdige Regression Predictions')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the predictions respect to the real values\n",
    "plt.scatter(y_test, l2.predict(X_test))\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Evaluation of Rdige Regression Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 292804\n",
      "train rmse: 541\n",
      "train r2: 0.7669719635174365\n",
      "\n",
      "test mse: 429965\n",
      "test rmse: 655\n",
      "test r2: 0.7017944385482556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# transform target and predictions (log) back to original\n",
    "\n",
    "# we evaluate using the mean squared error,\n",
    "# the root of the mean squared error, and r2\n",
    "\n",
    "# make predictions for train set\n",
    "pred = svr.predict(X_train)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('train mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print('train rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\n",
    "print('train r2: {}'.format(\n",
    "    r2_score(np.exp(y_train), np.exp(pred))))\n",
    "print()\n",
    "\n",
    "# make predictions for test set\n",
    "pred = svr.predict(X_test)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('test mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print('test rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\n",
    "print('test r2: {}'.format(\n",
    "    r2_score(np.exp(y_test), np.exp(pred))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Evaluation of SVR Predictions')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the predictions respect to the real values\n",
    "plt.scatter(y_test, svr.predict(X_test))\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Evaluation of SVR Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x204b409cac8>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the distribution of the errors:\n",
    "# they should be fairly normally distributed\n",
    "\n",
    "errors = y_test - svr.predict(X_test)\n",
    "errors.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(random_state=0)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 25861\n",
      "train rmse: 160\n",
      "train r2: 0.9794181029263175\n",
      "\n",
      "test mse: 454145\n",
      "test rmse: 673\n",
      "test r2: 0.6850241089221445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# transform target and predictions (log) back to original\n",
    "\n",
    "# we evaluate using the mean squared error,\n",
    "# the root of the mean squared error, and r2\n",
    "\n",
    "# make predictions for train set\n",
    "pred = dt.predict(X_train)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('train mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print('train rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\n",
    "print('train r2: {}'.format(\n",
    "    r2_score(np.exp(y_train), np.exp(pred))))\n",
    "print()\n",
    "\n",
    "# make predictions for test set\n",
    "pred = dt.predict(X_test)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('test mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print('test rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\n",
    "print('test r2: {}'.format(\n",
    "    r2_score(np.exp(y_test), np.exp(pred))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Evaluation of Decision Tree Regression Predictions')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the predictions respect to the real values\n",
    "plt.scatter(y_test, dt.predict(X_test))\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Evaluation of Decision Tree Regression Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 63481\n",
      "train rmse: 251\n",
      "train r2: 0.9494786232469163\n",
      "\n",
      "test mse: 302211\n",
      "test rmse: 549\n",
      "test r2: 0.7903993797649769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "# transform target and predictions (log) back to original\n",
    "\n",
    "# we evaluate using the mean squared error,\n",
    "# the root of the mean squared error, and r2\n",
    "\n",
    "# make predictions for train set\n",
    "pred = rf.predict(X_train)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('train mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print('train rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\n",
    "print('train r2: {}'.format(\n",
    "    r2_score(np.exp(y_train), np.exp(pred))))\n",
    "print()\n",
    "\n",
    "# make predictions for test set\n",
    "pred = rf.predict(X_test)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('test mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print('test rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\n",
    "print('test r2: {}'.format(\n",
    "    r2_score(np.exp(y_test), np.exp(pred))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Evaluation of Random Forest Regression Predictions')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the predictions respect to the real values\n",
    "plt.scatter(y_test, rf.predict(X_test))\n",
    "plt.xlabel('True Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Evaluation of Random Forest Regression Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x204b421ae80>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the distribution of the errors:\n",
    "# they should be fairly normally distributed\n",
    "\n",
    "errors = y_test - rf.predict(X_test)\n",
    "errors.hist(bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'n_estimators':range(10,300,10), 'criterion':('mse','rmse'), 'max_features':('auto','sqrt','log2')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 392, in fit\n",
      "    for i, t in enumerate(trees))\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 921, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\joblib\\parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 168, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1246, in fit\n",
      "    X_idx_sorted=X_idx_sorted)\n",
      "  File \"C:\\Users\\scofi\\Anaconda3_1\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 336, in fit\n",
      "    criterion = CRITERIA_REG[self.criterion](self.n_outputs_,\n",
      "KeyError: 'rmse'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(),\n",
       "             param_grid={'criterion': ('mse', 'rmse'),\n",
       "                         'max_features': ('auto', 'sqrt', 'log2'),\n",
       "                         'n_estimators': range(10, 300, 10)},\n",
       "             scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(rf,parameters,scoring='neg_root_mean_squared_error',cv=3, )\n",
    "gs.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'mse', 'max_features': 'log2', 'n_estimators': 290}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features='log2', n_estimators=290)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1870666881916266"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.005, random_state=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up the model\n",
    "# remember to set the random_state / seed\n",
    "\n",
    "lin_model = Lasso(alpha=0.005, random_state=0)\n",
    "\n",
    "# train the model\n",
    "\n",
    "lin_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 414118\n",
      "train rmse: 643\n",
      "train r2: 0.6704244126197878\n",
      "\n",
      "test mse: 495973\n",
      "test rmse: 704\n",
      "test r2: 0.656013826121537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions for train set\n",
    "pred = lin_model.predict(X_train)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('train mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_train), np.exp(pred)))))\n",
    "print('train rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_train), np.exp(pred))))))\n",
    "print('train r2: {}'.format(\n",
    "    r2_score(np.exp(y_train), np.exp(pred))))\n",
    "print()\n",
    "\n",
    "# make predictions for test set\n",
    "pred = lin_model.predict(X_test)\n",
    "\n",
    "# determine mse and rmse\n",
    "print('test mse: {}'.format(int(\n",
    "    mean_squared_error(np.exp(y_test), np.exp(pred)))))\n",
    "print('test rmse: {}'.format(int(\n",
    "    sqrt(mean_squared_error(np.exp(y_test), np.exp(pred))))))\n",
    "print('test r2: {}'.format(\n",
    "    r2_score(np.exp(y_test), np.exp(pred))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x204aafac518>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, lin_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
